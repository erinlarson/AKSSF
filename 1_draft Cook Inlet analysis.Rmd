---
title: "1_draft Cook Inlet analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(readxl)
library(lubridate)
library(tidyverse)
library(MARSS)
```


Stream temperature models for the Deshka, Anchor, and Kenai watersheds were completed for USFWS in Fall 2020. The reviewed temperature time series and DAYMET air temperatures can be recylced for this analysis as the data are ready.


# Read in data from Deshka, Anchor, and Kenai

## Deshka 

For each watershed, temperature data, site data, and climate data are all needed. Read in temperature data first and convert to daily means, link catchment ids to temperature data using sites table, and join climate data to final temperature dataset. 

For Deshka and Kenai, there is a separate temperature data csv with daily data.

```{r deshka temp data}

deshka_wd <- "W:/Github/Deshka_temperature/output/data_catalog"

deshka_temp <- read_csv(paste0(deshka_wd, "/deshka_temperature_data.csv", collapse = ""))
deshka_temp2 <- read_csv(paste0(deshka_wd, "/deshka_temperature_data2.csv", collapse = ""))

deshka_temp2 <- deshka_temp2 %>% 
  filter(useData == 1) %>% 
  rename(meanTemp = Temperature) %>% 
  mutate(SiteID = tolower(SiteID))

deshka_daily <- deshka_temp %>%
  filter(useData == 1) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanTemp = mean(Temperature))

deshka_daily <- bind_rows(deshka_daily, deshka_temp2)

deshka_daily %>% distinct(SiteID)
```

Three edits to site table - kroto 5 downstream misspelled, chijik trib should be chijik to match temperature SiteID and PMC1 listed twice. In temperature dataset, there is a PMC1 and PMC1-2, which are in the same catchment and just slightly separated. Keep PMC1, which has the longer time series.

```{r deshka catchment ID}

deshka_sites <- read_csv(paste0(deshka_wd, "/deshka_sites.csv", collapse = ""))

deshka_sites %>% 
  arrange(SiteID)

deshka_sites <- deshka_sites %>%
  mutate(SiteID = case_when(SiteID == "Kroto 5 Downstrean" ~ "Kroto 5 Downstream",
                            SiteID == "chijik trib.arri" ~ "chijik.arri",
                            TRUE ~ SiteID)) %>% 
  filter(!(SiteID == "PMC1" & useSite == 1))

deshka_daily %>% 
  filter(SiteID %in% c("PMC1", "PMC1-2")) %>% 
  summarize(mindate = min(sampleDate),
            maxdate = max(sampleDate))

deshka_daily <- left_join(deshka_daily, deshka_sites %>% select(SiteID, catchmentID))

deshka_daily %>% distinct(SiteID, catchmentID)

#remove pmc1-2
deshka_daily <- deshka_daily %>% 
  filter(!(is.na(catchmentID)))

deshka_daily

```


```{r deshka climate data}

deshka_climate <- read_csv(paste0(deshka_wd, "/deshka_climate_variables.csv", collapse = ""))

deshka_climate

deshka_daily <- left_join(deshka_daily, deshka_climate %>% select(catchmentID, sampleDate, tair3))

deshka_daily

```


## Anchor

```{r anchor temp data}

anchor_wd <- "W:/Github/Temperature_Data/output/data_catalog"

anchor_temp <- read_csv(paste0(anchor_wd, "/anchor_temperature_data.csv", collapse = ""))

anchor_daily <- anchor_temp %>%
  filter(useData == 1) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanTemp = mean(Temperature))

anchor_daily
```



```{r anchor catchment ID}

anchor_sites <- read_csv(paste0(anchor_wd, "/anchor_sites.csv", collapse = ""))

anchor_sites %>% 
  arrange(SiteID)

anchor_daily %>% 
  distinct(SiteID)

anchor_daily <- left_join(anchor_daily, anchor_sites %>% select(SiteID, catchmentID))

anchor_daily %>% distinct(SiteID, catchmentID)

anchor_daily

```


```{r anchor climate data}

anchor_climate <- read_csv(paste0(anchor_wd, "/anchor_climate_variables.csv", collapse = ""))

anchor_climate

anchor_daily <- left_join(anchor_daily, anchor_climate %>% select(catchmentID, sampleDate, tair3))

anchor_daily

```


## Kenai

```{r kenai temp data}

kenai_wd <- "W:/Github/Kenai_temperature/output/data_catalog"

kenai_temp <- read_csv(paste0(kenai_wd, "/kenai_temperature_data.csv", collapse = ""))
kenai_temp2 <- read_csv(paste0(kenai_wd, "/kenai_temperature_data2.csv", collapse = ""))

kenai_temp2 <- kenai_temp2 %>% 
  rename(meanTemp = Temperature) 

kenai_daily <- kenai_temp %>%
  filter(useData == 1) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanTemp = mean(Temperature))

kenai_daily <- bind_rows(kenai_daily, kenai_temp2)

kenai_daily %>% summary
```


```{r kenai catchment ID}

kenai_sites <- read_csv(paste0(kenai_wd, "/kenai_sites.csv", collapse = ""))

kenai_sites %>% 
  arrange(SiteID)

kenai_daily %>% 
  distinct(SiteID)

kenai_daily <- left_join(kenai_daily, kenai_sites %>% select(SiteID, catchmentID))

kenai_daily %>% distinct(SiteID, catchmentID)

kenai_daily %>% summary

```


```{r kenai climate data}

kenai_climate <- read_csv(paste0(kenai_wd, "/kenai_climate_variables.csv", collapse = ""))

kenai_climate <- kenai_climate %>% 
  mutate(sampleDate = as.Date(sampleDate))

str(kenai_daily)
str(kenai_climate)

kenai_daily <- left_join(kenai_daily, kenai_climate %>% select(catchmentID, sampleDate, tair3))

kenai_daily %>% summary

#lots of missing winter airtemps
kenai_daily %>%
  ungroup() %>% 
  filter(is.na(tair3)) %>% 
  count(month(sampleDate))

#missing 2019 summer airtemps that were downloaded for usgs sites
kenai_daily %>%
  ungroup() %>% 
  filter(is.na(tair3), month(sampleDate) == 6)

```


## Combine

Combine the three datasets and see if there are years with overlapping data. No overlap between Anchor or Kenai and Deshka because that project started in 2017.

```{r combine daily data}
ci_daily <- bind_rows(deshka_daily %>% mutate(wtd = "Deshka"), anchor_daily %>% mutate(wtd = "Anchor"), kenai_daily %>% mutate(wtd = "Kenai")) %>%
  ungroup() %>% 
  mutate(year = year(sampleDate))

```



```{r remove extra data}
rm(kenai_temp, kenai_sites, kenai_climate)
rm(anchor_temp, anchor_sites, anchor_climate)
rm(deshka_temp, deshka_sites, deshka_climate)

```
# Run DFA

Decide on best year(s) to try and run.

```{r identify complete summers and best years}

ci_summer_sites <- ci_daily %>% 
  filter(month(sampleDate) %in% 6:8) %>% 
  group_by(wtd, SiteID, year) %>% 
  summarize(n = n()) %>% 
  filter(n > 82) %>% 
  ungroup()

ci_summer_sites %>% 
  distinct(wtd, SiteID, year) %>% 
  count(year, wtd) %>% 
  pivot_wider(names_from = wtd, values_from = n, values_fill = 0) %>% 
  mutate(max_sites = Kenai + Deshka + Anchor)


```

```{r prepare data}

ci_daily_2017 <- left_join(ci_summer_sites %>% filter(year == 2017) %>% select(SiteID, year), 
                           ci_daily %>% filter(month(sampleDate) %in% 6:8)) %>% 
  arrange(SiteID, year)

ci_daily_2017_zsc <- bind_cols(ci_daily_2017 %>% 
  group_by(SiteID, year) %>% 
  summarize(meanTemp_zsc = scale(meanTemp),
            tair3_zsc = scale(tair3)),
  ci_daily_2017 %>% select(sampleDate, meanTemp, tair3))

ci_daily_2017_zsc 

ci_daily_2017 %>% 
  filter(SiteID == "CIK14") %>% 
  summarize(mean = mean(meanTemp),
            sd = sd(meanTemp))

ci_daily_2017 %>% 
  filter(SiteID == "CIK14") %>% 
  mutate(zsc = (meanTemp - 12.18014)/1.902544)



```


```{r zscores and remove na cols}

ci_daily_2017_zsc %>% 
  ggplot() +
  geom_line(aes(x = sampleDate, y = meanTemp, color = "blue")) +
  geom_line(aes(x = sampleDate, y = meanTemp_zsc, color = "red")) +
  facet_wrap(~SiteID)

ci_daily_2017_zsc

#convert to n x T: n time series over T time steps

ci_2017_stream <- ci_daily_2017_zsc %>%
  ungroup() %>% 
  select(SiteID, sampleDate, meanTemp_zsc) %>% 
  pivot_wider(names_from = sampleDate, values_from = meanTemp_zsc)

ci_2017_air <- ci_daily_2017_zsc %>%
  ungroup() %>% 
  select(SiteID, sampleDate, tair3_zsc) %>% 
  pivot_wider(names_from = sampleDate, values_from = tair3_zsc)

#remove dates with any missing values for stream temp
nacols <- colnames(ci_2017_stream)[colSums(is.na(ci_2017_stream)) > 0]
nacols

ci_2017_stream <- ci_2017_stream %>% select(-c(nacols, SiteID))
ci_2017_air <- ci_2017_air %>% select(-c(nacols, SiteID))

```

```{r dfa}

#example code line from https://github.com/tjcline/dfaTMB/blob/master/SimulationTesting/TestMARSSvsTMB.R
# mod13_marss<-MARSS(noNA_Stream[streamSet,],model=list(m=1,R='unconstrained'),covariates=matrix(noNA_Air,nrow=1),form='dfa')

x = 200 #set for minit
c = 5000 #set for maxit = (x + c) - to ensure convergence

#error won't run
ci_marss <- MARSS(as.matrix(ci_2017_stream), model = list(m = 3, R = 'unconstrained'), covariates = as.matrix(ci_2017_air),
                  form = "dfa")

#ran, but convergence error bc maxit set to 500.
ci_marss <- MARSS(as.matrix(ci_2017_stream), model = list(m = 1, R = 'unconstrained'),
                  form = "dfa")

```

