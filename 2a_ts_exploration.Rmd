---
title: "TS exploration"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(sf)
library(tmap)
library(lubridate)
library(ggpubr)
library(nlme)
library(e1071)
library(car)
```


Modeling thermal sensitivities for five regions in Southern AK.

1. Read in response and covariate data and combine to create model data frame
2. Explore TS response - removed 3 sites with very negative TS
3. Explore correlations and multi-collinearity of covariates -- only select those with pairwise correlation < 0.7 and stepwise remove covariates until all have VIF < 3
4. Transform/model covariates as needed. Arcsine square root transformation for proportional covariates since 0-1 scale. Log transformation for continuous covariates. Plotted watershed slope and LCLD (last day of the longest continuous snow season) and decided to remove effect of slope on snow and use the residual - per Tim's method -- new covariate called snow index. Check for outliers after transformations.


# DFA Results

Current DFA results include interaction between air temperature and daylength from 3/17/22. For first report, will move forward with TS in model with and without daylength.


```{r dfa output}
load("DFA/output_17Mar22/DFA_copy_DFAresults_GlobalDayLength_wInteraction_AllSubsets.Rdata")

dfa_ts <- GlobalResults_DayLen_Subset1 %>% 
  select(Region, SiteID, Year, TempSens_Air.DayLen, TempSens_Air) %>% 
  distinct()

dfa_trends <- GlobalTrends_DayLen_Subset1 %>%
  select(Year, DOY, Trend_Air, Trend_Air.DayLen) %>% 
  pivot_longer(cols = c(Trend_Air.DayLen, Trend_Air), names_to = "model", values_to = "trend")  

dfa_loads <- GlobalResults_DayLen_Subset1 %>% 
  select(SiteID, Year, TrendLoad_Air, TrendLoad_Air.DayLen) %>% 
  distinct()

```


Trends 

```{r trends by year}

dfa_trends %>%
  filter(!Year == 2020) %>% 
  mutate(Model = case_when(model == "Trend_Air" ~ "Air Only",
                           model == "Trend_Air.DayLen" ~ "Air + Daylength")) %>% 
  ggplot(aes(x = DOY, y = trend, color = Model)) +
  geom_line() +
  facet_wrap(~Year) +
  theme_bw() +
  labs(y = "Trend", x = "Day of Year")

```

Plot of thermal sensitivity versus loading on common trend. Tim commented that sites with low thermal sensitivities often load strongly on the common trend, these might be groundwater sites, glacial sites, etc. Some sites have negative loadings, most likely glacial sites that get cooler as the summer progresses.

```{r ts versus trend}
#w daylength
left_join(dfa_ts, dfa_loads) %>% 
  ggplot(aes(TempSens_Air.DayLen, TrendLoad_Air.DayLen)) +
  geom_point(aes(color = Region)) +
  facet_wrap(~Year)

#air only
left_join(dfa_ts, dfa_loads) %>% 
  ggplot(aes(TempSens_Air, TrendLoad_Air)) +
  geom_point(aes(color = Region)) +
  facet_wrap(~Year)

```

Frequency plots of thermal sensitivities by year and region.

```{r ts figure by model type}

dfa_ts %>% 
  pivot_longer(cols = starts_with("Temp")) %>%
  mutate(Model = case_when(name == "TempSens_Air" ~ "Air Only",
                           name == "TempSens_Air.DayLen" ~ "Air + Daylength")) %>% 
  ggplot(aes(x = value, color = Model)) +
  geom_freqpoly() +
  # facet_wrap(~Region) +
  theme_bw() +
  theme(legend.position = "bottom") +
  labs(x = expression(paste(tau)), y = "Count")

ggsave("output/ts by model type.jpeg", width = 4, height = 4)

```

```{r ts figure by region}

dfa_ts %>%
  mutate(Region = case_when(Region == "Copper" ~ "Copper River",
                            TRUE ~ Region)) %>% 
  group_by(Region) %>% 
  mutate(med = median(TempSens_Air.DayLen),
         med_pos = case_when(Region == "Cook Inlet" ~ 55,
                             Region == "Bristol Bay" ~ 50,
                             Region == "Copper River" ~ 45,
                             Region == "Kodiak" ~ 40,
                             Region == "Prince William Sound" ~ 35),
         med_lab = paste(Region, round(med, 2), sep = " = ")) %>% 
  ggplot() +
  geom_freqpoly(aes(x = TempSens_Air.DayLen, color = Region), show.legend = FALSE) +
  # geom_vline(aes(xintercept = med, color = Region), linetype = 2, size = 1, show.legend = FALSE) +
  geom_text(aes(x = 0.8, y = med_pos, label = med_lab, color = Region), show.legend = FALSE, check_overlap = TRUE) +
  theme_bw() +
  theme(legend.position = "bottom", text = element_text(size = 14)) +
  labs(y = "Count", x = expression(paste(tau))) +
  guides(color=guide_legend(nrow=2,byrow=TRUE))


ggsave("output/ts by Region.jpeg", width = 5, height = 4)

```


# Read in Covariates

Read in covariates. Copied over from Dustin's script, but modified so that lcld is combined with the other covariates.

```{r covariates, include = FALSE}

#Data from covariates script
cov.df.raw <- read.csv(file = 'data_preparation/sensitivity_drivers/AKSSF_Covariates.csv', header = TRUE) 
#Data from spatial join script
sites.df <- read.csv(file = 'data_preparation/sensitivity_drivers/AKSSF_sites_sj_maxfac.csv', header = TRUE)
#Data from Modis Script
lcld.df <- read.csv(file = 'data_preparation/sensitivity_drivers/AKSSF_wtd_lcld_mn.csv', header = TRUE)

lcld.long <- lcld.df %>% 
  pivot_longer(cols = wtd_lcld_mn_2001:wtd_lcld_mn_2019, names_to = "metric", values_to = "wtd_lcld") %>% 
  mutate(Year = as.numeric(substr(metric, 13, 17)),
         wtd_lcld_jd = wtd_lcld - 365) %>% 
  select(-metric)

#dput(colnames(sites.df))
site_keep_cols <- c("SiteID", "cat_ID_con", "site_max_acc", "site_acc_sqKm",
                    "Area_km2","area_diff","size_diff", "lat", "lon",
                    'str_ord','str_slope', "dist_coast_km", "ds_dist_outlet")

sites.df <- sites.df %>% 
  mutate( region = sub("_$","",gsub('[[:digit:]"]+', '', cat_ID_con)),
  size_diff =  case_when(area_diff >0~'Larger', area_diff <0~'Smaller',
                        TRUE~'No Change')) %>% 
  select(all_of(site_keep_cols), ts_id = ts_id_dm)

#don't keep ds_dist_outlet bc it is incorrect for some sites/networks
# sites.df %>% 
#   ggplot(aes(x = dist_coast_km, y = ds_dist_outlet)) +
#   geom_point()
  
#dput(colnames(cov.df.raw))
wtd_keep_cols <- c("cat_ID_con", "cat_ID", "cat_slope_MIN", "cat_slope_MAX",
  "cat_slope_MEAN", "cat_slope_STD", "cat_slope_SUM", "cat_elev_MIN", 
  "cat_elev_MAX", "cat_elev_MEAN", "cat_elev_STD", "wtd_elev_MIN",
  "wtd_elev_MAX", "wtd_elev_MEAN", "wtd_elev_STD", "wtd_slope_MIN",
  "wtd_slope_MAX", "wtd_slope_MEAN", "wtd_slope_STD", "wtd_north_per", 
  "wtd_wet_per","wtd_lake_per","wtd_glacier_per","wtd_area_sqKM")


cov.df <- cov.df.raw %>%
  mutate(wtd_area_sqKM = wtd_elev_AREA *1e-6) %>% 
  select(all_of(wtd_keep_cols))

# Convert na to 0
cov.df[is.na(cov.df)] <- 0

cov.all <- left_join(sites.df %>% select(SiteID, ts_id, cat_ID_con, str_ord, str_slope, dist_coast_km),
                     cov.df)

cov.all <- left_join(cov.all, lcld.long)
summary(cov.all)

```


```{r covariate exploration, eval = FALSE}

names(cov.all)

cov.all %>% 
  group_by(Region) %>% 
  summarize(meanslope = mean(wtd_slope_MEAN))

cov.all %>% 
  group_by(Region) %>% 
  summarize(meanelev = mean(wtd_elev_MEAN))


cov.all %>% 
  ggplot(aes(x = wtd_elev_MEAN, y = wtd_lcld_jd, color = Region)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(~Year)
  

```

Comparing modis output with snowtel.

```{r snow exploration, eval = FALSE}

#these sort of match up to hillside snowtel (2006-2019) april 1st swe 
# lowest years 2015, 2014, 2019
# biggest years 2012 by a lot, then 2007-2010 all about the same
cov.all %>% 
  group_by(Region, Year) %>% 
  summarize(meanlcld = mean(wtd_lcld_jd)) %>% 
  pivot_wider(names_from = Year, values_from = meanlcld, names_sort = TRUE)

cov.all %>% 
  ggplot() +
  geom_freqpoly(aes(x = wtd_lcld_jd, after_stat(density), color = Region)) +
  facet_wrap(~Year)


#egegik is here, but something is happening w merge.
cov.all %>% distinct(SiteID) %>% arrange(SiteID) 
```

## Combine SiteIDs 

Generate a data frame that associates the SiteIDs in the response data frame with the covariate data frame. Tim's response data frame drops > 70 sites because they had limited summer data or data were prior to 2011 (he ran 2011-2020). 

Used the original model data frame sent to Tim (stream + air temps) and generate lookup for merging the SiteIDs. Then apply this to the response data frame with TS. We added prefixes to most of the SiteIDs when generating the data that didn't get carried over to the spatial data frames. 

* find the SiteIDs that don't match in the two data frames (109) -- make sure all are converted to lower case because that is causing some mismatches.
* remove the prefix -- for just the mismatched sites -- so that that spatial SiteIDs will join.
* 13 sites in the response data frame not in the covariate data frame -- see [google sheet](https://docs.google.com/spreadsheets/d/1TJrNIwr14HJ4QV6-fB4ubmi0BoTNCG7UxoK-s--IsJM/edit#gid=1942759492) for how to fix them. 


```{r create siteid_join data frame, include = FALSE, eval = FALSE}

temp_dat <- readRDS("data_preparation/final_data/summer_data_wair2021-11-23.rds") %>% 
  mutate(site_lower = tolower(SiteID))

temp_md <- readRDS("data_preparation/final_data/md.rds")

cov.all <- cov.all %>% 
  mutate(site_lower = tolower(SiteID))

#sites in temperature data frame not in covariate data frame
remove_prefix <- anti_join(temp_dat %>% distinct(site_lower),
          cov.all %>% distinct(site_lower)) %>%
  arrange(site_lower) %>% 
  pull(site_lower)

siteid_join <- temp_dat %>% 
  mutate(cov_id = case_when(site_lower %in% remove_prefix ~  str_remove(site_lower, pattern = "^.*?(_|-)"),
                            TRUE ~ site_lower)) %>% 
  distinct(ts_id = site_lower, cov_id)

#13 mismatches
# anti_join(siteid_join, cov.all, by = c("cov_id" = "site_lower"))

siteid_join <- siteid_join %>% 
  mutate(cov_id = case_when(cov_id == "katm_lbrooo" ~ "katm_lbrooo_lvl",
                            cov_id == "katm_naknlo" ~ "katm_naknlo_lvl",
                            cov_id == "lacl_kijilo" ~ "lacl_kijilo_lvl",
                            cov_id == "lslil10" ~ "lslil",
                            cov_id == "lsnlt10" ~ "lsnlt1",
                            cov_id == "lsr93" ~ "lsr93lb",
                            cov_id == "lsr106" ~ "lsr112.5",
                            cov_id == "lacl_lclaro" ~ "lacl_lclaro_lvl",
                            TRUE ~ cov_id))

write.csv(siteid_join, "data_preparation/SiteID_join_spatial_to_temperature.csv")
```

Lots of sites in covariates not in response data frame -- check on how Tim filtered data completeness, which should explain why sites were dropped. For each site and year, Tim dropped any with less than 80% of days in June 1 - Aug 31 (jd 152-243). And, he only used sites with data from 2011-2020.

```{r sites dropped in dfa, eval = FALSE}

temp_dat %>% 
  # filter(year(sampleDate) %in% 2001:2019) %>% 
  distinct(SiteID) %>% nrow()
ts_dat %>% distinct(Site) %>% nrow() #dropped 50 overall, 27 when filter on year only, so other 23 presumably didn't have complete summers of data

anti_join(temp_dat %>% distinct(SiteID), ts_dat %>% distinct(SiteID = Site))

# 49 sites
temp_dat %>%
  mutate(Year = year(sampleDate), jd = as.numeric(format(sampleDate, "%j"))) %>% 
  filter(jd > 151, jd < 244) %>% 
  group_by(SiteID, Year) %>% 
  summarize(sum_percent = n()/92) %>%
  mutate(keep = case_when(sum_percent >= 0.8 ~ 1,
                          Year < 2011 ~ 0,
                          Year > 2019 ~ 0,
                          TRUE ~ 0)) %>% 
  group_by(SiteID) %>% 
  summarize(sum_complete = sum(keep)) %>% 
  filter(sum_complete == 0)
```


Sent the merge table to Dustin and he added the ts site ids (ts_id_dm) to the spatial data so now everything is merging correctly. Still need to filter the TS SiteIDs sent to Tim to lower case and remove 2020 data since we didn't have snow metrics for that year.

```{r create model data frame, include = FALSE}

mod_dat <- dfa_ts %>% 
  mutate(site_lower = tolower(SiteID)) %>% 
  filter(Year %in% 2011:2019) %>% 
  left_join(cov.all %>% select(-SiteID), by = c("site_lower" = "ts_id", "Year" = "Year")) 

```


## Add in daymet precipitation

Tim C. included total summer precipitation as a predictor in his models so add from daymet here. Note that this is site specific precip, not from across the watershed.

```{r merge with precip}

daymet <- read_csv("data_preparation/site_daymet_2011-19.csv")
daymet %>% distinct(measurement)

#correct conversion is date2 bc date example below has Jan 1 as day 0!
daymet %>% 
  filter(year == 2006, 
         yday == 152) %>% 
  mutate(date = as.Date(yday, origin = paste0(year, "-01-01")),
         date2 = as.Date(paste(year, yday), format = "%Y %j"))

precip <- daymet %>%
  filter(measurement %in% c("prcp..mm.day.")) %>% 
  mutate(sampleDate = as.Date(paste(year, yday), format = "%Y %j")) %>% 
  filter(month(sampleDate) %in% 6:8) %>% 
  rename(Site = site, Year = year) %>% 
  group_by(Site, Year) %>% 
  summarize(summer_precip = sum(value))

mod_dat <- left_join(mod_dat %>% rename(Site = SiteID) %>% select(-site_lower), precip) 
summary(mod_dat)

rm(daymet)

```



# Data exploration

## Response

Look at sites with lots of negative TS as a check. original list of sites using just air temperature and no daylength is much different than model with both in it. For now, leave all sites. Note that some of the lake affected sites may be very difficult to model TS for since lake temperatures can change so quickly. Also note that we have some of Luca's sites in this data frame that he suggested would be difficult to model in a landscape context because of local hydrologic inputs. We could revisit the final list of sites we have from him and see if some should be dropped. Possibly this could be done with the cross-validation in the final model --- take some time to look over sites that are extreme outliers in that step (e.g. high rmse).

```{r negative ts values}

left_join(mod_dat, md %>% select(Site, Waterbody_name, SourceName)) %>% 
  filter(TempSens_Air.DayLen < 0) %>% 
  group_by(Site, Waterbody_name) %>%
  summarize(n = n(), mean_ts = mean(TempSens_Air.DayLen)) %>% 
  arrange(desc(n))

  
# drop_sites <- c("NPS_Hardenburg_Bay", "usgs_15258000", "usgs_15260001", "USFS_Middle Arm Eyak")
# 
# mod_dat2 <- mod_dat %>% 
#   filter(!Site %in% drop_sites)


```

what about other sites in Luca's dataset? We kept some that are not ones he originally flagged to include in a landscape analysis. (see comment above)

* middle fork 18 mile
* 24.9 mile creek
* ibeck lower side channel
* eagle creek
* pigot bay spawn channel 
* rude river side channel 


Any outliers in TS? 

```{r ts boxplot by region}
mod_dat %>% 
  ggplot(aes(x = Region, y = TempSens_Air.DayLen)) +
  geom_boxplot()
```


```{r ts dotplot}
mod_dat %>% 
  arrange(TempSens_Air.DayLen) %>% 
  mutate(rowid = row_number()) %>%
  ggplot(aes(x = TempSens_Air.DayLen, y = rowid,  color = Region)) +
  geom_point() +
  # scale_y_continuous(limits = c(0, 1500)) +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        legend.position = "bottom", axis.title.y = element_blank())
```

some sites with high TS.

```{r}
mod_dat %>% 
  filter(TempSens_Air.DayLen > 0.7)
```

```{r ts histogram}
mod_dat %>% 
  ggplot(aes(x = TempSens_Air.DayLen)) +
  geom_histogram() +
  facet_wrap(~Region) +
  theme_bw()

```

## Covariates 

Checking outliers and collinearity. Drop those with high correlation.

Check slopes first. A couple of outliers -- one for stream slope and two for minimum catchment slope. We checked the max catchment slopes and they are accurate, streams in very steep watersheds. One of those also had the high stream slope. Note that the mean, min, max statistics for slope and elevation by watershed are all highly correlated so just go with means for now and recheck using r and vif.

Keep:

* stream slope
* mean catchment slope and elevation
* mean watershed slope and elevation
* watershed percent north facing 
* distance to coast
* watershed area
* wetland, glacier, lake cover
* modis snow date

```{r pair plots for groups of covariates}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}

#slope and aspect
mod_dat %>% 
  select(str_slope, cat_slope_MIN:cat_slope_STD, wtd_slope_MIN:wtd_slope_STD, wtd_north_per) %>% 
  pairs(upper.panel = panel.cor)

#elevation, area, and distance to coast
mod_dat %>% 
  select(cat_elev_MIN:cat_elev_STD, wtd_elev_MIN:wtd_elev_STD, wtd_area_sqKM, dist_coast_km) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist)

#landcover and snow date
mod_dat %>% 
  select(wtd_wet_per:summer_precip, snow_tc, cat_elev_MEAN, wtd_slope_MEAN) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist)

mod_dat %>% 
  select(wtd_wet_per, snow_tc,wtd_lcld_jd, cat_elev_MEAN, wtd_elev_MEAN, dist_coast_km, wtd_slope_MEAN) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist)

```


Correlations for all covariates in list above. All r < 0.7, which is good. Mean catchment elevation is correlated to mean watershed elevation and distance to coast (r = 0.64)

```{r pair plot for 13 cov}
mod_dat %>% 
  select(str_slope, cat_elev_MEAN, cat_slope_MEAN, wtd_north_per,
         wtd_elev_MEAN, wtd_slope_MEAN, wtd_area_sqKM, dist_coast_km,
         wtd_wet_per, wtd_glacier_per, wtd_lake_per, snow_tc, summer_precip) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist)
```

All VIF < 5.

```{r vif for 13 cov}

lm1 <- mod_dat %>% 
  select(str_slope, cat_elev_MEAN, cat_slope_MEAN, wtd_north_per,
         wtd_elev_MEAN, wtd_slope_MEAN, wtd_area_sqKM, dist_coast_km,
         wtd_wet_per, wtd_glacier_per, wtd_lake_per, wtd_lcld_jd, summer_precip, TempSens_Air) %>% 
  lm(TempSens_Air ~ ., data = .)

vif(lm1)
```

MODIS lcld as a function of watershed slope (Fig 2C in Tim's paper).

```{r lcld versus watershed slope}
mod_dat %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = wtd_lcld_jd)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ I(x^2))
```

Generate snow index as residuals of model of lcld as a function of watershed slope (per Tim's paper). Compare squared watershed slope versus linear model using AIC. Linear model preferred - delta AIC = 19.

```{r snow index}
mod_dat %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = wtd_lcld_jd)) +
  geom_point() +
  geom_smooth(method = "lm") +
  geom_smooth(color = "red", formula = y ~ x^2)


ess_lme1 <- lme(wtd_lcld_jd ~ wtd_slope_MEAN, dat = mod_dat, random = ~1|Year)
ess_lme2 <- lme(wtd_lcld_jd ~ I(wtd_slope_MEAN^2), dat = mod_dat, random = ~1|Year)

ess_lme3 <- lme(wtd_lcld_jd ~ wtd_slope_MEAN + wtd_north_per, dat = mod_dat, random = ~1|Year)

AIC(ess_lme1, ess_lme2, ess_lme3)

ess_lme_resid <- residuals(ess_lme1, level = 1) 
attributes(ess_lme_resid) <- NULL
str(ess_lme_resid)

ess_lme_resid2 <- residuals(ess_lme2, level = 1) 
attributes(ess_lme_resid) <- NULL
str(ess_lme_resid)

ess_lme1_ints <- ess_lme1$coefficients$random$Year %>% as_tibble(rownames = "Year") %>% 
  rename(intercept = `(Intercept)`) %>% 
  mutate(Year = as.numeric(Year))

mod_dat <- mod_dat %>% 
  left_join(ess_lme1_ints) %>% 
  mutate(snow_ind = ess_lme_resid,
         snow_ind_sq = ess_lme_resid2,
         snow_tc = intercept + (wtd_lcld_jd - 1.783443 * wtd_slope_MEAN))

#check that snow index is now not correlated to watershed slope.
mod_dat %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = snow_tc)) +
  geom_point() +
  geom_smooth(method = "lm")

mod_dat %>% 
  ggplot(aes(x = snow_ind, y = snow_tc)) +
  geom_point() +
  geom_smooth(method = "lm")

mod_dat %>% 
  ggplot(aes(x = wtd_lcld_jd, y = snow_tc)) +
  geom_point() +
  geom_smooth(method = "lm")


mod_dat %>% 
  filter(grepl("Hansen", Site)) %>% 
  select(Year, wtd_slope_MEAN, wtd_lcld_jd, snow_ind, snow_tc) %>% 
  arrange(Year)


```


Logit transformation of glacier and lake cover because both are percentages and have large outliers. Log transform watershed area, stream slope, catchment elevation and slope, and summer precip. Transformation for slope needs minimum value because one 0 (following methods in mccune and grace 2002).

First checking skewness, looking for abs skew < 1 in continuous covariates.


```{r covariate transformations}

mod_dat %>% 
  select(str_slope, cat_elev_MEAN, cat_slope_MEAN, wtd_north_per,
         wtd_elev_MEAN, wtd_slope_MEAN, wtd_area_sqKM, dist_coast_km,
         wtd_wet_per, wtd_glacier_per, wtd_lake_per, snow_ind, summer_precip) %>% 
  apply(., 2, function(x) skewness(x))

mod_dat %>%
  distinct(str_slope) %>%
  arrange(str_slope)

min_slope = 0.0000100000
c_slope = as.integer(log10(min_slope))
d_slope = 10^c_slope

mod_dat <- mod_dat %>% 
  mutate(log_area = log10(wtd_area_sqKM),
         log_slope = log10(str_slope + d_slope) - c_slope,
         asrt_glac = asin(sqrt(wtd_glacier_per/100)),
         asrt_lake = asin(sqrt(wtd_lake_per/100)),
         asrt_wet = asin(sqrt(wtd_wet_per/100)),
         glac_10 = case_when(wtd_glacier_per > 0 ~ 1,
                             TRUE ~ 0),
         log_cat_elev = log10(cat_elev_MEAN),
         log_cat_slope = log10(cat_slope_MEAN),
         log_precip = log10(summer_precip))

#does this change pairwise correlations? NO
mod_dat %>% 
  select(log_slope, log_cat_elev, log_cat_slope, wtd_north_per,
         wtd_elev_MEAN, wtd_slope_MEAN, log_area, dist_coast_km,
         snow_ind, asrt_glac, asrt_lake, asrt_wet, glac_10, log_precip) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist)


#glacier cover still skewed, but shouldn't be a problem with ML.
mod_dat %>% 
  select(log_slope, log_cat_elev, log_cat_slope, wtd_north_per,
         wtd_elev_MEAN, wtd_slope_MEAN, log_area, dist_coast_km,
         snow_ind, asrt_glac, asrt_lake, asrt_wet, glac_10, log_precip) %>% 
  apply(., 2, function(x) skewness(x))


```

Still one site in Bristol Bay -- Egegik River -- with very high lake cover not fixed by logit transformation. ~37%, next highest is 17%.

```{r lake outlier}
mod_dat %>% filter(asrt_lake > .5)

md %>% 
  filter(Site == "fws_580223156504200")

```


Set up a basic model to get VIF. 

```{r vif for 14 cov}
lm1 <- lm(TempSens_Air ~ log_slope + log_cat_elev + log_cat_slope + wtd_north_per +
         wtd_elev_MEAN + wtd_slope_MEAN + log_area + dist_coast_km + 
         asrt_wet + asrt_glac + asrt_lake + glac_10 + snow_ind + log_precip, data = mod_dat)

vif(lm1)
```

Remove mean watershed elevation now all vif < 3 or close to it. cat elev is highest at 3.12 and the only one above 3.

```{r vif 13 cov}
lm2 <- lm(TempSens_Air ~ log_slope + log_cat_elev + log_cat_slope + wtd_north_per +
         wtd_slope_MEAN + log_area + dist_coast_km + 
         asrt_wet + asrt_glac + asrt_lake + glac_10 + snow_ind + log_precip, data = mod_dat)


vif(lm2)
```

Save model data frame with snow index and covariate transformations.

```{r save mod_dat data frame}

saveRDS(mod_dat, paste("data_preparation/final_data/model_data", Sys.Date(), ".rds", sep = ""))


```




# EVERYTHING BELOW NOT RUN

4/25/22 Note: reran all the above with new DFA TS from models with and without daylength. Added in the snow index and the transformations although moving forward with a BRT model in script 2c so not actually needed. These were originally setup for the multiple regression models, but decided to pursue ML method instead. All covariates have pairwise correlations < 0.7 and VIF < 5.

## Cook Inlet sites

Look at Sue's sites and years and different scenarios - 

```{r cik sites and years}
mod_dat %>% 
  distinct(Site, Year, Waterbody_name) %>% 
  filter(grepl("CIK", Site), !Site %in% c("CIK1", "CIK2", "CIK3", "CIK4", "CIK6")) %>%
  group_by(Site, Waterbody_name) %>% 
  summarize(years = paste0(Year, collapse = ", "))
```

```{r cik ts by year}
mod_dat %>% 
  filter(grepl("CIK_", Site)) %>% 
  ggplot(aes(x = Year, y = TempSens, group = Site)) +
  geom_line() +
  scale_x_continuous(breaks = c(seq(2000, 2019, 2))) 
```


```{r cik ts boxplots}

mod_dat %>% 
  filter(grepl("Moose", Site))

mod_dat %>% 
  filter(grepl("CIK_", Site)) %>% 
  group_by(Site) %>% 
  dplyr::mutate(yr_ct = n(),
         name_ct = paste(Waterbody_name, yr_ct, sep = "-")) %>% 
  filter(yr_ct > 4) %>% 
  ggplot(aes(y = name_ct, x = TempSens)) +
  geom_boxplot() 
```

```{r}

mod_dat %>% 
  filter(Site %in% c("CIK_14", "CIK_3", "CIK_30", "CIK_35", "CIK_38", "CIK_4", "CIK_6", "CIK_8")) %>% 
  group_by(Site) %>% 
  mutate(ts_sc = scale(TempSens)) %>% 
  ggplot(aes(x = Year, y = ts_sc, color = Site)) +
  geom_line() +
  geom_point() +
  geom_hline(aes(yintercept = 0)) +
  scale_x_continuous(breaks = c(seq(2000, 2019, 2))) 
```

```{r}
mod_dat %>% 
  filter(Site %in% c("CIK_14", "CIK_3", "CIK_30", "CIK_35", "CIK_38", "CIK_4", "CIK_6", "CIK_8")) %>% 
  # group_by(Site) %>% 
  # mutate(ts_sc = scale(TempSens)) %>% 
  ggplot(aes(x = summer_precip, y = TempSens)) +
  geom_smooth() +
  geom_point(aes(size = wtd_slope_MEAN)) +
  facet_wrap(~Waterbody_name, scales = "free")

```
```{r}

scale_var <- function(x){
  (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)
}


mod_dat %>% 
  filter(Site %in% c("CIK_14", "CIK_3", "CIK_30", "CIK_35", "CIK_38", "CIK_4", "CIK_6", "CIK_8")) %>% 
  group_by(Site) %>% 
  dplyr::mutate(ts_sc = scale_var(TempSens),
         lcld_sc = scale_var(wtd_lcld_jd),
         prp_sc = scale_var(summer_precip)) %>%
  ggplot() +
  geom_line(aes(x = Year, y = ts_sc), color = "red") +
  geom_line(aes(x = Year, y = lcld_sc), color = "blue") +
  geom_line(aes(x = Year, y = prp_sc), color = "dark green") +
  geom_hline(aes(yintercept = 0)) +
  facet_wrap(~Waterbody_name) +
  scale_x_continuous(breaks = c(seq(2000, 2019, 2))) +
  theme_bw()


```



# Exploring relationships with TS

Tim reran using an AR1 model (time series model with lag of 1 day) because the Deshka sites were confounding the analysis in 2017-2019. The common trend was air temperatures and the TS were super low, which is not what we want. Read in new results, which has AR1 results linked to covariate data. This was rerun to include all sites and years from 2001-2019, which are the years covered by the MODIS snow metrics. 

Boxplots of TS by region and year. Note that 2019 does not appear to be an outlier for TS.

```{r}
mod_dat <- readRDS("data_preparation/final_data/model_data2022-02-08.rds") 

mod_dat %>% 
  ggplot(aes(x = as.factor(Year), y = TempSens)) +
  geom_boxplot() +
  facet_wrap(~Region) +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(x = "Year")
```

Boxplots of all streams by year. Pick two years in the 2011-2019 time period (most sites in these years) to map TS.

* 2019 still a good year with 2nd highest median TS.
* 2013 was a high snow year in both BB and CI, unfortunately, this is before the Kodiak network started.

```{r}
mod_dat %>% 
  ggplot(aes(y = fct_reorder(as.factor(Year), TempSens), x = TempSens)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(x = "Thermal sensitivity")
```


Plot comparing SD of TS to mean of TS for sites with at least 5 years of data.

```{r}
mod_dat %>% 
  group_by(Region, Site) %>% 
  mutate(count = n()) %>% 
  filter(count > 4) %>% 
  summarize(meants = mean(TempSens),
            sdts = sd(TempSens)) %>% 
  ggplot(aes(x = meants, y = sdts, color = Region)) +
  geom_point()
```

Variability in TS from year to year for sites with at least 5 years of data.

```{r}
mod_dat %>% 
  group_by(Region, Site) %>% 
  mutate(count = n()) %>% 
  filter(count > 4) %>% 
  ggplot(aes(x = TempSens, y = Site)) +
  geom_point() +
  facet_wrap(~Region, scales = "free_y", ncol = 1)

mod_dat %>% 
  group_by(Region, Site) %>% 
  mutate(count = n()) %>% 
  filter(count > 4) %>% 
  summarize(ts_range = range(TempSens)) %>% 
  ggplot() +
  geom_histogram(aes(x = ts_range))

mod_dat %>% 
  group_by(Region, Site, wtd_elev_MEAN) %>% 
  mutate(count = n()) %>% 
  filter(count > 4) %>% 
  summarize(ts_range = range(TempSens)) %>% 
  ggplot() +
  geom_point(aes(x = ts_range, y = wtd_elev_MEAN))

mod_dat %>% 
  group_by(Region, Site, cat_elev_MEAN) %>% 
  mutate(count = n()) %>% 
  filter(count > 4) %>% 
  summarize(ts_range = range(TempSens)) %>% 
  ggplot() +
  geom_point(aes(x = ts_range, y = cat_elev_MEAN))

mod_dat %>% 
  ggplot() +
  geom_histogram(aes(x = TempSens))
```



## Pairwise correlations between TS and covariates

Pairplots for covariates versus TS.

```{r pair plots against TS}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}

mod_dat %>% 
  select(TempSens, log_slope, cat_elev_MEAN, cat_slope_MEAN, 
         wtd_slope_MEAN, snow_ind, summer_precip) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist, lower.panel = panel.smooth)

mod_dat %>% 
  select(TempSens, log_area, dist_coast_km, wtd_north_per,
         wtd_wet_per, logit_glac, logit_lake) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist, lower.panel = panel.smooth)

```

## Exploring interactions 


Tim's results: north x snow, north x slope, north x precip, precip x slope, slope x snow.

Facets are snow index and x is north aspect, no obvious relationship to TS.

```{r north and snow}
ggplot(transform(mod_dat, fct = cut(snow_ind, seq(from = min(snow_ind)-1, to = max(snow_ind) +1, length.out = 6))), 
       aes(y = TempSens, x = wtd_north_per)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by north aspect and snow index", subtitle = "Facets are bins of snow index")

```

Facets are snow index and x is watershed slope, can still see negative relationship between slope and TS, but no interaction with snow.

```{r slope and snow}
ggplot(transform(mod_dat, fct = cut(snow_ind, seq(from = min(snow_ind)-1, to = max(snow_ind) +1, length.out = 6))), 
       aes(y = TempSens, x = wtd_slope_MEAN)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by slope and snow index", subtitle = "Facets are bins of snow index")

```

Interesting! Can start to see negative relationship of aspect with TS, but only at highest slope watersheds, which makes sense.

```{r ts by north and wtd slope}
summary(mod_dat)

facet_var <- "wtd_slope_MEAN"

ggplot(transform(mod_dat, fct = cut(get(facet_var), seq(from = min(get(facet_var)) - 0.01, to = max(get(facet_var)), length.out = 6))), 
       aes(y = TempSens, x = wtd_north_per)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed slope and north aspect", subtitle = paste0("Facets are bins of ", facet_var))
```

No interaction.

```{r ts by north and precip}

ggplot(transform(mod_dat, fct = cut(summer_precip, seq(from = min(summer_precip), to = max(summer_precip), length.out = 6))), 
       aes(y = TempSens, x = wtd_north_per)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by summer precip and north aspect", subtitle = "Facets are bins of summer precip")
```


No real evidence of an interaction between precip and slope.

```{r ts by precip and wtd slope}

ggplot(transform(mod_dat, fct = cut(wtd_slope_MEAN, seq(from = 0, to = 32, length.out = 6))), 
       aes(y = TempSens, x = summer_precip)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed slope and precip", subtitle = "Facets are bins of mean watershed slope")
```


TS by glacier cover and mean watershed slope. Low slope watersheds generally have no or very low glacier cover. In watersheds in bins 3 and 4, can see expected negative relationship between glacier cover and TS. 

```{r ts by glacier and wtd slope}

ggplot(transform(mod_dat, fct = cut(wtd_slope_MEAN, seq(from = 0, to = 32, length.out = 6))), 
       aes(y = TempSens, x = wtd_glacier_per)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed slope and glacier cover", subtitle = "Facets are bins of mean watershed slope")
```

TS by glacier cover and watershed area. Glacier cover decreases TS over all sizes of watersheds.

```{r ts by glacier and wtd area}

ggplot(transform(mod_dat, fct = cut(wtd_area_sqKM, breaks = quantile(wtd_area_sqKM))), 
       aes(y = TempSens, x = wtd_glacier_per)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed area and glacier cover", subtitle = "Facets are quantiles of mean watershed area")

```

TS by lake cover and mean watershed slope. No obvious relationship between TS and lake cover. 

```{r ts by lakes and wtd slope}

ggplot(transform(mod_dat, fct = cut(wtd_slope_MEAN, seq(from = 0, to = 32, length.out = 6))), 
       aes(y = TempSens, x = wtd_lake_per)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed slope and lake cover", subtitle = "Facets are bins of mean watershed slope")
```

TS by lake cover and watershed area. 

```{r ts by lake and wtd area}

ggplot(transform(mod_dat, fct = cut(wtd_area_sqKM, breaks = quantile(wtd_area_sqKM))), 
       aes(y = TempSens, x = wtd_lake_per)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed area and lake cover", subtitle = "Facets are quantiles of mean watershed area")

```

TS by precip and watershed area. 

```{r ts by precip and wtd area}

ggplot(transform(mod_dat, fct = cut(wtd_area_sqKM, breaks = quantile(wtd_area_sqKM))), 
       aes(y = TempSens, x = summer_precip)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed area and precipitation", subtitle = "Facets are quantiles of mean watershed area")

```



Bring in metadata which has field for Deshka sites and also lat/longs.

```{r include = FALSE}
md <- read_rds("data_preparation/final_data/md.rds")

md %>% summary()

mod_dat <- left_join(mod_dat, md %>% select(SiteID, deshka, HUC8, Name, Latitude, Longitude), by = c("Site" = "SiteID")) 

mod_dat %>% 
  filter(deshka == 1) %>% 
  ggplot(aes(x = TempSens, y = fct_reorder(Site, TempSens), color = as.factor(Year))) +
  geom_point()
```



```{r, spatial data, include = FALSE}
mod_sf <- st_as_sf(mod_dat, coords = c("Longitude", "Latitude"), crs = "WGS84")
huc8 <- st_read(dsn = "S:/Leslie/GIS/NHD Harmonized/WBD_19_GDB.gdb", layer = "WBDHU8")  
huc8_wgs84 <- st_transform(huc8, crs = "WGS84")

huc8_sa <- st_filter(huc8_wgs84, mod_sf)

regions <- huc8_sa %>% 
  mutate(Region = case_when(HUC8 %in% 19020301:19020602 ~ "Cook Inlet",
                            HUC8 == 19020701 ~ "Kodiak",
                            HUC8 %in% 19020101:19020104 ~ "Copper",
                            HUC8 %in% 19020201:19020203 ~ "Prince William Sound",
                            TRUE ~ "Bristol Bay")) %>% 
  group_by(Region) %>% 
  summarize()
```

Plot the TS for 2019 (high) and 2013 (low) years.

```{r static map of TS, fig.width=16, fig.height=14}

ggplot() +
  geom_sf(data = regions, aes(fill = Region), alpha = 0.1) +
  # geom_sf(data = huc8_sa) +
  geom_sf(data = mod_sf %>% filter(Year %in% c(2019, 2013)), aes(color = TempSens), size = 3) +
  facet_wrap(~Year, ncol =1) +
  scale_color_viridis_c() +
  theme_bw() +
  theme(legend.position = "bottom", text = element_text(size = 14)) +
  guides(fill = guide_legend(nrow = 2))

```


Set up an interactive mapper with a data summary that can be used as a popup.

- spatial covariates (just wtd slope right now)
- TS for 2019 and 2013 as high and low years

```{r}
map.dat <- mod_dat %>% 
  select(Site, Year, TempSens, Region:dist_coast_km, wtd_elev_MEAN, wtd_slope_MEAN, wtd_north_per:wtd_area_sqKM) %>% 
  filter(Year %in% c(2013, 2019)) %>% 
  pivot_wider(names_from = Year, values_from = TempSens) %>% 
  left_join(md %>% select(SiteID, deshka, HUC8, Name, Latitude, Longitude, Waterbody_name, SourceName), by = c("Site" = "SiteID")) %>% 
  st_as_sf(coords = c("Longitude", "Latitude"), crs = "WGS84")

```

Save version for arcgis online mapper that Dustin can link to points. Add spatial ID to this output so he only has to do one join.

```{r}
spatial_join <- read_csv("data_preparation/SiteID_join_spatial_to_temperature.csv") %>% select(-X1)

mod_dat %>% 
  select(Site, Year, TempSens, Region:dist_coast_km, wtd_elev_MEAN, cat_elev_MEAN, wtd_slope_MEAN, cat_slope_MEAN, wtd_north_per:wtd_area_sqKM) %>% 
  filter(Year %in% 2001:2019) %>% 
  pivot_wider(names_from = Year, values_from = TempSens) %>% 
  mutate(site_lower = tolower(Site)) %>% 
  left_join(spatial_join, by = c("site_lower" = "ts_id")) %>% 
  write_csv("output/sites_wTScovars.csv")

```

Check why the file created above is not linking to Dustin's point file. We should have spatial locations for all 408 sites with data.





```{r tmap map}
tmap_mode("view")

sitemap <- tm_shape(regions) +
  tm_borders("black") +
  tm_shape(map.dat) +
  tm_dots(id = "Site", size = 0.05, group = "Region", col = "2019",
          popup.vars = c("Waterbody_name", "2019", "2013", "wtd_slope_MEAN", "wtd_glacier_per", "wtd_lake_per")) +
  tm_text("Site", size = 1.5, shadow = TRUE, auto.placement = TRUE,
          just = "bottom", remove.overlap = TRUE, clustering = TRUE, group = "Labels" ) +
  tm_basemap(server = c(Topo = "Esri.WorldTopoMap", Imagery = "Esri.WorldImagery" ))

sitemap

```





# Generating model subsets for new DFA by Tim

During team meeting on 2/4/22, decided to investigate DFA again after subsetting sites in Deshka. We're interested in trying out different model trends that might accommodate the different types of sites we have in the data -- both TS and loadings on residual trends could be informative for understanding TS and scenarios of change in the future.

There is already a field called deshka that flags the sites in that watershed. Add two new subsets:

* subset1 of sites in Deshka that span a gradient of watershed slope and are spread out across the three stream segments. Sites are numbered from upstream to downstream so can also use numbering to separate them.
* Two tributaries in each: deshka tribs 3 and 4, kroto tribs 1 and 3, moose tribs 2 and 8.
* Two mainstem sites in each: deshka 1 down and 4 up, kroto 2 up and 8 up, moose 2 up and 8 up. 
* CIK_28 is Sue's site at bottom of Deshka. 

```{r}
md <- readRDS("data_preparation/final_data/md.rds") %>% 
  rename(Site = SiteID)

left_join(mod_dat, md %>% select(Site, SourceName, Waterbody_name, deshka)) %>% 
  filter(deshka == 1|Site == "CIK_28") %>% 
  mutate(wtd = case_when(grepl("Moose", Site) ~ "moose",
                         grepl("Deshka", Site) ~ "deshka",
                         grepl("Kroto", Site) ~ "kroto",
                         TRUE ~ NA_character_),
         trib = case_when(grepl("Trib", Site) ~ 1,
                          TRUE ~ 0)) %>% 
  distinct(Site, wtd, trib, wtd_slope_MEAN, wtd_area_sqKM) %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = fct_reorder(Site, wtd_slope_MEAN), 
             color = as.factor(trib), size = wtd_area_sqKM)) +
  geom_point() +
  facet_wrap(~wtd, scales = "free_y")

```

```{r}
left_join(mod_dat, md %>% select(Site, SourceName, Waterbody_name, deshka)) %>% 
  filter(deshka == 1|Site == "CIK_28") %>% 
  count(Site) %>% 
  arrange(n)
```

Same dotplot of selected deshka sites.

```{r 13 deshka sites to keep}
desh_sites <- c("Deshka 3 Tributary", "Deshka 4 Tributary",
                "Kroto 1 Tributary", "Kroto 3 Tributary",
                "Moose 2 Tributary", "Moose 8 Tributary",
                "Deshka 1 Downstream", "Deshka 4 Upstream",
                "Kroto 2 Upstream", "Kroto 8 Upstream",
                "Moose 2 Upstream", "Moose 8 Upstream",
                "CIK_28")

#check that these sites have 3 years of data.
mod_dat %>% 
  filter(Site %in% desh_sites) %>% 
  count(Site) %>% 
  arrange(n)


mod_dat %>% 
  filter(Site %in% desh_sites) %>% 
  mutate(wtd = case_when(grepl("Moose", Site) ~ "moose",
                         grepl("Deshka", Site) ~ "deshka",
                         grepl("Kroto", Site) ~ "kroto",
                         TRUE ~ NA_character_),
         trib = case_when(grepl("Trib", Site) ~ 1,
                          TRUE ~ 0)) %>% 
  distinct(Site, wtd, trib, wtd_slope_MEAN, wtd_area_sqKM) %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = fct_reorder(Site, wtd_slope_MEAN), 
             color = as.factor(trib), size = wtd_area_sqKM)) +
  geom_point() 

```

```{r add subset removing 77 deshka sites}
md %>% filter(deshka == 1) %>% distinct(Site) # 89 + sue's site 

md <- md %>% 
  mutate(subset1 = case_when(deshka == 0 ~ 1,
                             Site %in% desh_sites ~ 1,
                             TRUE ~ 0)) 

md %>% count(subset1)
md %>% count(deshka)


```

Create a second subset where we remove sites that have less than 3 years of data, focusing on the period 2011-2019.

Also look at count by huc8: 

* lower susitna 19020505 has 145, removing 77
* then comes lower kenai with 56, filtering on years should remove some kbrr and apu data from the anchor.

```{r}
md %>% count(HUC8) %>% arrange(desc(n))

left_join(mod_dat, md %>% select(subset1, Site)) %>% 
  filter(Region == "Cook_Inlet", subset1 == 1) %>%
  filter(Year %in% 2011:2019) %>%
  count(Site) %>% 
  filter(n > 1)

sites_2yr <- mod_dat %>% 
  filter(Year %in% 2011:2019) %>% 
  count(Site) %>% #383 sites
  arrange(n) %>% 
  filter(n > 1) %>% #286 sites
  pull(Site)

sites_3yr <- mod_dat %>% 
  filter(Year %in% 2011:2019) %>% 
  count(Site) %>% #383 sites
  arrange(n) %>% 
  filter(n > 2) %>% #222 sites
  pull(Site)

md <- md %>% 
  mutate(subset2 = case_when(subset1 == 1 & Site %in% sites_2yr ~ 1,
                             TRUE ~ 0),
         subset3 = case_when(subset1 == 1 & Site %in% sites_3yr ~ 1,
                             TRUE ~ 0)) 
```

Plotting both subsets against watershed slope. If anything, now Bristol Bay is heavily biased by sites using the 3 year cutoff since a lot of Sue's data and sites ended in 2012.

```{r}
left_join(mod_dat %>% distinct(Site, Region, wtd_slope_MEAN), 
          md %>% select(Site, subset1, subset2)) %>% 
  filter(subset1 == 1) %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = fct_reorder(Site, wtd_slope_MEAN))) +
  geom_point() +
  facet_wrap(~Region, scales = "free_y")

left_join(mod_dat %>% distinct(Site, Region, wtd_slope_MEAN), 
          md %>% select(Site, subset1, subset2)) %>% 
  filter(subset2 == 1) %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = fct_reorder(Site, wtd_slope_MEAN))) +
  geom_point() +
  facet_wrap(~Region, scales = "free_y")

left_join(mod_dat %>% distinct(Site, Region, wtd_slope_MEAN), 
          md %>% select(Site, subset3)) %>% 
  filter(subset3 == 1) %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = fct_reorder(Site, wtd_slope_MEAN))) +
  geom_point() +
  facet_wrap(~Region, scales = "free_y")

ggsave("output/subset3 wtd slope.jpeg", width = 12, height = 10)

left_join(mod_dat, md %>% select(Site, subset1)) %>% 
  distinct(Site, Region, subset1) %>%
  filter(subset1 == 1)
            count(subset1, Region) #146 in cook inlet
left_join(mod_dat, md %>% select(Site, subset2)) %>% 
  distinct(Site, Region, subset2) %>%
  filter(subset2==1)
            count(subset2, Region) #78 in cook inlet
left_join(mod_dat, md %>% select(Site, subset3)) %>% 
  distinct(Site, Region, subset3) %>% 
  # filter(subset3 == 1)
            count(subset3, Region) #53 in cook inlet

```

Save metadata file with data subsets.

```{r save md with subsets 123}

saveRDS(md, paste0("data_preparation/final_data/md_", Sys.Date(), ".rds"))        

write_csv(md, paste0("data_preparation/final_data/md_", Sys.Date(), ".csv"))


```


# DFA February 2022 - data subsets 

Tim reran the DFA using the different subsets created above and models with 1-3 trends. Email from him with results:

"The DFA analysis have finished running. I went ahead and ran 1-3 trend DFA’s for each of the three subsets. I created some pdf’s that show the trends and the loadings to evaluate if things had improved using the subsets. I’m having some issues with GIT so I couldn’t push the requisite files (probably owing to my noob git status). I saved the Temperature Sensitivity and Trend Loading parameters in RDS files by data subset (i.e., ‘GlobalResults_Subset1’ and ‘GlobalTrends_Subset1’). They are still on the VM so hopefully you all can grab them.

Here are a couple of observations:
* Its common/unsurprising that DFA’s with more trends are favored by AIC model selection. However, these multiple trend models are less interesting to our questions as they tend to have the same issue we were having in that some of the common trends are soaking up the air temperature variation. In many systems the temp sensitivity parameters decline with increasing numbers of trends (which indicates their variation is going to trends).
* The single trend models look improved across the subsets so these maybe useful as per our original intentions.
* I think it will be interesting to look at the distributions of temperature sensitivity again and the relations between trend loadings and geomorphic attributes."

This is complicated. We now have results for 

* 3 data subsets: subset 1 removed 65 deshka sites, subset 2 further removed sites with only 1 year of data, subset 3 removed sites with only 1-2 years of data.
* for each subset, Tim ran DFA models by year for 1, 2, and 3 trends.
* for each DFA, there are TS and trends.

## Subset 1 DFA results


```{r dfa results}
dfa_ss1 <- readRDS("DFA/GlobalResults_Subset1.RDS")
dfa_ss2 <- readRDS("DFA/GlobalResults_Subset2.RDS")
dfa_ss3 <- readRDS("DFA/GlobalResults_Subset3.RDS")
md <- readRDS("data_preparation/final_data/md_2022-02-08.rds")    

dfa_ss1_wd <- left_join(dfa_ss1 %>% 
            select(SiteID:TempSens_3) %>% 
            pivot_longer(cols = TempSens_1:TempSens_3,
                         names_to = "trends",
                         values_to = "TempSens") %>% 
            mutate(trends = as.numeric(substr(trends, 10, 10))),
          dfa_ss1 %>% 
            select(SiteID, Year, TrendLoad_1_1:TrendLoad_3_3) %>% 
            pivot_longer(cols = TrendLoad_1_1:TrendLoad_3_3,
                         names_to = "name",
                         values_to = "Loadings") %>% 
            mutate(trends = as.numeric(substr(name, 11, 11)),
                   trend_no = as.numeric(substr(name, 13,13)))) %>% 
              mutate(subset = 1) %>% 
  left_join(md %>% select(SiteID = Site, Region, Waterbody_name))

dfa_ss2_wd <- left_join(dfa_ss2 %>% 
            select(SiteID:TempSens_3) %>% 
            pivot_longer(cols = TempSens_1:TempSens_3,
                         names_to = "trends",
                         values_to = "TempSens") %>% 
            mutate(trends = as.numeric(substr(trends, 10, 10))),
          dfa_ss2 %>% 
            select(SiteID, Year, TrendLoad_1_1:TrendLoad_3_3) %>% 
            pivot_longer(cols = TrendLoad_1_1:TrendLoad_3_3,
                         names_to = "name",
                         values_to = "Loadings") %>% 
            mutate(trends = as.numeric(substr(name, 11, 11)),
                   trend_no = as.numeric(substr(name, 13,13)))) %>% 
              mutate(subset = 2) %>% 
  left_join(md %>% select(SiteID = Site, Region, Waterbody_name))

dfa_ss3_wd <- left_join(dfa_ss3 %>% 
            select(SiteID:TempSens_3) %>% 
            pivot_longer(cols = TempSens_1:TempSens_3,
                         names_to = "trends",
                         values_to = "TempSens") %>% 
            mutate(trends = as.numeric(substr(trends, 10, 10))),
          dfa_ss3 %>% 
            select(SiteID, Year, TrendLoad_1_1:TrendLoad_3_3) %>% 
            pivot_longer(cols = TrendLoad_1_1:TrendLoad_3_3,
                         names_to = "name",
                         values_to = "Loadings") %>% 
            mutate(trends = as.numeric(substr(name, 11, 11)),
                   trend_no = as.numeric(substr(name, 13,13)))) %>% 
              mutate(subset = 3) %>% 
  left_join(md %>% select(SiteID = Site, Region, Waterbody_name))

dfa_results <- bind_rows(dfa_ss1_wd, dfa_ss2_wd, dfa_ss3_wd)
dfa_results %>% distinct(subset)

saveRDS(dfa_results, "output/dfa_results.rds")
```

```{r trend results}
trends_ss1 <- readRDS("DFA/GlobalTrends_Subset1.RDS")
trends_ss2 <- readRDS("DFA/GlobalTrends_Subset2.RDS")
trends_ss3 <- readRDS("DFA/GlobalTrends_Subset3.RDS")

trend_results <- bind_rows(
  trends_ss1 %>% 
    pivot_longer(cols = Trend_1_1:Trend_3_3,
                 names_to = "name",
                 values_to = "value") %>% 
    mutate(trends = as.numeric(substr(name, 7, 7)),
           trend_no = as.numeric(substr(name, 9, 9)),
           subset = 1),
  trends_ss2 %>% 
    pivot_longer(cols = Trend_1_1:Trend_3_3,
                 names_to = "name",
                 values_to = "value") %>% 
    mutate(trends = as.numeric(substr(name, 7, 7)),
           trend_no = as.numeric(substr(name, 9, 9)),
           subset = 2),
  trends_ss3 %>% 
    pivot_longer(cols = Trend_1_1:Trend_3_3,
                 names_to = "name",
                 values_to = "value") %>% 
    mutate(trends = as.numeric(substr(name, 7, 7)),
           trend_no = as.numeric(substr(name, 9, 9)),
           subset = 3)
)

saveRDS(trend_results, "output/trend_results.rds")
```



```{r ts boxplots by number of trends}
dfa_results %>% 
  distinct(TempSens, trends, subset) %>% 
  ggplot(aes(x = as.factor(trends), y = TempSens)) +
  geom_boxplot() +
  facet_wrap(~subset)

```


```{r}
dfa_results %>% 
  filter(TempSens < 0) %>%
  select(Waterbody_name, SiteID, Year, TempSens, subset, trends) %>% 
  arrange(TempSens)

dfa_ss1_wd %>% 
  filter(Year == 2019, trends == 3, trend_no == "trend_2") %>% 
  arrange(desc(Loadings))


dfa_ss1_wd %>% 
  filter(Year == 2020, trends == 2, trend_no == "trend_1") %>% 
  arrange(desc(Loadings)) %>% 
  select(SiteID, Waterbody_name)
```



```{r ts versus trend}

dfa_results %>% 
  filter(trends == 1) %>% 
  ggplot(aes(TempSens, Loadings)) +
  geom_point(aes(color = as.factor(subset))) +
  facet_wrap(~Year) +
  labs(title = "DFA with One Trend")

dfa_results %>% 
  filter(trends == 2, trend_no == 1) %>% 
  ggplot(aes(TempSens, Loadings)) +
  geom_point(aes(color = as.factor(subset))) +
  facet_wrap(~Year) +
  labs(title = "DFA with Two Trends, Trend 1")

dfa_results %>% 
  filter(trends == 2, trend_no == 2) %>% 
  ggplot(aes(TempSens, Loadings)) +
  geom_point(aes(color = as.factor(subset))) +
  facet_wrap(~Year) +
  labs(title = "DFA with Two Trends, Trend 2")

dfa_results %>% 
  filter(trends == 3, trend_no == 1) %>% 
  ggplot(aes(TempSens, Loadings)) +
  geom_point(aes(color = as.factor(subset))) +
  facet_wrap(~Year) +
  labs(title = "DFA with Three Trends, Trend 1")

dfa_results %>% 
  filter(trends == 3, trend_no == 2) %>% 
  ggplot(aes(TempSens, Loadings)) +
  geom_point(aes(color = as.factor(subset))) +
  facet_wrap(~Year) +
  labs(title = "DFA with Three Trends, Trend 2")

dfa_results %>% 
  filter(trends == 3, trend_no == 3) %>% 
  ggplot(aes(TempSens, Loadings)) +
  geom_point(aes(color = as.factor(subset))) +
  facet_wrap(~Year) +
  labs(title = "DFA with Three Trends, Trend 3")
```

Frequency plots of thermal sensitivities by year and region.

```{r}

dfa_results %>% 
  ggplot(aes(TempSens)) +
  geom_freqpoly(aes(color = Region)) +
  facet_grid(rows = vars(Year), cols = vars(subset))


```


```{r}
trend_results %>%
  filter(trends == 1) %>% 
  ggplot(aes(x = DOY, y = value, color = as.factor(trend_no))) +
  geom_line() + 
  facet_grid(rows = vars(Year), cols = vars(subset))

trend_results %>%
  filter(trends == 2) %>% 
  ggplot(aes(x = DOY, y = value, color = as.factor(trend_no))) +
  geom_line() + 
  facet_grid(rows = vars(Year), cols = vars(subset))

trend_results %>%
  filter(trends == 3) %>% 
  ggplot(aes(x = DOY, y = value, color = as.factor(trend_no))) +
  geom_line() + 
  facet_grid(rows = vars(Year), cols = vars(subset))

```


