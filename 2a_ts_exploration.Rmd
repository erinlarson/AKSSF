---
title: "TS exploration"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(sf)
library(tmap)
library(lubridate)
library(ggpubr)
library(nlme)
library(e1071)
library(car)
```


Modeling thermal sensitivities for five regions in Southern AK.

1. Read in response and covariate data and combine to create model data frame
2. Explore TS response - removed 3 sites with very negative TS
3. Explore correlations and multi-collinearity of covariates -- only select those with pairwise correlation < 0.7 and stepwise remove covariates until all have VIF < 3
4. Transform/model covariates as needed. Arcsine square root transformation for proportional covariates since 0-1 scale. Log transformation for continuous covariates. Plotted watershed slope and LCLD (last day of the longest continuous snow season) and decided to remove effect of slope on snow and use the residual - per Tim's method -- new covariate called snow index. Check for outliers after transformations.


# Original DFA

Tim provided DFA output on November 8, 2021. Here are comments on the files from his email:
- GlobalModelEstimates_1trend_DUE.csv contains the estimated model coefficients. It has columns Region, SiteID, Year, TempSens_Z, TempSens, TrendLoad_Z, TrendLoad
- TempSens_Z are the temperature sensitivity values in Z-score space, which come directly out of the model.
- TempSens are the temperature sensitivity values that have been back-transformed to be in units of ºC water / ºC air.
- TrendLoad_Z are the loadings on the common trend in Z-score space.
- GlobalTrendEstimates_1trend_DUE.csv contains the estimated trends by year. This has columns Year, DOY, and Trend. DOY is just Julian day of year. Trend is the trend value for that DOY.

Read in final thermal sensitivities and trend loadings from global model. Tim found that regional models did not change the results.

```{r responses}

dfa_dat <- read_csv("DFA/output_8Nov21/GlobalModelEstimates_1trend_DUE.csv")
trend_dat <- read_csv("DFA/output_8Nov21/GlobalTrends_1trend_DUE.csv")

remove_lssites <- dfa_dat %>% distinct(SiteID) %>% filter(grepl("lsr", SiteID)) 
dfa_dat <- dfa_dat %>% 
  filter(!SiteID %in% remove_lssites)

```

Plot of thermal sensitivity versus loading on common trend. Tim commented that sites with low thermal sensitivities often load strongly on the common trend, these might be groundwater sites, glacial sites, etc.

```{r ts versus trend}

dfa_dat %>% 
  ggplot(aes(TempSens, TrendLoad_Z)) +
  geom_point(aes(color = Region)) +
  facet_wrap(~Year)

```

Frequency plots of thermal sensitivities by year and region.

```{r}
dfa_dat %>% 
  ggplot(aes(TempSens)) +
  geom_freqpoly(aes(color = Region)) +
  facet_wrap(~Year)

# ggsave("output/ts_region_year.jpeg", width = 9, units = "in")
```

Something strange going on in Cook Inlet in 2019, lots of site in Deshka with very low thermal sensitivities which makes no sense, these are relatively warm sites. This result does not match raw air temperature coefficients from simple lm fits.

```{r}
dfa_dat %>% 
  filter(Region == "Cook Inlet", Year == 2019, TempSens < 0.25)
```

Somethings strange going on with sites in Deshka having very low thermal sensitivities. Read in raw air-stream temperatures and plot.
Filter on time series with 70% of days from June 1 to August 31.

```{r}
dat <- readRDS("data_preparation/final_data/summer_data_wair2021-11-23.rds")

dat <- dat %>% 
  mutate(year = year(sampleDate))

jd_completeness_filter <- function(inputDat, jd_filter, completeness_filter) {
  dat <- inputDat %>% 
    mutate(jd = as.numeric(format(sampleDate, "%j")),
           year = year(sampleDate)) %>% 
    filter(jd >= 152, jd <= jd_filter) #June 1st to end date
  ndays <- max(dat$jd) - min(dat$jd)
  dat2 <- dat %>%
    group_by(SiteID, year) %>%
    mutate(completeness = n()/ndays * 100) %>% 
    filter(completeness > completeness_filter) %>% 
    ungroup()
  return(dat2)
}

dat2 <- jd_completeness_filter(dat, 243, 70)

dat %>% distinct(SiteID, year) %>% nrow()
dat2 %>% distinct(SiteID, year) %>% nrow()
```


Plot some of the deshka sites from 2019.

```{r}

dat2 %>% 
  filter(SiteID == c("Deshka 1 Downstream", "Deshka 1 Tributary"), year == 2019) %>% 
  ggplot() +
  geom_line(aes(x = sampleDate, y = meanDT)) +
  geom_line(aes(x = sampleDate, y= airDT), color = "blue") +
  facet_wrap(~SiteID)

# ggsave("output/desha_air-stream_2sites.jpeg", width = 9, units = "in")
```

Plot a couple of little susitna sites. Looks much more reasonable, lower site pretty cold in June probably due to snowmelt, then tracking air temperatures pretty well later in year. Upper site very cold and not tracking air temps.

```{r}
dat %>% filter(grepl("ls", SiteID)) %>% distinct(SiteID)

dat2 %>% 
  filter(SiteID == c("lsr28.3", "lsr112"), year == 2020) %>% 
  ggplot() +
  geom_line(aes(x = sampleDate, y = meanDT)) +
  geom_line(aes(x = sampleDate, y= airDT), color = "blue") +
  facet_wrap(~SiteID)
```

Plots of common trends: 2017-2019 are very strange and don't match general pattern of gw warming over season found in Cline et al. paper.

```{r}
trend_dat %>% 
  ggplot(aes(x = DOY, y = Trend)) +
  geom_line() + 
  facet_wrap(~Year)
```


# Read in data and combine

## ARIMA results

New AR(1) model output from Tim.

```{r responses, include = FALSE}
ts_dat <- readRDS("output/TempSens_Arima_Results_from2000_2022-01-14.RDS") %>% as_tibble()

# write_csv(ts_dat, "DFA/TempSens_Arima_Results2021.12.07.csv")

```

## Covariates

Read in covariates. Copied over from Dustin's script, but modified so that lcld is combined with the other covariates.

```{r covariates, include = FALSE}

#Data from covariates script
cov.df.raw <- read.csv(file = 'data_preparation/sensitivity_drivers/AKSSF_Covariates.csv', header = TRUE) 
#Data from spatial join script
sites.df <- read.csv(file = 'data_preparation/sensitivity_drivers/AKSSF_sites_sj_maxfac.csv', header = TRUE)
#Data from Modis Script
lcld.df <- read.csv(file = 'data_preparation/sensitivity_drivers/AKSSF_wtd_lcld_mn.csv', header = TRUE)

lcld.long <- lcld.df %>% 
  pivot_longer(cols = wtd_lcld_mn_2001:wtd_lcld_mn_2019, names_to = "metric", values_to = "wtd_lcld") %>% 
  mutate(Year = as.numeric(substr(metric, 13, 17)),
         wtd_lcld_jd = wtd_lcld - 365) %>% 
  select(-metric)

#dput(colnames(sites.df))
site_keep_cols <- c("SiteID","region", "cat_ID_con", "site_max_acc", "site_acc_sqKm",
                    "Area_km2","area_diff","size_diff", "lat", "lon",
                    'str_ord','str_slope', "dist_coast_km", "ds_dist_outlet")

sites.df <- sites.df %>% 
  mutate( region = sub("_$","",gsub('[[:digit:]"]+', '', cat_ID_con)),
  size_diff =  case_when(area_diff >0~'Larger', area_diff <0~'Smaller',
                        TRUE~'No Change')) %>% 
  subset(select = site_keep_cols )

#don't keep ds_dist_outlet bc it is incorrect for some sites/networks
# sites.df %>% 
#   ggplot(aes(x = dist_coast_km, y = ds_dist_outlet)) +
#   geom_point()
  
#dput(colnames(cov.df.raw))
wtd_keep_cols <- c("cat_ID_con", "cat_ID", "region", "cat_slope_MIN", "cat_slope_MAX",
  "cat_slope_MEAN", "cat_slope_STD", "cat_slope_SUM", "cat_elev_MIN", 
  "cat_elev_MAX", "cat_elev_MEAN", "cat_elev_STD", "wtd_elev_MIN",
  "wtd_elev_MAX", "wtd_elev_MEAN", "wtd_elev_STD", "wtd_slope_MIN",
  "wtd_slope_MAX", "wtd_slope_MEAN", "wtd_slope_STD", "wtd_north_per", 
  "wtd_wet_per","wtd_lake_per","wtd_glacier_per","wtd_area_sqKM")


cov.df <- cov.df.raw %>%
  mutate(wtd_area_sqKM = wtd_elev_AREA *1e-6) %>% 
  subset(select = wtd_keep_cols )

# Convert na to 0
cov.df[is.na(cov.df)] <- 0

cov.all <- left_join(sites.df %>% select(SiteID, cat_ID_con, Region = region, str_ord, str_slope, dist_coast_km),
                     cov.df)

cov.all <- left_join(cov.all, lcld.long)
# summary(cov.all)

```

Early exploration turned up lots of small problems that Dustin has fixed -- all recorded on [google sheet](https://docs.google.com/spreadsheets/d/1TJrNIwr14HJ4QV6-fB4ubmi0BoTNCG7UxoK-s--IsJM/edit#gid=624864644), QA review tab.

```{r covariate exploration, eval = FALSE}

names(cov.all)

cov.all %>% 
  group_by(Region) %>% 
  summarize(meanslope = mean(wtd_slope_MEAN))

cov.all %>% 
  group_by(Region) %>% 
  summarize(meanslope = mean(wtd_elev_MEAN))

#2011 all fixed now.
cov.all %>% 
  filter(wtd_lcld_jd <0)

cov.all %>% 
  mutate(yr11 = case_when(Year == 2011 ~ 1, 
                          TRUE ~ 0)) %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = wtd_lcld_jd, color = as.factor(yr11))) +
  geom_point() +
  facet_wrap(~Region)


cov.all %>% 
  # filter(!Year == 2011) %>% 
  ggplot(aes(x = wtd_elev_MEAN, y = wtd_lcld_jd, color = Region)) +
  geom_point() +
  geom_smooth() 
  facet_wrap(~Year)
  

```

Comparing modis output with snowtel.

```{r snow exploration, eval = FALSE}

#these sort of match up to hillside snowtel (2006-2019) april 1st swe 
# lowest years 2015, 2014, 2019
# biggest years 2012 by a lot, then 2007-2010 all about the same
cov.all %>% 
  group_by(Region, Year) %>% 
  summarize(meanlcld = mean(wtd_lcld_jd)) %>% 
  pivot_wider(names_from = Year, values_from = meanlcld, names_sort = TRUE)

cov.all %>% 
  ggplot() +
  geom_freqpoly(aes(x = wtd_lcld_jd, after_stat(density), color = Region)) +
  facet_wrap(~Year)
```

## Combine SiteIDs 

Generate a data frame that associates the SiteIDs in the response data frame with the covariate data frame. Tim's response data frame drops > 70 sites because they had limited summer data or data were prior to 2011 (he ran 2011-2020). We may go back and include older years now that we aren't running DFA so use the original model data frame sent to Tim (stream + air temps) and generate lookup for merging the SiteIDs. Then apply this to the response data frame with AR1 model output.

We added prefixes to most of the SiteIDs when generating the data that didn't get carried over to the spatial data frames. 

* find the SiteIDs that don't match in the two data frames (109) -- make sure all are converted to lower case because that is causing some mismatches.
* remove the prefix -- for just the mismatched sites -- so that that spatial SiteIDs will join.
* 13 sites in the response data frame not in the covariate data frame -- see [google sheet](https://docs.google.com/spreadsheets/d/1TJrNIwr14HJ4QV6-fB4ubmi0BoTNCG7UxoK-s--IsJM/edit#gid=1942759492) for how to fix them. 


```{r create siteid_join data frame, include = FALSE}

temp_dat <- readRDS("data_preparation/final_data/summer_data_wair2021-11-23.rds") %>% 
  mutate(site_lower = tolower(SiteID))

temp_md <- readRDS("data_preparation/final_data/md.rds")

cov.all <- cov.all %>% 
  mutate(site_lower = tolower(SiteID))

#sites in temperature data frame not in covariate data frame
remove_prefix <- anti_join(temp_dat %>% distinct(site_lower),
          cov.all %>% distinct(site_lower)) %>%
  arrange(site_lower) %>% 
  pull(site_lower)

siteid_join <- temp_dat %>% 
  mutate(cov_id = case_when(site_lower %in% remove_prefix ~  str_remove(site_lower, pattern = "^.*?(_|-)"),
                            TRUE ~ site_lower)) %>% 
  distinct(ts_id = site_lower, cov_id)

#13 mismatches
# anti_join(siteid_join, cov.all, by = c("cov_id" = "site_lower"))

siteid_join <- siteid_join %>% 
  mutate(cov_id = case_when(cov_id == "katm_lbrooo" ~ "katm_lbrooo_lvl",
                            cov_id == "katm_naknlo" ~ "katm_naknlo_lvl",
                            cov_id == "lacl_kijilo" ~ "lacl_kijilo_lvl",
                            cov_id == "lslil10" ~ "lslil",
                            cov_id == "lsnlt10" ~ "lsnlt1",
                            cov_id == "lsr93" ~ "lsr93lb",
                            cov_id == "lsr106" ~ "lsr112.5",
                            cov_id == "lacl_lclaro" ~ "lacl_lclaro_lvl",
                            TRUE ~ cov_id))

write.csv(siteid_join, "data_preparation/SiteID_join_spatial_to_temperature.csv")
```

Lots of sites in covariates not in response data frame -- check on how Tim filtered data completeness, which should explain why sites were dropped. For each site and year, Tim dropped any with less than 80% of days in June 1 - Aug 31 (jd 152-243). And, he only used sites with data from 2011-2020.

```{r sites dropped in arima analysis, eval = FALSE}

temp_dat %>% 
  # filter(year(sampleDate) %in% 2001:2019) %>% 
  distinct(SiteID) %>% nrow()
ts_dat %>% distinct(Site) %>% nrow() #dropped 50 overall, 27 when filter on year only, so other 23 presumably didn't have complete summers of data

anti_join(temp_dat %>% distinct(SiteID), ts_dat %>% distinct(SiteID = Site))

# 49 sites
temp_dat %>%
  mutate(Year = year(sampleDate), jd = as.numeric(format(sampleDate, "%j"))) %>% 
  filter(jd > 151, jd < 244) %>% 
  group_by(SiteID, Year) %>% 
  summarize(sum_percent = n()/92) %>%
  mutate(keep = case_when(sum_percent >= 0.8 ~ 1,
                          Year < 2011 ~ 0,
                          Year > 2019 ~ 0,
                          TRUE ~ 0)) %>% 
  group_by(SiteID) %>% 
  summarize(sum_complete = sum(keep)) %>% 
  filter(sum_complete == 0)
```

Use merge table to join on SiteIDs. One site in responses without spatial data - Nerka Pike Creek, only 1 year, will be dropped.

```{r create model data frame, include = FALSE}

siteid_join <- read_csv("data_preparation/SiteID_join_spatial_to_temperature.csv")

# nerka pike creek is only missing site now
ts_dat %>% mutate(site_lower = tolower(Site)) %>% 
  left_join(siteid_join, by = c("site_lower" = "ts_id")) %>% 
  filter(Year %in% 2001:2019) %>% 
  left_join(cov.all %>% mutate(cov_id = tolower(SiteID))) %>%
  filter(is.na(str_ord))

mod_dat <- ts_dat %>% 
  mutate(site_lower = tolower(Site)) %>% 
  left_join(siteid_join, by = c("site_lower" = "ts_id")) %>% 
  filter(Year %in% 2001:2019) %>% 
  left_join(cov.all %>% mutate(cov_id = tolower(SiteID))) %>% 
  filter(!is.na(str_ord))

```


## Add in daymet precipitation

Tim C. included total summer precipitation as a predictor in his models so add from daymet here. Note that this is site specific precip, not from across the watershed.

```{r merge with precip}

daymet <- read_csv("data_preparation/daymet/site_daymet.csv")
daymet %>% distinct(measurement)

#correct conversion is date2 bc date example below has Jan 1 as day 0!
daymet %>% 
  filter(year == 2006, 
         yday == 152) %>% 
  mutate(date = as.Date(yday, origin = paste0(year, "-01-01")),
         date2 = as.Date(paste(year, yday), format = "%Y %j"))

precip <- daymet %>%
  filter(measurement %in% c("prcp..mm.day.")) %>% 
  mutate(sampleDate = as.Date(paste(year, yday), format = "%Y %j")) %>% 
  filter(month(sampleDate) %in% 6:8) %>% 
  rename(Site = site, Year = year) %>% 
  group_by(Site, Year) %>% 
  summarize(summer_precip = sum(value))

mod_dat <- left_join(mod_dat %>% select(-(site_lower:SiteID)), precip)

rm(daymet)

```



# Data exploration

## Response

Look at sites with lots of negative TS as a check.

* NPS Hardenburg bay is a bay in Lake Clark - drop.
* USFS_Middle Arm Eyak - streambed measurements per Luca so location is just off.
* usgs 15258000 is outlet of kenai lake, but still seems very lake affected or lake-like (kenai r at cooper landing) - drop
* usgs 15260001 is outlet of cooper lake right below hydro dam - this seems very anthro-influenced -- drop
* usfs solf lake fish pass and outlet creek - outlet creek is lake influenced per Luca's notes. Location may be off if it looks like a lake site.
* usfs mckinley lake site is a lake outlet and influenced by it per Luca's notes. Location is off a bit since it looks like it is in the lake.
* 572656154082400 is karluk river on kodiak.

```{r negative ts values}
mod_dat %>% 
  filter(TempSens < 0) %>% 
  ggplot(aes(x = TempSens, y = SiteID)) +
  geom_point()

mod_dat %>% 
  filter(TempSens < 0) %>% 
  count(SiteID) %>% 
  arrange(desc(n))

md <- readRDS("data_preparation/final_data/md.rds")

left_join(mod_dat, md %>% select(SiteID, Waterbody_name, SourceName)) %>% 
  filter(Site == "572656154082400")

drop_sites <- c("NPS_Hardenburg_Bay", "usgs_15258000", "usgs_15260001", "USFS_Middle Arm Eyak")

mod_dat2 <- mod_dat %>% 
  filter(!Site %in% drop_sites)


```

what about other sites in Luca's dataset? We kept some that are not ones he originally flagged to include in a landscape analysis.

* middle fork 18 mile
* 24.9 mile creek
* ibeck lower side channel
* eagle creek
* pigot bay spawn channel - remove?
* rude river side channel - remove?


Any outliers in TS? The only major one is the very negative value at Karluk River.

```{r ts boxplot by region}
mod_dat2 %>% 
  ggplot(aes(x = Region, y = TempSens)) +
  geom_boxplot()
```


```{r ts dotplot}
mod_dat2 %>% 
  arrange(TempSens) %>% 
  mutate(rowid = row_number()) %>%
  ggplot(aes(x = TempSens, y = rowid,  color = Region)) +
  geom_point() +
  # scale_y_continuous(limits = c(0, 1500)) +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        legend.position = "bottom", axis.title.y = element_blank())
```

Two sites with rather high TS.

```{r}
mod_dat2 %>% 
  filter(TempSens > 0.7)
```

```{r ts histogram}
mod_dat2 %>% 
  ggplot(aes(x = TempSens)) +
  geom_histogram() +
  facet_wrap(~Region) +
  theme_bw()

```


## Covariates 

Checking outliers and collinearity. Drop those with high correlation.

Check slopes first. A couple of outliers -- one for stream slope and two for minimum catchment slope. We checked the min catchment slopes and they are accurate, streams in very steep watersheds. One of those aslo had the high stream slope. Note that the mean, min, max statistics for slope and elevation by watershed are all highly correlated so just go with means for now and recheck using r and vif.

Keep:

* stream slope
* mean catchment slope and elevation
* mean watershed slope and elevation
* watershed percent north facing 
* distance to coast
* watershed area
* wetland, glacier, lake cover
* modis snow date

```{r pair plots for groups of covariates}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}

#slope and aspect
mod_dat2 %>% 
  select(str_slope, cat_slope_MIN:cat_slope_STD, wtd_slope_MIN:wtd_slope_STD, wtd_north_per) %>% 
  pairs(upper.panel = panel.cor)

#elevation, area, and distance to coast
mod_dat2 %>% 
  select(cat_elev_MIN:cat_elev_STD, wtd_elev_MIN:wtd_elev_STD, wtd_area_sqKM, dist_coast_km) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist)

#landcover and snow date
mod_dat2 %>% 
  select(wtd_wet_per:summer_precip) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist)


```


Correlations for all covariates in list above. All r < 0.7, which is good.

```{r pair plot for 13 cov}
mod_dat2 %>% 
  select(str_slope, cat_elev_MEAN, cat_slope_MEAN, wtd_north_per,
         wtd_elev_MEAN, wtd_slope_MEAN, wtd_area_sqKM, dist_coast_km,
         wtd_wet_per, wtd_glacier_per, wtd_lake_per, wtd_lcld_jd, summer_precip) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist)
```

MODIS lcld as a function of watershed slope (Fig 2C in Tim's paper).

```{r lcld versus watershed slope}
mod_dat2 %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = wtd_lcld_jd)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ I(x^2))
```

Generate snow index as residuals of model of lcld as a function of watershed slope (per Tim's paper). Compare squared watershed slope versus linear model using AIC.

```{r snow index}
ess_lme1 <- lme(wtd_lcld_jd ~ I(wtd_slope_MEAN^2), dat = mod_dat2, random = ~1|Year)
ess_lme2 <- lme(wtd_lcld_jd ~ wtd_slope_MEAN, dat = mod_dat2, random = ~1|Year)
AIC(ess_lme1, ess_lme2)

ess_lme_resid <- resid(ess_lme1, level = 1)

mod_dat2 <- mod_dat2 %>% 
  mutate(snow_ind = ess_lme_resid)

#check that snow index is now not correlated to watershed slope.
mod_dat2 %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = snow_ind)) +
  geom_point() +
  geom_smooth(method = "lm")

```


Logit transformation of glacier and lake cover because both are percentages and have large outliers. Log transform watershed area, stream slope, catchment elevation and slope, and summer precip. Transformation for slope needs minimum value because one 0 (following methods in mccune and grace 2002).


```{r covariate transformations}

#looking for abs skew < 1 in cont covariates.
mod_dat2 %>% 
  select(str_slope, cat_elev_MEAN, cat_slope_MEAN, wtd_north_per,
         wtd_elev_MEAN, wtd_slope_MEAN, wtd_area_sqKM, dist_coast_km,
         wtd_wet_per, wtd_glacier_per, wtd_lake_per, snow_ind, summer_precip) %>% 
  apply(., 2, function(x) skewness(x))

mod_dat2 %>%
  distinct(str_slope) %>%
  arrange(str_slope)

min_slope = 0.0000100000
c_slope = as.integer(log10(min_slope))
d_slope = 10^c_slope

mod_dat2 <- mod_dat2 %>% 
  mutate(log_area = log10(wtd_area_sqKM),
         log_slope = log10(str_slope + d_slope) - c_slope,
         asrt_glac = asin(sqrt(wtd_glacier_per/100)),
         asrt_lake = asin(sqrt(wtd_lake_per/100)),
         asrt_wet = asin(sqrt(wtd_wet_per/100)),
         glac_10 = case_when(wtd_glacier_per > 0 ~ 1,
                             TRUE ~ 0),
         log_cat_elev = log10(cat_elev_MEAN),
         log_cat_slope = log10(cat_slope_MEAN),
         log_precip = log10(summer_precip))

mod_dat2 %>% 
  select(log_slope, log_cat_elev, log_cat_slope, wtd_north_per,
         wtd_elev_MEAN, wtd_slope_MEAN, log_area, dist_coast_km,
         snow_ind, asrt_glac, asrt_lake, asrt_wet, glac_10, log_precip) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist)


mod_dat2 %>% 
  select(log_slope, log_cat_elev, log_cat_slope, wtd_north_per,
         wtd_elev_MEAN, wtd_slope_MEAN, log_area, dist_coast_km,
         snow_ind, asrt_glac, asrt_lake, asrt_wet, glac_10, log_precip) %>% 
  apply(., 2, function(x) skewness(x))


```

Still one site in Bristol Bay -- Egegik River -- with very high lake cover not fixed by logit transformation. ~37%, next highest is 17%.

```{r lake outlier}
mod_dat2 %>% filter(asrt_lake > .5)

md %>% 
  filter(SiteID == "fws_580223156504200")

```


Set up a basic model to get VIF. 

```{r vif for 14 cov}
lm1 <- lm(TempSens ~ log_slope + log_cat_elev + log_cat_slope + wtd_north_per +
         wtd_elev_MEAN + wtd_slope_MEAN + log_area + dist_coast_km + 
         asrt_wet + asrt_glac + asrt_lake + glac_10 + snow_ind + log_precip, data = mod_dat2)

vif(lm1)
```

Remove mean watershed elevation now all vif < 3 or close to it. Wtd slope is highest at 3.17 and the only one above 3.

```{r vif 13 cov}
lm2 <- lm(TempSens ~ log_slope + log_cat_elev + log_cat_slope + wtd_north_per +
         wtd_slope_MEAN + log_area + dist_coast_km + 
         asrt_wet + asrt_glac + asrt_lake + glac_10 + snow_ind + log_precip, data = mod_dat2)


vif(lm2)
```

Try removing either catchment elevation or slope and see if VIF for watershed slope gets a little lower.

Removing catchment slope brings the VIF for watershed slope from 3.1 to 2.8, whereas removing catchment elevation doesn't do much at all to VIF for watershed slope.

```{r vif 12 cova}
lm3 <- lm(TempSens ~ log_slope + log_cat_elev + wtd_north_per +
         wtd_slope_MEAN + log_area + dist_coast_km + 
         asrt_wet + asrt_glac + asrt_lake + glac_10 + snow_ind + log_precip, data = mod_dat2)


vif(lm3)
```

Save model data frame with snow index and covariate transformations.

```{r save mod_dat data frame}

saveRDS(mod_dat2, paste("data_preparation/final_data/model_data", Sys.Date(), ".rds", sep = ""))


```


## Cook Inlet sites

Look at Sue's sites and years and different scenarios - 

```{r cik sites and years}
mod_dat %>% 
  distinct(Site, Year, Waterbody_name) %>% 
  filter(grepl("CIK", Site), !Site %in% c("CIK1", "CIK2", "CIK3", "CIK4", "CIK6")) %>%
  group_by(Site, Waterbody_name) %>% 
  summarize(years = paste0(Year, collapse = ", "))
```

```{r cik ts by year}
mod_dat %>% 
  filter(grepl("CIK_", Site)) %>% 
  ggplot(aes(x = Year, y = TempSens, group = Site)) +
  geom_line() +
  scale_x_continuous(breaks = c(seq(2000, 2019, 2))) 
```


```{r cik ts boxplots}

mod_dat %>% 
  filter(grepl("Moose", Site))

mod_dat %>% 
  filter(grepl("CIK_", Site)) %>% 
  group_by(Site) %>% 
  dplyr::mutate(yr_ct = n(),
         name_ct = paste(Waterbody_name, yr_ct, sep = "-")) %>% 
  filter(yr_ct > 4) %>% 
  ggplot(aes(y = name_ct, x = TempSens)) +
  geom_boxplot() 
```

```{r}

mod_dat %>% 
  filter(Site %in% c("CIK_14", "CIK_3", "CIK_30", "CIK_35", "CIK_38", "CIK_4", "CIK_6", "CIK_8")) %>% 
  group_by(Site) %>% 
  mutate(ts_sc = scale(TempSens)) %>% 
  ggplot(aes(x = Year, y = ts_sc, color = Site)) +
  geom_line() +
  geom_point() +
  geom_hline(aes(yintercept = 0)) +
  scale_x_continuous(breaks = c(seq(2000, 2019, 2))) 
```

```{r}
mod_dat %>% 
  filter(Site %in% c("CIK_14", "CIK_3", "CIK_30", "CIK_35", "CIK_38", "CIK_4", "CIK_6", "CIK_8")) %>% 
  # group_by(Site) %>% 
  # mutate(ts_sc = scale(TempSens)) %>% 
  ggplot(aes(x = summer_precip, y = TempSens)) +
  geom_smooth() +
  geom_point(aes(size = wtd_slope_MEAN)) +
  facet_wrap(~Waterbody_name, scales = "free")

```
```{r}

scale_var <- function(x){
  (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)
}


mod_dat %>% 
  filter(Site %in% c("CIK_14", "CIK_3", "CIK_30", "CIK_35", "CIK_38", "CIK_4", "CIK_6", "CIK_8")) %>% 
  group_by(Site) %>% 
  dplyr::mutate(ts_sc = scale_var(TempSens),
         lcld_sc = scale_var(wtd_lcld_jd),
         prp_sc = scale_var(summer_precip)) %>%
  ggplot() +
  geom_line(aes(x = Year, y = ts_sc), color = "red") +
  geom_line(aes(x = Year, y = lcld_sc), color = "blue") +
  geom_line(aes(x = Year, y = prp_sc), color = "dark green") +
  geom_hline(aes(yintercept = 0)) +
  facet_wrap(~Waterbody_name) +
  scale_x_continuous(breaks = c(seq(2000, 2019, 2))) +
  theme_bw()


```



# Exploring relationships with TS

Tim reran using an AR1 model (time series model with lag of 1 day) because the Deshka sites were confounding the analysis in 2017-2019. The common trend was air temperatures and the TS were super low, which is not what we want. Read in new results, which has AR1 results linked to covariate data. This was rerun to include all sites and years from 2001-2019, which are the years covered by the MODIS snow metrics. 

Boxplots of TS by region and year. Note that 2019 does not appear to be an outlier for TS.

```{r}
mod_dat <- readRDS("data_preparation/final_data/model_data2022-02-08.rds") 

mod_dat %>% 
  ggplot(aes(x = as.factor(Year), y = TempSens)) +
  geom_boxplot() +
  facet_wrap(~Region) +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(x = "Year")
```

Boxplots of all streams by year. Pick two years in the 2011-2019 time period (most sites in these years) to map TS.

* 2019 still a good year with 2nd highest median TS.
* 2013 was a high snow year in both BB and CI, unfortunately, this is before the Kodiak network started.

```{r}
mod_dat %>% 
  ggplot(aes(y = fct_reorder(as.factor(Year), TempSens), x = TempSens)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(x = "Thermal sensitivity")
```


Plot comparing SD of TS to mean of TS for sites with at least 5 years of data.

```{r}
mod_dat %>% 
  group_by(Region, Site) %>% 
  mutate(count = n()) %>% 
  filter(count > 4) %>% 
  summarize(meants = mean(TempSens),
            sdts = sd(TempSens)) %>% 
  ggplot(aes(x = meants, y = sdts, color = Region)) +
  geom_point()
```

Variability in TS from year to year for sites with at least 5 years of data.

```{r}
mod_dat %>% 
  group_by(Region, Site) %>% 
  mutate(count = n()) %>% 
  filter(count > 4) %>% 
  ggplot(aes(x = TempSens, y = Site)) +
  geom_point() +
  facet_wrap(~Region, scales = "free_y", ncol = 1)

mod_dat %>% 
  group_by(Region, Site) %>% 
  mutate(count = n()) %>% 
  filter(count > 4) %>% 
  summarize(ts_range = range(TempSens)) %>% 
  ggplot() +
  geom_histogram(aes(x = ts_range))

mod_dat %>% 
  group_by(Region, Site, wtd_elev_MEAN) %>% 
  mutate(count = n()) %>% 
  filter(count > 4) %>% 
  summarize(ts_range = range(TempSens)) %>% 
  ggplot() +
  geom_point(aes(x = ts_range, y = wtd_elev_MEAN))

mod_dat %>% 
  group_by(Region, Site, cat_elev_MEAN) %>% 
  mutate(count = n()) %>% 
  filter(count > 4) %>% 
  summarize(ts_range = range(TempSens)) %>% 
  ggplot() +
  geom_point(aes(x = ts_range, y = cat_elev_MEAN))

mod_dat %>% 
  ggplot() +
  geom_histogram(aes(x = TempSens))
```



## Pairwise correlations between TS and covariates

Pairplots for covariates versus TS.

```{r pair plots against TS}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}

mod_dat %>% 
  select(TempSens, log_slope, cat_elev_MEAN, cat_slope_MEAN, 
         wtd_slope_MEAN, snow_ind, summer_precip) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist, lower.panel = panel.smooth)

mod_dat %>% 
  select(TempSens, log_area, dist_coast_km, wtd_north_per,
         wtd_wet_per, logit_glac, logit_lake) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist, lower.panel = panel.smooth)

```

## Exploring interactions 


Tim's results: north x snow, north x slope, north x precip, precip x slope, slope x snow.

Facets are snow index and x is north aspect, no obvious relationship to TS.

```{r north and snow}
ggplot(transform(mod_dat, fct = cut(snow_ind, seq(from = min(snow_ind)-1, to = max(snow_ind) +1, length.out = 6))), 
       aes(y = TempSens, x = wtd_north_per)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by north aspect and snow index", subtitle = "Facets are bins of snow index")

```

Facets are snow index and x is watershed slope, can still see negative relationship between slope and TS, but no interaction with snow.

```{r slope and snow}
ggplot(transform(mod_dat, fct = cut(snow_ind, seq(from = min(snow_ind)-1, to = max(snow_ind) +1, length.out = 6))), 
       aes(y = TempSens, x = wtd_slope_MEAN)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by slope and snow index", subtitle = "Facets are bins of snow index")

```

Interesting! Can start to see negative relationship of aspect with TS, but only at highest slope watersheds, which makes sense.

```{r ts by north and wtd slope}
summary(mod_dat)

facet_var <- "wtd_slope_MEAN"

ggplot(transform(mod_dat, fct = cut(get(facet_var), seq(from = min(get(facet_var)) - 0.01, to = max(get(facet_var)), length.out = 6))), 
       aes(y = TempSens, x = wtd_north_per)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed slope and north aspect", subtitle = paste0("Facets are bins of ", facet_var))
```

No interaction.

```{r ts by north and precip}

ggplot(transform(mod_dat, fct = cut(summer_precip, seq(from = min(summer_precip), to = max(summer_precip), length.out = 6))), 
       aes(y = TempSens, x = wtd_north_per)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by summer precip and north aspect", subtitle = "Facets are bins of summer precip")
```


No real evidence of an interaction between precip and slope.

```{r ts by precip and wtd slope}

ggplot(transform(mod_dat, fct = cut(wtd_slope_MEAN, seq(from = 0, to = 32, length.out = 6))), 
       aes(y = TempSens, x = summer_precip)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed slope and precip", subtitle = "Facets are bins of mean watershed slope")
```


TS by glacier cover and mean watershed slope. Low slope watersheds generally have no or very low glacier cover. In watersheds in bins 3 and 4, can see expected negative relationship between glacier cover and TS. 

```{r ts by glacier and wtd slope}

ggplot(transform(mod_dat, fct = cut(wtd_slope_MEAN, seq(from = 0, to = 32, length.out = 6))), 
       aes(y = TempSens, x = wtd_glacier_per)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed slope and glacier cover", subtitle = "Facets are bins of mean watershed slope")
```

TS by glacier cover and watershed area. Glacier cover decreases TS over all sizes of watersheds.

```{r ts by glacier and wtd area}

ggplot(transform(mod_dat, fct = cut(wtd_area_sqKM, breaks = quantile(wtd_area_sqKM))), 
       aes(y = TempSens, x = wtd_glacier_per)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed area and glacier cover", subtitle = "Facets are quantiles of mean watershed area")

```

TS by lake cover and mean watershed slope. No obvious relationship between TS and lake cover. 

```{r ts by lakes and wtd slope}

ggplot(transform(mod_dat, fct = cut(wtd_slope_MEAN, seq(from = 0, to = 32, length.out = 6))), 
       aes(y = TempSens, x = wtd_lake_per)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed slope and lake cover", subtitle = "Facets are bins of mean watershed slope")
```

TS by lake cover and watershed area. 

```{r ts by lake and wtd area}

ggplot(transform(mod_dat, fct = cut(wtd_area_sqKM, breaks = quantile(wtd_area_sqKM))), 
       aes(y = TempSens, x = wtd_lake_per)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed area and lake cover", subtitle = "Facets are quantiles of mean watershed area")

```

TS by precip and watershed area. 

```{r ts by precip and wtd area}

ggplot(transform(mod_dat, fct = cut(wtd_area_sqKM, breaks = quantile(wtd_area_sqKM))), 
       aes(y = TempSens, x = summer_precip)) +
  geom_point() +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed area and precipitation", subtitle = "Facets are quantiles of mean watershed area")

```



Bring in metadata which has field for Deshka sites and also lat/longs.

```{r include = FALSE}
md <- read_rds("data_preparation/final_data/md.rds")

md %>% summary()

mod_dat <- left_join(mod_dat, md %>% select(SiteID, deshka, HUC8, Name, Latitude, Longitude), by = c("Site" = "SiteID")) 

mod_dat %>% 
  filter(deshka == 1) %>% 
  ggplot(aes(x = TempSens, y = fct_reorder(Site, TempSens), color = as.factor(Year))) +
  geom_point()
```



```{r, spatial data, include = FALSE}
mod_sf <- st_as_sf(mod_dat, coords = c("Longitude", "Latitude"), crs = "WGS84")
huc8 <- st_read(dsn = "S:/Leslie/GIS/NHD Harmonized/WBD_19_GDB.gdb", layer = "WBDHU8")  
huc8_wgs84 <- st_transform(huc8, crs = "WGS84")

huc8_sa <- st_filter(huc8_wgs84, mod_sf)

regions <- huc8_sa %>% 
  mutate(Region = case_when(HUC8 %in% 19020301:19020602 ~ "Cook Inlet",
                            HUC8 == 19020701 ~ "Kodiak",
                            HUC8 %in% 19020101:19020104 ~ "Copper",
                            HUC8 %in% 19020201:19020203 ~ "Prince William Sound",
                            TRUE ~ "Bristol Bay")) %>% 
  group_by(Region) %>% 
  summarize()
```

Plot the TS for 2019 (high) and 2013 (low) years.

```{r static map of TS, fig.width=16, fig.height=14}

ggplot() +
  geom_sf(data = regions, aes(fill = Region), alpha = 0.1) +
  # geom_sf(data = huc8_sa) +
  geom_sf(data = mod_sf %>% filter(Year %in% c(2019, 2013)), aes(color = TempSens), size = 3) +
  facet_wrap(~Year, ncol =1) +
  scale_color_viridis_c() +
  theme_bw() +
  theme(legend.position = "bottom", text = element_text(size = 14)) +
  guides(fill = guide_legend(nrow = 2))

```


Set up an interactive mapper with a data summary that can be used as a popup.

- spatial covariates (just wtd slope right now)
- TS for 2019 and 2013 as high and low years

```{r}
map.dat <- mod_dat %>% 
  select(Site, Year, TempSens, Region:dist_coast_km, wtd_elev_MEAN, wtd_slope_MEAN, wtd_north_per:wtd_area_sqKM) %>% 
  filter(Year %in% c(2013, 2019)) %>% 
  pivot_wider(names_from = Year, values_from = TempSens) %>% 
  left_join(md %>% select(SiteID, deshka, HUC8, Name, Latitude, Longitude, Waterbody_name, SourceName), by = c("Site" = "SiteID")) %>% 
  st_as_sf(coords = c("Longitude", "Latitude"), crs = "WGS84")

```

Save version for arcgis online mapper that Dustin can link to points. Add spatial ID to this output so he only has to do one join.

```{r}
spatial_join <- read_csv("data_preparation/SiteID_join_spatial_to_temperature.csv") %>% select(-X1)

mod_dat %>% 
  select(Site, Year, TempSens, Region:dist_coast_km, wtd_elev_MEAN, cat_elev_MEAN, wtd_slope_MEAN, cat_slope_MEAN, wtd_north_per:wtd_area_sqKM) %>% 
  filter(Year %in% 2001:2019) %>% 
  pivot_wider(names_from = Year, values_from = TempSens) %>% 
  mutate(site_lower = tolower(Site)) %>% 
  left_join(spatial_join, by = c("site_lower" = "ts_id")) %>% 
  write_csv("output/sites_wTScovars.csv")

```

Check why the file created above is not linking to Dustin's point file. We should have spatial locations for all 408 sites with data.





```{r tmap map}
tmap_mode("view")

sitemap <- tm_shape(regions) +
  tm_borders("black") +
  tm_shape(map.dat) +
  tm_dots(id = "Site", size = 0.05, group = "Region", col = "2019",
          popup.vars = c("Waterbody_name", "2019", "2013", "wtd_slope_MEAN", "wtd_glacier_per", "wtd_lake_per")) +
  tm_text("Site", size = 1.5, shadow = TRUE, auto.placement = TRUE,
          just = "bottom", remove.overlap = TRUE, clustering = TRUE, group = "Labels" ) +
  tm_basemap(server = c(Topo = "Esri.WorldTopoMap", Imagery = "Esri.WorldImagery" ))

sitemap

```





# Generating model subsets for new DFA by Tim

During team meeting on 2/4/22, decided to investigate DFA again after subsetting sites in Deshka. We're interested in trying out different model trends that might accommodate the different types of sites we have in the data -- both TS and loadings on residual trends could be informative for understanding TS and scenarios of change in the future.

There is already a field called deshka that flags the sites in that watershed. Add two new subsets:

* subset1 of sites in Deshka that span a gradient of watershed slope and are spread out across the three stream segments. Sites are numbered from upstream to downstream so can also use numbering to separate them.
* Two tributaries in each: deshka tribs 3 and 4, kroto tribs 1 and 3, moose tribs 2 and 8.
* Two mainstem sites in each: deshka 1 down and 4 up, kroto 2 up and 8 up, moose 2 up and 8 up. 
* CIK_28 is Sue's site at bottom of Deshka. 

```{r}
md <- readRDS("data_preparation/final_data/md.rds") %>% 
  rename(Site = SiteID)

left_join(mod_dat, md %>% select(Site, SourceName, Waterbody_name, deshka)) %>% 
  filter(deshka == 1|Site == "CIK_28") %>% 
  mutate(wtd = case_when(grepl("Moose", Site) ~ "moose",
                         grepl("Deshka", Site) ~ "deshka",
                         grepl("Kroto", Site) ~ "kroto",
                         TRUE ~ NA_character_),
         trib = case_when(grepl("Trib", Site) ~ 1,
                          TRUE ~ 0)) %>% 
  distinct(Site, wtd, trib, wtd_slope_MEAN, wtd_area_sqKM) %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = fct_reorder(Site, wtd_slope_MEAN), 
             color = as.factor(trib), size = wtd_area_sqKM)) +
  geom_point() +
  facet_wrap(~wtd, scales = "free_y")

```

```{r}
left_join(mod_dat, md %>% select(Site, SourceName, Waterbody_name, deshka)) %>% 
  filter(deshka == 1|Site == "CIK_28") %>% 
  count(Site) %>% 
  arrange(n)
```

Same dotplot of selected deshka sites.

```{r 13 deshka sites to keep}
desh_sites <- c("Deshka 3 Tributary", "Deshka 4 Tributary",
                "Kroto 1 Tributary", "Kroto 3 Tributary",
                "Moose 2 Tributary", "Moose 8 Tributary",
                "Deshka 1 Downstream", "Deshka 4 Upstream",
                "Kroto 2 Upstream", "Kroto 8 Upstream",
                "Moose 2 Upstream", "Moose 8 Upstream",
                "CIK_28")

#check that these sites have 3 years of data.
mod_dat %>% 
  filter(Site %in% desh_sites) %>% 
  count(Site) %>% 
  arrange(n)


mod_dat %>% 
  filter(Site %in% desh_sites) %>% 
  mutate(wtd = case_when(grepl("Moose", Site) ~ "moose",
                         grepl("Deshka", Site) ~ "deshka",
                         grepl("Kroto", Site) ~ "kroto",
                         TRUE ~ NA_character_),
         trib = case_when(grepl("Trib", Site) ~ 1,
                          TRUE ~ 0)) %>% 
  distinct(Site, wtd, trib, wtd_slope_MEAN, wtd_area_sqKM) %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = fct_reorder(Site, wtd_slope_MEAN), 
             color = as.factor(trib), size = wtd_area_sqKM)) +
  geom_point() 

```

```{r add subset removing 77 deshka sites}
md %>% filter(deshka == 1) %>% distinct(Site) # 89 + sue's site 

md <- md %>% 
  mutate(subset1 = case_when(deshka == 0 ~ 1,
                             Site %in% desh_sites ~ 1,
                             TRUE ~ 0)) 

md %>% count(subset1)
md %>% count(deshka)


```

Create a second subset where we remove sites that have less than 3 years of data, focusing on the period 2011-2019.

Also look at count by huc8: 

* lower susitna 19020505 has 145, removing 77
* then comes lower kenai with 56, filtering on years should remove some kbrr and apu data from the anchor.

```{r}
md %>% count(HUC8) %>% arrange(desc(n))

left_join(mod_dat, md %>% select(subset1, Site)) %>% 
  filter(Region == "Cook_Inlet", subset1 == 1) %>%
  filter(Year %in% 2011:2019) %>%
  count(Site) %>% 
  filter(n > 1)

sites_2yr <- mod_dat %>% 
  filter(Year %in% 2011:2019) %>% 
  count(Site) %>% #383 sites
  arrange(n) %>% 
  filter(n > 1) %>% #286 sites
  pull(Site)

sites_3yr <- mod_dat %>% 
  filter(Year %in% 2011:2019) %>% 
  count(Site) %>% #383 sites
  arrange(n) %>% 
  filter(n > 2) %>% #222 sites
  pull(Site)

md <- md %>% 
  mutate(subset2 = case_when(subset1 == 1 & Site %in% sites_2yr ~ 1,
                             TRUE ~ 0),
         subset3 = case_when(subset1 == 1 & Site %in% sites_3yr ~ 1,
                             TRUE ~ 0)) 
```

Plotting both subsets against watershed slope. If anything, now Bristol Bay is heavily biased by sites using the 3 year cutoff since a lot of Sue's data and sites ended in 2012.

```{r}
left_join(mod_dat %>% distinct(Site, Region, wtd_slope_MEAN), 
          md %>% select(Site, subset1, subset2)) %>% 
  filter(subset1 == 1) %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = fct_reorder(Site, wtd_slope_MEAN))) +
  geom_point() +
  facet_wrap(~Region, scales = "free_y")

left_join(mod_dat %>% distinct(Site, Region, wtd_slope_MEAN), 
          md %>% select(Site, subset1, subset2)) %>% 
  filter(subset2 == 1) %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = fct_reorder(Site, wtd_slope_MEAN))) +
  geom_point() +
  facet_wrap(~Region, scales = "free_y")

left_join(mod_dat %>% distinct(Site, Region, wtd_slope_MEAN), 
          md %>% select(Site, subset3)) %>% 
  filter(subset3 == 1) %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = fct_reorder(Site, wtd_slope_MEAN))) +
  geom_point() +
  facet_wrap(~Region, scales = "free_y")

ggsave("output/subset3 wtd slope.jpeg", width = 12, height = 10)

left_join(mod_dat, md %>% select(Site, subset1)) %>% 
  distinct(Site, Region, subset1) %>%
  filter(subset1 == 1)
            count(subset1, Region) #146 in cook inlet
left_join(mod_dat, md %>% select(Site, subset2)) %>% 
  distinct(Site, Region, subset2) %>%
  filter(subset2==1)
            count(subset2, Region) #78 in cook inlet
left_join(mod_dat, md %>% select(Site, subset3)) %>% 
  distinct(Site, Region, subset3) %>% 
  # filter(subset3 == 1)
            count(subset3, Region) #53 in cook inlet

```

Save metadata file with data subsets.

```{r save md with subsets 123}

saveRDS(md, paste0("data_preparation/final_data/md_", Sys.Date(), ".rds"))        

write_csv(md, paste0("data_preparation/final_data/md_", Sys.Date(), ".csv"))


```


# DFA February 2022 - data subsets 

Tim reran the DFA using the different subsets created above and models with 1-3 trends. Email from him with results:

"The DFA analysis have finished running. I went ahead and ran 1-3 trend DFA’s for each of the three subsets. I created some pdf’s that show the trends and the loadings to evaluate if things had improved using the subsets. I’m having some issues with GIT so I couldn’t push the requisite files (probably owing to my noob git status). I saved the Temperature Sensitivity and Trend Loading parameters in RDS files by data subset (i.e., ‘GlobalResults_Subset1’ and ‘GlobalTrends_Subset1’). They are still on the VM so hopefully you all can grab them.

Here are a couple of observations:
* Its common/unsurprising that DFA’s with more trends are favored by AIC model selection. However, these multiple trend models are less interesting to our questions as they tend to have the same issue we were having in that some of the common trends are soaking up the air temperature variation. In many systems the temp sensitivity parameters decline with increasing numbers of trends (which indicates their variation is going to trends).
* The single trend models look improved across the subsets so these maybe useful as per our original intentions.
* I think it will be interesting to look at the distributions of temperature sensitivity again and the relations between trend loadings and geomorphic attributes."

This is complicated. We now have results for 

* 3 data subsets: subset 1 removed 65 deshka sites, subset 2 further removed sites with only 1 year of data, subset 3 removed sites with only 1-2 years of data.
* for each subset, Tim ran DFA models by year for 1, 2, and 3 trends.
* for each DFA, there are TS and trends.

## Subset 1 DFA results


```{r dfa results}
dfa_ss1 <- readRDS("DFA/GlobalResults_Subset1.RDS")
dfa_ss2 <- readRDS("DFA/GlobalResults_Subset2.RDS")
dfa_ss3 <- readRDS("DFA/GlobalResults_Subset3.RDS")
md <- readRDS("data_preparation/final_data/md_2022-02-08.rds")    

dfa_ss1_wd <- left_join(dfa_ss1 %>% 
            select(SiteID:TempSens_3) %>% 
            pivot_longer(cols = TempSens_1:TempSens_3,
                         names_to = "trends",
                         values_to = "TempSens") %>% 
            mutate(trends = as.numeric(substr(trends, 10, 10))),
          dfa_ss1 %>% 
            select(SiteID, Year, TrendLoad_1_1:TrendLoad_3_3) %>% 
            pivot_longer(cols = TrendLoad_1_1:TrendLoad_3_3,
                         names_to = "name",
                         values_to = "Loadings") %>% 
            mutate(trends = as.numeric(substr(name, 11, 11)),
                   trend_no = as.numeric(substr(name, 13,13)))) %>% 
              mutate(subset = 1) %>% 
  left_join(md %>% select(SiteID = Site, Region, Waterbody_name))

dfa_ss2_wd <- left_join(dfa_ss2 %>% 
            select(SiteID:TempSens_3) %>% 
            pivot_longer(cols = TempSens_1:TempSens_3,
                         names_to = "trends",
                         values_to = "TempSens") %>% 
            mutate(trends = as.numeric(substr(trends, 10, 10))),
          dfa_ss2 %>% 
            select(SiteID, Year, TrendLoad_1_1:TrendLoad_3_3) %>% 
            pivot_longer(cols = TrendLoad_1_1:TrendLoad_3_3,
                         names_to = "name",
                         values_to = "Loadings") %>% 
            mutate(trends = as.numeric(substr(name, 11, 11)),
                   trend_no = as.numeric(substr(name, 13,13)))) %>% 
              mutate(subset = 2) %>% 
  left_join(md %>% select(SiteID = Site, Region, Waterbody_name))

dfa_ss3_wd <- left_join(dfa_ss3 %>% 
            select(SiteID:TempSens_3) %>% 
            pivot_longer(cols = TempSens_1:TempSens_3,
                         names_to = "trends",
                         values_to = "TempSens") %>% 
            mutate(trends = as.numeric(substr(trends, 10, 10))),
          dfa_ss3 %>% 
            select(SiteID, Year, TrendLoad_1_1:TrendLoad_3_3) %>% 
            pivot_longer(cols = TrendLoad_1_1:TrendLoad_3_3,
                         names_to = "name",
                         values_to = "Loadings") %>% 
            mutate(trends = as.numeric(substr(name, 11, 11)),
                   trend_no = as.numeric(substr(name, 13,13)))) %>% 
              mutate(subset = 3) %>% 
  left_join(md %>% select(SiteID = Site, Region, Waterbody_name))

dfa_results <- bind_rows(dfa_ss1_wd, dfa_ss2_wd, dfa_ss3_wd)
dfa_results %>% distinct(subset)

saveRDS(dfa_results, "output/dfa_results.rds")
```

```{r trend results}
trends_ss1 <- readRDS("DFA/GlobalTrends_Subset1.RDS")
trends_ss2 <- readRDS("DFA/GlobalTrends_Subset2.RDS")
trends_ss3 <- readRDS("DFA/GlobalTrends_Subset3.RDS")

trend_results <- bind_rows(
  trends_ss1 %>% 
    pivot_longer(cols = Trend_1_1:Trend_3_3,
                 names_to = "name",
                 values_to = "value") %>% 
    mutate(trends = as.numeric(substr(name, 7, 7)),
           trend_no = as.numeric(substr(name, 9, 9)),
           subset = 1),
  trends_ss2 %>% 
    pivot_longer(cols = Trend_1_1:Trend_3_3,
                 names_to = "name",
                 values_to = "value") %>% 
    mutate(trends = as.numeric(substr(name, 7, 7)),
           trend_no = as.numeric(substr(name, 9, 9)),
           subset = 2),
  trends_ss3 %>% 
    pivot_longer(cols = Trend_1_1:Trend_3_3,
                 names_to = "name",
                 values_to = "value") %>% 
    mutate(trends = as.numeric(substr(name, 7, 7)),
           trend_no = as.numeric(substr(name, 9, 9)),
           subset = 3)
)

saveRDS(trend_results, "output/trend_results.rds")
```



```{r ts boxplots by number of trends}
dfa_results %>% 
  distinct(TempSens, trends, subset) %>% 
  ggplot(aes(x = as.factor(trends), y = TempSens)) +
  geom_boxplot() +
  facet_wrap(~subset)

```


```{r}
dfa_results %>% 
  filter(TempSens < 0) %>%
  select(Waterbody_name, SiteID, Year, TempSens, subset, trends) %>% 
  arrange(TempSens)

dfa_ss1_wd %>% 
  filter(Year == 2019, trends == 3, trend_no == "trend_2") %>% 
  arrange(desc(Loadings))


dfa_ss1_wd %>% 
  filter(Year == 2020, trends == 2, trend_no == "trend_1") %>% 
  arrange(desc(Loadings)) %>% 
  select(SiteID, Waterbody_name)
```



```{r ts versus trend}

dfa_results %>% 
  filter(trends == 1) %>% 
  ggplot(aes(TempSens, Loadings)) +
  geom_point(aes(color = as.factor(subset))) +
  facet_wrap(~Year) +
  labs(title = "DFA with One Trend")

dfa_results %>% 
  filter(trends == 2, trend_no == 1) %>% 
  ggplot(aes(TempSens, Loadings)) +
  geom_point(aes(color = as.factor(subset))) +
  facet_wrap(~Year) +
  labs(title = "DFA with Two Trends, Trend 1")

dfa_results %>% 
  filter(trends == 2, trend_no == 2) %>% 
  ggplot(aes(TempSens, Loadings)) +
  geom_point(aes(color = as.factor(subset))) +
  facet_wrap(~Year) +
  labs(title = "DFA with Two Trends, Trend 2")

dfa_results %>% 
  filter(trends == 3, trend_no == 1) %>% 
  ggplot(aes(TempSens, Loadings)) +
  geom_point(aes(color = as.factor(subset))) +
  facet_wrap(~Year) +
  labs(title = "DFA with Three Trends, Trend 1")

dfa_results %>% 
  filter(trends == 3, trend_no == 2) %>% 
  ggplot(aes(TempSens, Loadings)) +
  geom_point(aes(color = as.factor(subset))) +
  facet_wrap(~Year) +
  labs(title = "DFA with Three Trends, Trend 2")

dfa_results %>% 
  filter(trends == 3, trend_no == 3) %>% 
  ggplot(aes(TempSens, Loadings)) +
  geom_point(aes(color = as.factor(subset))) +
  facet_wrap(~Year) +
  labs(title = "DFA with Three Trends, Trend 3")
```

Frequency plots of thermal sensitivities by year and region.

```{r}

dfa_results %>% 
  ggplot(aes(TempSens)) +
  geom_freqpoly(aes(color = Region)) +
  facet_grid(rows = vars(Year), cols = vars(subset))


```


```{r}
trend_results %>%
  filter(trends == 1) %>% 
  ggplot(aes(x = DOY, y = value, color = as.factor(trend_no))) +
  geom_line() + 
  facet_grid(rows = vars(Year), cols = vars(subset))

trend_results %>%
  filter(trends == 2) %>% 
  ggplot(aes(x = DOY, y = value, color = as.factor(trend_no))) +
  geom_line() + 
  facet_grid(rows = vars(Year), cols = vars(subset))

trend_results %>%
  filter(trends == 3) %>% 
  ggplot(aes(x = DOY, y = value, color = as.factor(trend_no))) +
  geom_line() + 
  facet_grid(rows = vars(Year), cols = vars(subset))

```


