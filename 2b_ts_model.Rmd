---
title: "Stream Thermal Sensitivity Model"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readxl)
library(lubridate)
library(GGally)
library(corrplot)
library(car)
library(nlme)
library(MuMIn)
library(gpboost)
library(randomForest)
library(rfUtilities)
library(MASS)
library(geosphere)
library(sf)
library(glmmLasso)
library(ggpubr)
library(rnoaa)
library(tidyverse)

```


Modeling thermal sensitivities for five regions in Southern AK.

1. Read in response and covariate data and combine to create model data frame
2. Explore TS response - removed 3 sites with very negative TS
3. Explore correlations and multi-collinearity of covariates -- only select those with pairwise correlation < 0.7 and stepwise remove covariates until all have VIF < 3
4. Transform/model covariates as needed. Logit transformation for glacier cover and lake cover since 0-1 scale and both have large outliers -- note that after transformation there is still one outlier for lake coverage. Log transformation for watershed area and stream slope because skewed and continuous. Plotted watershed slope and LCLD (last day of the longest continuous snow season) and decided to remove effect of slope on snow and use the residual - per Tim's method -- new covariate called snow index. 
5. Model thermal sensitivities: mixed model or RF with random effects. First tested for random effect then removed covariates using liklihood ratio tests. Compared results to variable importance from dredging all model subsets of the global model.

# Read in data and combine

New AR(1) model output from Tim.

```{r responses, include = FALSE}
ts_dat <- readRDS("output/TempSens_Arima_Results_from2000_2022-01-14.RDS") %>% as_tibble()

# write_csv(ts_dat, "DFA/TempSens_Arima_Results2021.12.07.csv")

```

Read in covariates. Copied over from Dustin's script, but modified so that lcld is combined with the other covariates.

```{r covariates, include = FALSE}

#Data from covariates script
cov.df.raw <- read.csv(file = 'data_preparation/sensitivity_drivers/AKSSF_Covariates.csv', header = TRUE) 
#Data from spatial join script
sites.df <- read.csv(file = 'data_preparation/sensitivity_drivers/AKSSF_sites_sj_maxfac.csv', header = TRUE)
#Data from Modis Script
lcld.df <- read.csv(file = 'data_preparation/sensitivity_drivers/AKSSF_wtd_lcld_mn.csv', header = TRUE)

lcld.long <- lcld.df %>% 
  pivot_longer(cols = wtd_lcld_mn_2001:wtd_lcld_mn_2019, names_to = "metric", values_to = "wtd_lcld") %>% 
  mutate(Year = as.numeric(substr(metric, 13, 17)),
         wtd_lcld_jd = wtd_lcld - 365) %>% 
  select(-metric)

#dput(colnames(sites.df))
site_keep_cols <- c("SiteID","region", "cat_ID_con", "site_max_acc", "site_acc_sqKm",
                    "Area_km2","area_diff","size_diff", "lat", "lon",
                    'str_ord','str_slope', "dist_coast_km", "ds_dist_outlet")

sites.df <- sites.df %>% 
  mutate( region = sub("_$","",gsub('[[:digit:]"]+', '', cat_ID_con)),
  size_diff =  case_when(area_diff >0~'Larger', area_diff <0~'Smaller',
                        TRUE~'No Change')) %>% 
  subset(select = site_keep_cols )

#don't keep ds_dist_outlet bc it is incorrect for some sites/networks
# sites.df %>% 
#   ggplot(aes(x = dist_coast_km, y = ds_dist_outlet)) +
#   geom_point()
  
#dput(colnames(cov.df.raw))
wtd_keep_cols <- c("cat_ID_con", "cat_ID", "region", "cat_slope_MIN", "cat_slope_MAX",
  "cat_slope_MEAN", "cat_slope_STD", "cat_slope_SUM", "cat_elev_MIN", 
  "cat_elev_MAX", "cat_elev_MEAN", "cat_elev_STD", "wtd_elev_MIN",
  "wtd_elev_MAX", "wtd_elev_MEAN", "wtd_elev_STD", "wtd_slope_MIN",
  "wtd_slope_MAX", "wtd_slope_MEAN", "wtd_slope_STD", "wtd_north_per", 
  "wtd_wet_per","wtd_lake_per","wtd_glacier_per","wtd_area_sqKM")


cov.df <- cov.df.raw %>%
  mutate(wtd_area_sqKM = wtd_elev_AREA *1e-6) %>% 
  subset(select = wtd_keep_cols )

# Convert na to 0
cov.df[is.na(cov.df)] <- 0

cov.all <- left_join(sites.df %>% select(SiteID, cat_ID_con, Region = region, str_ord, str_slope, dist_coast_km),
                     cov.df)

cov.all <- left_join(cov.all, lcld.long)
# summary(cov.all)


```

Early exploration turned up lots of small problems that Dustin has fixed -- all recorded on [google sheet](https://docs.google.com/spreadsheets/d/1TJrNIwr14HJ4QV6-fB4ubmi0BoTNCG7UxoK-s--IsJM/edit#gid=624864644), QA review tab.

```{r covariate exploration, eval = FALSE}

names(cov.all)

cov.all %>% 
  group_by(Region) %>% 
  summarize(meanslope = mean(wtd_slope_MEAN))

cov.all %>% 
  group_by(Region) %>% 
  summarize(meanslope = mean(wtd_elev_MEAN))

#2011 all fixed now.
cov.all %>% 
  filter(wtd_lcld_jd <0)

cov.all %>% 
  mutate(yr11 = case_when(Year == 2011 ~ 1, 
                          TRUE ~ 0)) %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = wtd_lcld_jd, color = as.factor(yr11))) +
  geom_point() +
  facet_wrap(~Region)


cov.all %>% 
  # filter(!Year == 2011) %>% 
  ggplot(aes(x = wtd_elev_MEAN, y = wtd_lcld_jd, color = Region)) +
  geom_point() +
  geom_smooth() 
  facet_wrap(~Year)
  

```

Comparing modis output with snowtel.

```{r snow exploration, eval = FALSE}

#these sort of match up to hillside snowtel (2006-2019) april 1st swe 
# lowest years 2015, 2014, 2019
# biggest years 2012 by a lot, then 2007-2010 all about the same
cov.all %>% 
  group_by(Region, Year) %>% 
  summarize(meanlcld = mean(wtd_lcld_jd)) %>% 
  pivot_wider(names_from = Year, values_from = meanlcld, names_sort = TRUE)

cov.all %>% 
  ggplot() +
  geom_freqpoly(aes(x = wtd_lcld_jd, after_stat(density), color = Region)) +
  facet_wrap(~Year)
```

Generate a data frame that associates the SiteIDs in the response data frame with the covariate data frame. Tim's response data frame drops > 70 sites because they had limited summer data or data were prior to 2011 (he ran 2011-2020). We may go back and include older years now that we aren't running DFA so use the original model data frame sent to Tim (stream + air temps) and generate lookup for merging the SiteIDs. Then apply this to the response data frame with AR1 model output.

We added prefixes to most of the SiteIDs when generating the data that didn't get carried over to the spatial data frames. 

* find the SiteIDs that don't match in the two data frames (109) -- make sure all are converted to lower case because that is causing some mismatches.
* remove the prefix -- for just the mismatched sites -- so that that spatial SiteIDs will join.
* 13 sites in the response data frame not in the covariate data frame -- see [google sheet](https://docs.google.com/spreadsheets/d/1TJrNIwr14HJ4QV6-fB4ubmi0BoTNCG7UxoK-s--IsJM/edit#gid=1942759492) for how to fix them. 


```{r create siteid_join data frame, include = FALSE}

temp_dat <- readRDS("data_preparation/final_data/summer_data_wair2021-11-23.rds") %>% 
  mutate(site_lower = tolower(SiteID))

temp_md <- readRDS("data_preparation/final_data/md.rds")

cov.all <- cov.all %>% 
  mutate(site_lower = tolower(SiteID))

#sites in temperature data frame not in covariate data frame
remove_prefix <- anti_join(temp_dat %>% distinct(site_lower),
          cov.all %>% distinct(site_lower)) %>%
  arrange(site_lower) %>% 
  pull(site_lower)

siteid_join <- temp_dat %>% 
  mutate(cov_id = case_when(site_lower %in% remove_prefix ~  str_remove(site_lower, pattern = "^.*?(_|-)"),
                            TRUE ~ site_lower)) %>% 
  distinct(ts_id = site_lower, cov_id)

#13 mismatches
# anti_join(siteid_join, cov.all, by = c("cov_id" = "site_lower"))

siteid_join <- siteid_join %>% 
  mutate(cov_id = case_when(cov_id == "katm_lbrooo" ~ "katm_lbrooo_lvl",
                            cov_id == "katm_naknlo" ~ "katm_naknlo_lvl",
                            cov_id == "lacl_kijilo" ~ "lacl_kijilo_lvl",
                            cov_id == "lslil10" ~ "lslil",
                            cov_id == "lsnlt10" ~ "lsnlt1",
                            cov_id == "lsr93" ~ "lsr93lb",
                            cov_id == "lsr106" ~ "lsr112.5",
                            cov_id == "lacl_lclaro" ~ "lacl_lclaro_lvl",
                            TRUE ~ cov_id))

write.csv(siteid_join, "data_preparation/SiteID_join_spatial_to_temperature.csv")
```

Lots of sites in covariates not in response data frame -- check on how Tim filtered data completeness, which should explain why sites were dropped. For each site and year, Tim dropped any with less than 80% of days in June 1 - Aug 31 (jd 152-243). And, he only used sites with data from 2011-2020.


```{r sites dropped in arima analysis, eval = FALSE}

temp_dat %>% 
  # filter(year(sampleDate) %in% 2001:2019) %>% 
  distinct(SiteID) %>% nrow()
ts_dat %>% distinct(Site) %>% nrow() #dropped 50 overall, 27 when filter on year only, so other 23 presumably didn't have complete summers of data

anti_join(temp_dat %>% distinct(SiteID), ts_dat %>% distinct(SiteID = Site))

# 49 sites
temp_dat %>%
  mutate(Year = year(sampleDate), jd = as.numeric(format(sampleDate, "%j"))) %>% 
  filter(jd > 151, jd < 244) %>% 
  group_by(SiteID, Year) %>% 
  summarize(sum_percent = n()/92) %>%
  mutate(keep = case_when(sum_percent >= 0.8 ~ 1,
                          Year < 2011 ~ 0,
                          Year > 2019 ~ 0,
                          TRUE ~ 0)) %>% 
  group_by(SiteID) %>% 
  summarize(sum_complete = sum(keep)) %>% 
  filter(sum_complete == 0)
```

Use merge table to join on SiteIDs. One site in responses without spatial data - Nerka Pike Creek, only 1 year, will be dropped.


```{r create model data frame, include = FALSE}

# nerka pike creek is only missing site now
left_join(ts_dat %>% mutate(site_lower = tolower(Site)), siteid_join, by = c("site_lower" = "ts_id")) %>% 
  filter(Year %in% 2001:2019) %>% 
  left_join(cov.all %>% rename(cov_id = site_lower)) %>%
  distinct(Site)
  filter(is.na(str_ord))

mod_dat <- left_join(ts_dat %>% mutate(site_lower = tolower(Site)), siteid_join, by = c("site_lower" = "ts_id")) %>% 
  filter(!Year == 2020) %>% 
  left_join(cov.all %>% rename(cov_id = site_lower)) %>% 
  filter(!is.na(str_ord))

```


## Add in daymet precipitation

Tim C. included total summer precipitation as a predictor in his models so add from daymet here. Note that this is site specific precip, not from across the watershed.

```{r merge with precip}

daymet <- read_csv("data_preparation/daymet/site_daymet.csv")
daymet %>% distinct(measurement)

#correct conversion is date2 bc date example below has Jan 1 as day 0!
daymet %>% 
  filter(year == 2006, 
         yday == 152) %>% 
  mutate(date = as.Date(yday, origin = paste0(year, "-01-01")),
         date2 = as.Date(paste(year, yday), format = "%Y %j"))

precip <- daymet %>%
  filter(measurement %in% c("prcp..mm.day.")) %>% 
  mutate(sampleDate = as.Date(paste(year, yday), format = "%Y %j")) %>% 
  filter(month(sampleDate) %in% 6:8) %>% 
  rename(SiteID = site) %>% 
  group_by(SiteID, Year = year(sampleDate)) %>% 
  summarize(summer_precip = mean(value))

rm(daymet)

mod_dat <- left_join(mod_dat %>% select(-(site_lower:SiteID)), precip %>% rename(Site = SiteID))

```



# Data exploration

## Response

Look at sites with lots of negative TS as a check.

* NPS Hardenburg bay is a bay in Lake Clark - drop.
* usgs 15258000 is outlet of kenai lake, but still seems very lake affected or lake-like (kenai r at cooper landing) - drop
* usgs 15260001 is outlet of cooper lake right below hydro dam - this seems very anthro-influenced -- drop
* usfs solf lake fish pass and outlet creek are both located in lake -- is that right, should they be dropped?

```{r negative ts values}
mod_dat %>% 
  filter(TempSens < 0) %>%
  arrange(TempSens)

drop_sites <- c("NPS_Hardenburg_Bay", "usgs_15258000", "usgs_15260001")

mod_dat2 <- mod_dat %>% 
  filter(!Site %in% drop_sites)

```

Any outliers in TS?

```{r ts boxplot by region}
mod_dat2 %>% 
  ggplot(aes(x = Region, y = TempSens)) +
  geom_boxplot()
```


```{r ts dotplot}
mod_dat2 %>% 
  arrange(TempSens) %>% 
  mutate(rowid = row_number()) %>%
  ggplot(aes(x = TempSens, y = rowid,  color = Region)) +
  geom_point() +
  # scale_y_continuous(limits = c(0, 1500)) +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),
        legend.position = "bottom", axis.title.y = element_blank())
```

Two sites with rather high TS.

```{r}
mod_dat2 %>% 
  filter(TempSens > 0.7)

```

```{r ts histogram}
mod_dat2 %>% 
  ggplot(aes(x = TempSens)) +
  geom_histogram() +
  facet_wrap(~Region) +
  theme_bw()

```



## Covariates 

Checking outliers and collinearity. Drop those with high correlation.

Check slopes first. A couple of outliers -- one for stream slope and two for minimum catchment slope. We checked the min catchment slopes and they are accurate, streams in very steep watersheds. One of those aslo had the high stream slope. Note that the mean, min, max statistics for slope and elevation by watershed are all highly correlated so just go with means for now and recheck using r and vif.

Keep:

* stream slope
* mean catchment slope and elevation
* mean watershed slope and elevation
* watershed percent north facing 
* distance to coast
* watershed area
* wetland, glacier, lake cover
* modis snow date

```{r pair plots for groups of covariates}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}

#slope and aspect
mod_dat %>% 
  select(str_slope, cat_slope_MIN:cat_slope_STD, wtd_slope_MIN:wtd_slope_STD, wtd_north_per) %>% 
  pairs(upper.panel = panel.cor)

#elevation, area, and distance to coast
mod_dat %>% 
  select(cat_elev_MIN:cat_elev_STD, wtd_elev_MIN:wtd_elev_STD, wtd_area_sqKM, dist_coast_km) %>% 
  pairs(upper.panel = panel.cor)

#landcover and snow date
mod_dat %>% 
  select(wtd_wet_per:wtd_lcld_jd) %>% 
  pairs(upper.panel = panel.cor)


```


Correlations for all covariates in list above. All r < 0.7, which is good.

```{r pair plot for 12 cov}
mod_dat %>% 
  select(str_slope, cat_elev_MEAN, cat_slope_MEAN, wtd_north_per,
         wtd_elev_MEAN, wtd_slope_MEAN, wtd_area_sqKM, dist_coast_km,
         wtd_wet_per, wtd_glacier_per, wtd_lake_per, wtd_lcld_jd) %>% 
  pairs(upper.panel = panel.cor)
```

MODIS lcld as a function of watershed slope (Fig 2C in Tim's paper).

```{r lcld versus watershed slope}
mod_dat2 %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = wtd_lcld_jd)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ I(x^2))
```

Generate snow index as residuals of model of lcld as a function of watershed slope (per Tim's paper). Compare squared watershed slope versus linear model using AIC.

```{r snow index}
ess_lme1 <- lme(wtd_lcld_jd ~ I(wtd_slope_MEAN^2), dat = mod_dat2, random = ~1|Year)
ess_lme2 <- lme(wtd_lcld_jd ~ wtd_slope_MEAN, dat = mod_dat2, random = ~1|Year)
AIC(ess_lme1, ess_lme2)

ess_lme_resid <- resid(ess_lme1, level = 1)

mod_dat2 <- mod_dat2 %>% 
  mutate(snow_ind = ess_lme_resid)

#check that snow index is now not correlated to watershed slope.
mod_dat2 %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = snow_ind)) +
  geom_point() +
  geom_smooth(method = "lm")

```


Logit transformation of glacier and lake cover because both are percentages and have large outliers. Log transform watershed area and stream slope and replot. Transformation for slope needs minimum value because one 0 (following methods in mccune and grace 2002).


```{r covariate transformations}

# mod_dat2 %>% 
#   distinct(str_slope) %>% 
#   arrange(str_slope)

min_slope = 0.0000100000
c_slope = as.integer(log10(min_slope))
d_slope = 10^c_slope

mod_dat2 <- mod_dat2 %>% 
  mutate(log_area = log10(wtd_area_sqKM),
         log_slope = log10(str_slope + min_slope),
         logit_glac = logit(wtd_glacier_per),
         logit_lake = logit(wtd_lake_per))

mod_dat2 %>% 
  select(log_slope, cat_elev_MEAN, cat_slope_MEAN, wtd_north_per,
         wtd_elev_MEAN, wtd_slope_MEAN, log_area, dist_coast_km,
         wtd_wet_per, logit_glac, logit_lake, snow_ind) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist)

```

Still one site in Bristol Bay -- Egegik River -- with very high lake cover not fixed by logit transformation. ~37%, next highest is 17%.

```{r lake outlier}
mod_dat2 %>% filter(logit_lake > -1)

temp_md %>% 
  filter(SiteID == "fws_580223156504200")

```


Set up a basic model to get VIF. 

```{r vif for 12 cov}
lm1 <- lm(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
         wtd_elev_MEAN + wtd_slope_MEAN + log_area + dist_coast_km + 
         wtd_wet_per + logit_glac + logit_lake + snow_ind + summer_precip, data = mod_dat2)

vif(lm1)
```

Remove mean watershed elevation now all vif < 3.

```{r vif 11 cov}
lm2 <- lm(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
          wtd_slope_MEAN + log_area + dist_coast_km + 
         wtd_wet_per + logit_glac + logit_lake + snow_ind + summer_precip, data = mod_dat2)

vif(lm2)
```

Save model data frame with snow index and covariate transformations.

```{r}

saveRDS(mod_dat2, paste("data_preparation/final_data/model_data", Sys.Date(), ".rds", sep = ""))

```



# Mixed effects model

Simple mixed effects model for this dataset. 

* split data into training and test sets (80/20).
* assess need for random effect for region/site and site using AIC -- lme with REML.
* generate global model with 

```{r read in data}

mod_dat <- readRDS("data_preparation/final_data/model_data2022-01-19.rds")

md <- read_rds("data_preparation/final_data/md.rds")
mod_dat <- left_join(mod_dat, md %>% dplyr::select(SiteID, deshka, HUC8, Name, Latitude, Longitude), by = c("Site" = "SiteID")) 

# #80% of the sample size for training the model
# smp_size <- floor(0.8 * nrow(mod_dat))
# 
# ## set the seed to make your partition reproducible
# set.seed(123)
# train_ind <- sample(seq_len(nrow(mod_dat)), size = smp_size)
# 
# train <- mod_dat[train_ind, ]
# test <- mod_dat[-train_ind, ]

```


Random effects on global model.

1. should consider possible interactions - add 5 from Tim's results: north x snow, north x slope, north x precip, precip x slope, slope x snow.
2. Better for prediction would be hu12 or hu10. But can still include site within region but predict with level 1 (level 0 is population, level 1 is region, level 2 is site).
3a. use liklihood ratio test or step aic to get a simpler model and evaluate using 10 fold xval and test set.
3b. alternatively try glmmlasso to use regularization to minimize model coefficients for unimportant predictors. (and other recommendations in Treddenick 2021). I could not get this to run!
3c. or start with global model, use dredge and cross-validate all models with AIC < 6 (Tim's cutoff), which is ~300,
4. for xval, could set up a temporal using years and a separate spatial using huc8 or other major grouping since plan will be to predict for normal year + 2019 and also across all catchments.

```{r random effects}
lme1Ar <- lme(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                wtd_slope_MEAN + log_area + dist_coast_km + 
                wtd_wet_per + logit_glac + logit_lake + snow_ind +summer_precip +
                wtd_north_per*snow_ind + wtd_north_per*wtd_slope_MEAN + wtd_north_per*summer_precip +
                summer_precip*wtd_slope_MEAN + wtd_slope_MEAN*snow_ind, 
              data = mod_dat, random = ~1 | Region/Site, method = "REML")

lme1Br <- lme(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                wtd_slope_MEAN + log_area + dist_coast_km + 
                wtd_wet_per + logit_glac + logit_lake + snow_ind +summer_precip +
                wtd_north_per*snow_ind + wtd_north_per*wtd_slope_MEAN + wtd_north_per*summer_precip +
                summer_precip*wtd_slope_MEAN + wtd_slope_MEAN*snow_ind, 
              data = mod_dat, random = ~1 | Site, method = "REML")

lme1Cr <- lme(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                wtd_slope_MEAN + log_area + dist_coast_km + 
                wtd_wet_per + logit_glac + logit_lake + snow_ind +summer_precip +
                wtd_north_per*snow_ind + wtd_north_per*wtd_slope_MEAN + wtd_north_per*summer_precip +
                summer_precip*wtd_slope_MEAN + wtd_slope_MEAN*snow_ind, 
              data = mod_dat, random = ~1 | Region, method = "REML")

lme1Dr <- lme(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                wtd_slope_MEAN + log_area + dist_coast_km + 
                wtd_wet_per + logit_glac + logit_lake + snow_ind +summer_precip +
                wtd_north_per*snow_ind + wtd_north_per*wtd_slope_MEAN + wtd_north_per*summer_precip +
                summer_precip*wtd_slope_MEAN + wtd_slope_MEAN*snow_ind, 
              data = mod_dat, random = ~1 | HUC8, method = "REML")

lme1Er <- lme(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                wtd_slope_MEAN + log_area + dist_coast_km + 
                wtd_wet_per + logit_glac + logit_lake + snow_ind +summer_precip +
                wtd_north_per*snow_ind + wtd_north_per*wtd_slope_MEAN + wtd_north_per*summer_precip +
                summer_precip*wtd_slope_MEAN + wtd_slope_MEAN*snow_ind, 
              data = mod_dat, random = ~1 | HUC8/Site, method = "REML")

lme1Fr <- gls(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                wtd_slope_MEAN + log_area + dist_coast_km + 
                wtd_wet_per + logit_glac + logit_lake + snow_ind +summer_precip +
                wtd_north_per*snow_ind + wtd_north_per*wtd_slope_MEAN + wtd_north_per*summer_precip +
                summer_precip*wtd_slope_MEAN + wtd_slope_MEAN*snow_ind, 
              data = mod_dat, method = "REML")

AIC(lme1Ar, lme1Br, lme1Cr, lme1Dr, lme1Er, lme1Fr) %>% arrange(AIC)
```




```{r HUC8 intercepts}

mod_dat %>% count(HUC8)

left_join(as_tibble(lme1Er$coefficients$random$HUC8, rownames = "HUC8"), mod_dat %>% distinct(HUC8, Name)) %>% 
  arrange(`(Intercept)`)
```



Move forward with fixed effects, use AIC or liklihood ratio test to remove parameters. Switch to ML.

```{r include = FALSE}
lme1m <- lme(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                wtd_slope_MEAN + log_area + dist_coast_km + 
                wtd_wet_per + logit_glac + logit_lake + snow_ind +summer_precip +
                wtd_north_per*snow_ind + wtd_north_per*wtd_slope_MEAN + wtd_north_per*summer_precip +
                summer_precip*wtd_slope_MEAN + wtd_slope_MEAN*snow_ind, 
             data = mod_dat, random = ~1 | HUC8/Site, method = "ML")


lme1m_nosite <- lme(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                wtd_slope_MEAN + log_area + dist_coast_km + 
                wtd_wet_per + logit_glac + logit_lake + snow_ind +summer_precip +
                wtd_north_per*snow_ind + wtd_north_per*wtd_slope_MEAN + wtd_north_per*summer_precip +
                summer_precip*wtd_slope_MEAN + wtd_slope_MEAN*snow_ind, 
             data = mod_dat, random = ~1 | HUC8, method = "ML")

plot(lme1m)
hist(resid(lme1m))
summary(lme1m)

data.frame(preds = predict(lme1m, newdata = test, level = 1), test %>% select(TempSens, Site)) %>% 
  mutate(error.sq = (preds - TempSens)^2,
         error.abs = abs(preds - TempSens)) %>% 
  summarize(rmse = sqrt(mean(error.sq)),
            mae = mean(error.abs))

#model with site grouping doesn't improve predictions at regional level.
data.frame(preds = predict(lme1m_nosite, newdata = test, level = 1), test %>% select(TempSens, Site)) %>% 
  mutate(error.sq = (preds - TempSens)^2,
         error.abs = abs(preds - TempSens)) %>% 
  summarize(rmse = sqrt(mean(error.sq)),
            mae = mean(error.abs))


r.squaredGLMM(lme1m_nosite)
r.squaredGLMM(lme1m)


```

## model selection


Alternative is to dredge the global model for all subsets and examine most important predictors by summing the model weight for the models in which each covariate is included.


```{r dredge global model}
lme1m <- lme(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                wtd_slope_MEAN + log_area + dist_coast_km + 
                wtd_wet_per + logit_glac + logit_lake + snow_ind +summer_precip +
                wtd_north_per*snow_ind + wtd_north_per*wtd_slope_MEAN + wtd_north_per*summer_precip +
                summer_precip*wtd_slope_MEAN + wtd_slope_MEAN*snow_ind, 
             data = mod_dat, random = ~1 | HUC8/Site, method = "ML")


lme1_dr <- dredge(lme1m, rank = "AIC")
saveRDS(lme1_dr, "output/lme1m_dredge_results.rds")
```

Look at dredge results and use subset for model-selection. Test using cross validation. 
Temporal cross-validation is 5 groups that are relatively balanced:

* 2001-2011
* 2012-2014
* 2015-2016
* 2017-2018
* 2019


```{r generate temporal xval groups}

mod_dat %>% count(Year) #not much data in early years

#generate balanced groups using quantile
yr_breaks <- quantile(mod_dat$Year, probs = seq(0, 1, 0.2)) %>% unique()
yr_breaks[1] <- 2000 #replace leftmost bound with 2000 so 2001 is included

#add temporal xval groups
mod_dat <- mod_dat %>% 
  mutate(yeargr = cut(Year, yr_breaks))

mod_dat %>% 
  count(yeargr)
mod_dat %>% 
  count(Region)

mod_dat %>% 
  distinct(Region, Site) %>% 
  count(Region)

mod_dat %>% 
  distinct(yeargr, Site) %>% 
  count(yeargr)

```

I think that finding the distance at which sites are correlated is a good guidance for picking spatial xval groups, but I don't really understand this so need to dig further if it is important to guide the spatial groups.

```{r spatial variogram}

dists <- distm(mod_dat %>% select(Longitude, Latitude), fun = distCosine)
summary(dists)

dists <- dist(mod_dat %>% select(Longitude, Latitude))


breaks <- seq(0, 20, l = 11)

lme1reml <- lme(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                wtd_slope_MEAN + log_area + dist_coast_km + 
                wtd_wet_per + logit_glac + logit_lake + snow_ind +summer_precip +
                wtd_north_per*snow_ind + wtd_north_per*wtd_slope_MEAN + wtd_north_per*summer_precip +
                summer_precip*wtd_slope_MEAN + wtd_slope_MEAN*snow_ind, 
             data = mod_dat, random = ~1 | Region/Site, method = "REML")


resid(lme1reml)
var <- nlme::Variogram(resid(lme1reml), dists)
var <- Variogram(lme1reml, dists, "normalized", maxDist = 20)
plot(var, smooth = TRUE)
```



I tried using region for a first go, but then I can't predict using that using the null model, so try other grouping based on combingin HUC8 that is a little larger (8 groups).

Possibly change this to huc10 because for the final model we'll be able to use the huc8 levels for prediction so we want all in each training dataset so we can predict with level 1 groups. If I randomly combine different HU10 then the sites are still spatially clustered but don't include all sites within a HU8.

```{r generate spatial xval groups}

mod_dat %>% 
  distinct(Site, HUC8, Name) %>% 
  count(HUC8, Name)

#this results in 9 groups, but one with just 5, combine with closest HUC6 to west.
mod_dat %>% 
  mutate(HUC6 = substr(HUC8, 1, 6)) %>% 
  distinct(Site, HUC6) %>% 
  count(HUC6)

mod_dat <- mod_dat %>% 
  mutate(HU6 = substr(HUC8, 1, 6),
         sp_xval = case_when(HU6 %in% c(190206, 190302) ~ "190206_302",
                             TRUE ~ as.character(HU6))) %>% 
  group_by(sp_xval) %>%
  mutate(huc8_names = toString(unique(Name))) %>% 
  ungroup()

mod_dat %>% 
  distinct(Site, sp_xval) %>% 
  count(sp_xval)

```


```{r hu10 xval groups}
hu10 <- st_read(dsn = "S:/Leslie/GIS/NHD Harmonized/WBD_19_GDB.gdb", layer = "WBDHU10")
st_crs(hu10)
hu10_wgs84 <- st_transform(hu10, crs = "WGS84")

md_sf <- st_as_sf(md, coords = c("Longitude", "Latitude"), crs = "WGS84")

st_is_valid(hu10_wgs84) %>% sum() #2410 that is correct

```

This still isn't working because if I shoot for a similar size of spatial xval groups as there are huc8, there is still a 1:1 relationship between huc8 and group, which means we aren't predicting with the random intercept, which really improves the model. Try randomly assigning huc10 to groups that are small.

```{r add huc10 code}

ptm <- proc.time()
md_sf2 <- st_join(md_sf, hu10_wgs84 %>% select(HU10_Name = Name, HUC10))
join_time <- proc.time() - ptm
join_time

#limit to just 408 sites with data
huc10_groups <- left_join(mod_dat %>% distinct(Site), md_sf2, by = c("Site" = "SiteID"))

#create random groups of huc10 across the study area by ordering by huc10 and assigning ids of 1-21 (each group will have 6 huc10)
#this gives us 21 groupings of huc10 with 7 - 48 sites in each group (6 huc10 in each group)
huc10_groups <- left_join(huc10_groups, huc10_groups %>% 
                            arrange(HUC10) %>% 
                            count(HUC10) %>% 
                            mutate(gr_id = rep(seq(1:21), 6)))

huc10_groups %>% 
  count(gr_id)

ggplot() +
  geom_sf(data = left_join(md_sf2, huc10_groups), aes(color = as.factor(gr_id)))

mod_dat <- left_join(mod_dat, huc10_groups %>% select(Site, HUC10, gr_id)) 
  
```


```{r variable importance}

lme1_dr <- readRDS("output/lme1m_dredge_results.rds")

#variable importance
lme1_dr %>% 
  as_tibble() %>% 
  mutate(across(cat_elev_MEAN:'wtd_north_per:wtd_slope_MEAN', ~ case_when(!is.na(.) ~ weight))) %>% 
  select(cat_elev_MEAN:'wtd_north_per:wtd_slope_MEAN') %>% 
  colSums(., na.rm = TRUE) %>% 
  enframe() %>% 
  arrange(desc(value)) %>% 
  ggplot(aes(y = fct_reorder(name, value), x = value)) +
  geom_point()

```

Create model set based on variable importance:

* global model
* drop 6 covariates and interactions in group with lowest variable important - 11 left
* drop next group of 3 - 8 left
* drop final group of 5 only keeping top 3 predictors


```{r setup xval model formulas}

lme_all <- formula(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                wtd_slope_MEAN + log_area + dist_coast_km + 
                wtd_wet_per + logit_glac + logit_lake + snow_ind +summer_precip +
                wtd_north_per*snow_ind + wtd_north_per*wtd_slope_MEAN + wtd_north_per*summer_precip +
                summer_precip*wtd_slope_MEAN + wtd_slope_MEAN*snow_ind)

lme_11vars <- formula(TempSens ~ cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                wtd_slope_MEAN + dist_coast_km + 
                logit_glac + logit_lake + snow_ind + summer_precip +
                wtd_north_per*wtd_slope_MEAN + 
                summer_precip*wtd_slope_MEAN)

lme_8vars <- formula(TempSens ~ wtd_north_per +
                wtd_slope_MEAN + dist_coast_km + 
                logit_glac + logit_lake + snow_ind + summer_precip +
                  summer_precip*wtd_slope_MEAN)

lme_3vars <- formula(TempSens ~ wtd_slope_MEAN + summer_precip +
                  summer_precip*wtd_slope_MEAN)
  
```

Temporal xvalidation. For each test set, make predictions using the models built from the training data and the set of years in the test set. Once the OOB predictions are saved, use them to calculate a data frame with model performance metrics for each model: RMSE, MAE, and obs v pred. 

```{r temporal cross-validation}

tempxval_preds <- data.frame()

for(i in 1:5) {
  temp_xval <- mod_dat %>% distinct(yeargr) %>% slice(i) 
  train <- mod_dat %>% 
    anti_join(temp_xval)
  test <- mod_dat %>% 
    right_join(temp_xval)
  
  #global model
  lmegl <- lme(lme_all, data = train, random = ~1 | HUC8/Site, method = "REML")    
  lme2 <- lme(lme_11vars, data = train, random = ~1 | HUC8/Site, method = "REML")    
  lme3 <- lme(lme_8vars, data = train, random = ~1 | HUC8/Site, method = "REML")    
  lme4 <- lme(lme_3vars, data = train, random = ~1 | HUC8/Site, method = "REML")    
  lme5 <- lme(TempSens~1, data = train, random = ~1 | HUC8/Site, method = "REML")
  rf1 <- randomForest(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                        wtd_slope_MEAN + log_area + dist_coast_km + wtd_wet_per + 
                        logit_glac + logit_lake + snow_ind + summer_precip, 
                      data = train %>% group_by(Site) %>% sample_n(1),
                      ntree=1000, importance=TRUE,proximity=TRUE,mtry=5)
  
  tempxval_preds <- bind_rows(tempxval_preds,
                            bind_cols(test %>% select(Site, Year, TempSens), 
                                      global = predict(lmegl, newdata = test, level = 1),
                                      lme_11vars = predict(lme2, newdata = test, level = 1),
                                      lme_8vars = predict(lme3, newdata = test, level = 1),
                                      lme_3vars = predict(lme4, newdata = test, level = 1),
                                      lme_null = predict(lme5, newdata = test, level = 1),
                                      rf = predict(rf1, newdata = test)) %>% 
                              pivot_longer(names_to = "model", values_to = "preds", cols = global:rf) %>% 
                              mutate(xval_type = "temporal",
                                     xval_group = temp_xval %>% pull(yeargr) %>% as.character())
  )
}


tempxval_preds <- xval_preds %>% filter(xval_type == "temporal")

tempxval_results <- tempxval_preds %>% 
  mutate(sqerror = (preds - TempSens)^2,
         abserror = abs(preds - TempSens)) %>%
  group_by(model, xval_type, xval_group) %>% 
  summarize(rmse = sqrt(mean(sqerror, na.rm = TRUE)),
            mae = mean(abserror, na.rm = TRUE),
            obs_pred = cor(preds, TempSens, use = "pairwise.complete.obs"),
            obs_pred_rank = cor(preds, TempSens, method = "spearman", use = "pairwise.complete.obs")) 

tempxval_results %>% 
  ggplot(aes(x = xval_group, y = rmse, color = model)) +
  geom_point()

```

Everything I tried above to get spatial xval groups that would allow me to predict at level 1 isn't working. I would have to predict site by site and some would be level 0 and some would be level 1. Just use huc8 as spatial xval groups and predict at level 0.

These are new spatial cross-validation groups where I sequentially placed HUC10 in 21 groups (6 HUC10 each) in order to try and randomize the spatial clusters of sites across the study area in each test set (rather than all together). Predictions are still at population level because many of these groups overlap with HUC8.

```{r spatial cross-validation}

spxval_preds <- data.frame()

for(i in 1:21) {
  train <- mod_dat %>% filter(!gr_id == i)
  test <- mod_dat %>% filter(gr_id == i)

  #global model
  lmegl <- lme(lme_all, data = train, random = ~1 | HUC8/Site, method = "REML")    
  lme2 <- lme(lme_11vars, data = train, random = ~1 | HUC8/Site, method = "REML")    
  lme3 <- lme(lme_8vars, data = train, random = ~1 | HUC8/Site, method = "REML")    
  lme4 <- lme(lme_3vars, data = train, random = ~1 | HUC8/Site, method = "REML")    
  lme5 <- lme(TempSens~1, data = train, random = ~1 | HUC8/Site, method = "REML")
  #for rf, make sure just one year from each site
  rf1 <- randomForest(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                        wtd_slope_MEAN + log_area + dist_coast_km + wtd_wet_per + 
                        logit_glac + logit_lake + snow_ind + summer_precip, 
                      data = train %>% group_by(Site) %>% sample_n(1),
                      ntree=1000, importance=TRUE,proximity=TRUE,mtry=5)
  
spxval_preds <- bind_rows(spxval_preds,
                            bind_cols(test %>% select(Site, Year, TempSens), 
                                      global = predict(lmegl, newdata = test, level = 0),
                                      lme_11vars = predict(lme2, newdata = test, level = 0),
                                      lme_8vars = predict(lme3, newdata = test, level = 0),
                                      lme_3vars = predict(lme4, newdata = test, level = 0),
                                      lme_null = predict(lme5, newdata = test, level = 0),
                                      rf = predict(rf1, newdata = test)) %>% 
                              pivot_longer(names_to = "model", values_to = "preds", cols = global:rf) %>% 
                              mutate(xval_type = "spatial",
                                     xval_group = i)
  )
}

# spxval_preds <- xval_preds %>% filter(xval_type == "spatial")

spxval_results <- spxval_preds %>% 
  mutate(sqerror = (preds - TempSens)^2,
         abserror = abs(preds - TempSens)) %>%
  group_by(model, xval_type, xval_group) %>% 
  summarize(rmse = sqrt(mean(sqerror, na.rm = TRUE)),
            mae = mean(abserror, na.rm = TRUE),
            obs_pred = cor(preds, TempSens, use = "pairwise.complete.obs"),
            obs_pred_rank = cor(preds, TempSens, method = "spearman", use = "pairwise.complete.obs")) 

spxval_results %>% 
  ggplot(aes(x = xval_group, y = rmse, color = model)) +
  geom_point()
  
```



```{r spatial LOO cross-validation}

sploo_preds <- data.frame()

# DF of sites: remove any that comprise a single huc8 so can predict using level 1.
sites <- mod_dat %>% distinct(Site, HUC8)
sites <- sites %>% 
  group_by(HUC8) %>% 
  mutate(count = n()) %>% 
  filter(count > 1) %>% 
  ungroup()
  
for(i in 1:nrow(sites)) {
  sp_xval <- sites %>% slice(i) 
  train <- mod_dat %>%
    anti_join(sp_xval)
  test <- mod_dat %>%
    right_join(sp_xval)
  
  #global model
  lmegl <- lme(lme_all, data = train, random = ~1 | HUC8/Site, method = "REML")    
  lme2 <- lme(lme_11vars, data = train, random = ~1 | HUC8/Site, method = "REML")    
  lme3 <- lme(lme_8vars, data = train, random = ~1 | HUC8/Site, method = "REML")    
  lme4 <- lme(lme_3vars, data = train, random = ~1 | HUC8/Site, method = "REML")    
  lme5 <- lme(TempSens~1, data = train, random = ~1 | HUC8/Site, method = "REML")
  #for rf, make sure just one year from each site
  rf1 <- randomForest(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                        wtd_slope_MEAN + log_area + dist_coast_km + wtd_wet_per + 
                        logit_glac + logit_lake + snow_ind + summer_precip, 
                      data = train %>% group_by(Site) %>% sample_n(1),
                      ntree=1000, importance=TRUE,proximity=TRUE,mtry=5)
  
  sploo_preds <- 
    bind_rows(sploo_preds, 
              bind_cols(test %>% select(Site, Year, TempSens), 
                        global = predict(lmegl, newdata = test, level = 1),
                        lme_11vars = predict(lme2, newdata = test, level = 1),
                        lme_8vars = predict(lme3, newdata = test, level = 1),
                        lme_3vars = predict(lme4, newdata = test, level = 1),
                        lme_null = predict(lme5, newdata = test, level = 1),
                        rf = predict(rf1, newdata = test)) %>% 
                pivot_longer(names_to = "model", values_to = "preds", cols = global:rf) %>% 
                mutate(xval_type = "LOO",
                       xval_group = sp_xval %>% pull(Site))
    )
}

# sploo_preds <- xval_preds %>% filter(xval_type == "LOO")

sploo_results <- sploo_preds %>% 
  mutate(sqerror = (preds - TempSens)^2,
         abserror = abs(preds - TempSens)) %>%
  group_by(model, xval_type) %>% 
  summarize(rmse = sqrt(mean(sqerror, na.rm = TRUE)),
            mae = mean(abserror, na.rm = TRUE),
            obs_pred = cor(preds, TempSens, use = "pairwise.complete.obs"),
            obs_pred_rank = cor(preds, TempSens, method = "spearman", use = "pairwise.complete.obs")) 
```


```{r rmse plot for different xval types}
bind_rows(tempxval_results, spxval_results %>% mutate(xval_group = as.character(xval_group)), sploo_results) %>% 
  mutate(modelf = factor(model, levels = c("global", "lme_11vars", "lme_8vars", "lme_3vars", "lme_null", "rf"),
                         labels = c("global", "11 vars.", "8 vars.", "3 vars.", "null", "RF"))) %>% 
  ggplot(aes(x = modelf, y = rmse, color = xval_type)) +
  stat_summary(fun = "mean", geom = "point") +
  stat_summary(fun.data = "mean_se", geom = "errorbar", width = 0.2) +
  labs(y = "RMSE on Validation Data", x = "Model", color = "Cross-validation Type") +
  theme_bw() +
  theme(legend.position = "bottom")

bind_rows(tempxval_results, spxval_results %>% mutate(xval_group = as.character(xval_group)), sploo_results) %>% 
  saveRDS("output/xval_results.rds")

```



```{r loo observed v predicted}
left_join(sploo_preds, mod_dat %>% distinct(Site, Region))  %>% 
  mutate(modelf = factor(model, levels = c("global", "lme_11vars", "lme_8vars", "lme_3vars", "lme_null", "rf"),
                         labels = c("global", "11 vars.", "8 vars.", "3 vars.", "null", "RF"))) %>% 
  ggplot(aes(x = TempSens, y = preds)) +
  geom_point() +
  geom_abline(aes(intercept = 0, slope = 1)) +
  facet_grid(cols = vars(Region), rows = vars(modelf)) +
  coord_cartesian(xlim = c(-0.2, 1), ylim = c(-0.2,1)) +
  stat_cor(label.x.npc = 0, label.y = c(seq(0.6, 1, 0.1)))

```

Save predictions for all of the cross-validations.

```{r}
left_join(sploo_preds, mod_dat %>% distinct(Site, Region))  %>% 
  bind_rows(tempxval_preds, spxval_preds %>% mutate(xval_group = as.character(xval_group))) %>% 
  saveRDS("output/xval_preds.rds")

# xval_preds <- readRDS("output/xval_preds.rds")

summary(xval_preds)
xval_preds %>% filter(is.na(preds))
```

# Regional models

The models with all data are really only predicting Cook Inlet with much accuracy. Try building separate models for Kodiak or PWS or Copper River and see if they are much better. The dredging is where we ended up with different models. Try dredging for each region and see if the variable importance plots look different.

```{r dredge Copper River}
cr_dat <- mod_dat %>% filter(Region == "Copper_River")
cr_dat %>% count(Site) %>% arrange(n) # 28 sites with 1-10 years of data - 162 total site years

lme1m_cr <- lme(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                wtd_slope_MEAN + log_area + dist_coast_km + 
                wtd_wet_per + logit_glac + logit_lake + snow_ind +summer_precip +
                wtd_north_per*snow_ind + wtd_north_per*wtd_slope_MEAN + wtd_north_per*summer_precip +
                summer_precip*wtd_slope_MEAN + wtd_slope_MEAN*snow_ind, 
             data = cr_dat, random = ~1 | HUC8/Site, method = "ML")


lme1_cr_dr <- dredge(lme1m_cr, rank = "AIC")
# saveRDS(lme1_dr, "output/lme1m_dredge_results.rds")
```

```






# LASSO regularization 

Try LASSO for predictive model.

```{r glmmlasso demo}
data("soccer")

## generalized additive mixed model
## grid for the smoothing parameter

## center all metric variables so that also the starting values with glmmPQL are in the correct scaling

soccer[,c(4,5,9:16)]<-scale(soccer[,c(4,5,9:16)],center=T,scale=T)

soccer<-data.frame(soccer)

lambda <- seq(500,0,by=-5)

family = poisson(link = log)
 

################## Second Simple Method ###########################
## Using 5-fold CV to determine the optimal tuning parameter lambda
 
### set seed
set.seed(123)

N<-dim(soccer)[1]

ind<-sample(N,N)

lambda <- seq(500,0,by=-5)

kk<-5

nk <- floor(N/kk)

Devianz_ma<-matrix(Inf,ncol=kk,nrow=length(lambda))

## first fit good starting model

PQL<-glmmPQL(points~1,random = ~1|team,family=family,data=soccer)

Delta.start<-c(as.numeric(PQL$coef$fixed),rep(0,6),as.numeric(t(PQL$coef$random$team)))

Q.start<-as.numeric(VarCorr(PQL)[1,1])

for(j in 1:length(lambda))
{
  print(paste("Iteration ", j,sep=""))
  
  for (i in 1:kk)
  {
    if (i < kk)
    {
      indi <- ind[(i-1)*nk+(1:nk)]
    }else{
      indi <- ind[((i-1)*nk+1):N]
    }
    
    soccer.train<-soccer[-indi,]
    soccer.test<-soccer[indi,]
    
    glm2 <- try(glmmLasso(points~transfer.spendings  
                          + ave.unfair.score  + ball.possession
                          + tackles + ave.attend + sold.out, rnd = list(team=~1),  
                          family = family, data = soccer.train, lambda=lambda[j],switch.NR=F,final.re=TRUE,
                          control=list(start=Delta.start,q_start=Q.start))
                ,silent=TRUE) 
    
    if(class(glm2)!="try-error")
    {  
      y.hat<-predict(glm2,soccer.test)    
      
      Devianz_ma[j,i]<-sum(family$dev.resids(soccer.test$points,y.hat,wt=rep(1,length(y.hat))))
    }
  }
  print(sum(Devianz_ma[j,]))
}

Devianz_vec<-apply(Devianz_ma,1,sum)

 opt2<-which.min(Devianz_vec)

 glm2_final <- glmmLasso(points~transfer.spendings  
                         + ave.unfair.score + ball.possession
                         + tackles + ave.attend + sold.out, rnd = list(team=~1),  
                         family = family, data = soccer, lambda=lambda[opt2],switch.NR=F,final.re=TRUE,
                         control=list(start=Delta.start,q_start=Q.start))

 summary(glm2_final)
```

Try code above with our dataset, but with different cross-validation groups (10 sets of 2 years each and 5 regions).

```{r glmmlasso xval}


## center all metric variables so that also the starting values with glmmPQL are in the correct scaling

mod_dat

mod_dat_sc <- bind_cols(mod_dat %>% select(Region, TempSens) %>% mutate(Region = factor(Region)), 
                        data.frame(scale(mod_dat %>% select(log_slope, cat_elev_MEAN, cat_slope_MEAN, 
                                                            wtd_north_per,wtd_slope_MEAN, log_area, 
                                                            dist_coast_km, wtd_wet_per, logit_glac, 
                                                            logit_lake, snow_ind, summer_precip), scale = TRUE, center = TRUE)))


lambda <- seq(500,0,by=-5)

family = gaussian()
 

################## Second Simple Method ###########################
## Using 5-fold CV to determine the optimal tuning parameter lambda
 
### set seed
set.seed(123)

N<-dim(mod_dat_sc)[1]

ind<-sample(N,N)

lambda <- seq(500,0,by=-5)

kk<-5 #first attempt with just 5 random xval

nk <- floor(N/kk)

Devianz_ma<-matrix(Inf,ncol=kk,nrow=length(lambda))

## first fit good starting model

PQL<-glmmPQL(TempSens~1,random = ~1|Region,family=family,data=mod_dat_sc)

#note rep 0s by number of fixed effects
Delta.start<-c(as.numeric(PQL$coef$fixed),rep(0,17),as.numeric(t(PQL$coef$random$Region)))

Q.start<-as.numeric(VarCorr(PQL)[1,1])

for(j in 1:length(lambda)) {
  print(paste("Iteration ", j,sep=""))
  
  for (i in 1:kk)
  {
    if (i < kk)
    {
      indi <- ind[(i-1)*nk+(1:nk)]
    }else{
      indi <- ind[((i-1)*nk+1):N]
    }
    
    ts.train<-mod_dat_sc[-indi,]
    ts.test<-mod_dat_sc[indi,]
    
    glm2 <- try(glmmLasso(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                            wtd_slope_MEAN + log_area + dist_coast_km + 
                            wtd_wet_per + logit_glac + logit_lake + snow_ind +summer_precip +
                            wtd_north_per:snow_ind + wtd_north_per:wtd_slope_MEAN + wtd_north_per:summer_precip +
                            summer_precip:wtd_slope_MEAN + wtd_slope_MEAN:snow_ind, 
                          rnd = list(Region=~1),  
                          family = family, data = ts.train, lambda=lambda[j], switch.NR=F, final.re=TRUE,
                          control=list(start=Delta.start,q_start=Q.start))
                ,silent=TRUE) 
    
    print(class(glm2))
    
    if(class(glm2)!="try-error")
    {  
      y.hat<-predict(glm2,ts.test)    
      
      Devianz_ma[j,i]<-sum(family$dev.resids(ts.test$points,y.hat,wt=rep(1,length(y.hat))))
    }
  }
  print(sum(Devianz_ma[j,]))
}

Devianz_vec<-apply(Devianz_ma,1,sum)

opt2<-which.min(Devianz_vec)

glm2_final <- glmmLasso(points~transfer.spendings  
                         + ave.unfair.score + ball.possession
                         + tackles + ave.attend + sold.out, rnd = list(team=~1),  
                         family = family, data = soccer, lambda=lambda[opt2],switch.NR=F,final.re=TRUE,
                         control=list(start=Delta.start,q_start=Q.start))

 summary(glm2_final)
```




# Machine learning methods

First option is to build a random forest using subsets of the data. In this case, I randomly selected one year from each site to create 10 different forests and combined them to generate one model of 10,000 trees for prediction (per JAWRA 2020 paper).

```{r random forest by site}

sites <- mod_dat %>% distinct(Site) %>% pull(Site)
random.df <- data.frame()


for (i in sites) {
  yrs <- mod_dat %>% filter(Site %in% i) %>% distinct(Year) %>% pull(Year) 
  yrs10 <- rep(yrs,10)[1:10] #creates vector of 10 years
  ran.yrs <- sample(yrs10,10,replace=FALSE) #reorders vector randomly
  newrow <- data.frame(Site = i, Year = ran.yrs, Forest = 1:10)
  random.df <- bind_rows(random.df, newrow)
}

#initialize first random forest and then combine forests 2:10 to it.
rf_10 <- randomForest(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                      wtd_slope_MEAN + log_area + dist_coast_km + 
                      wtd_wet_per + logit_glac + logit_lake + snow_ind + Region, 
                    data = left_join(random.df %>% filter(Forest ==  1), mod_dat),
                    ntree=1000, importance=TRUE,proximity=TRUE,mtry=5)
rf_stats <- data.frame(mse = rf_10$mse, rsq = rf_10$rsq)


for (i in 2:10){
  rfnew <- randomForest(TempSens ~ log_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
                      wtd_slope_MEAN + log_area + dist_coast_km + 
                      wtd_wet_per + logit_glac + logit_lake + snow_ind + Region, 
                      data = left_join(random.df %>% filter(Forest ==  i), mod_dat),
                    ntree=1000, importance=TRUE,proximity=TRUE,mtry=5)
  rf_10 <- randomForest::combine(rf_10, rfnew)
  rf_stats <- bind_rows(rf_stats, data.frame(mse = rfnew$mse, rsq = rfnew$rsq))
}

varImpPlot(rf_10)

#xval MSE
rf_stats %>% 
  summarize(mean(mse),
            mean(rsq))


bind_cols(mod_dat %>% select(Site, Year, TempSens, Region), 
          rf = predict(rf_10, newdata = mod_dat)) %>% 
  ggplot(aes(x = TempSens, y = rf, color = Region)) +
  geom_point() +
  geom_abline(aes(intercept = 0, slope = 1)) +
  coord_cartesian(xlim = c(-0.2, 1), ylim = c(-0.2,1)) +
  stat_cor(label.x.npc = 0, label.y = c(seq(0.6, 1, 0.1)))
```





Other alternatives that allow for extrapolation:
lightgbm: https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf

# Climate data for 2001-2019

Anchorage forecast office at airport: GHCND:USC00500275
King salmon airport: GHCND:USW00025503 (missing 2006-2012)
Iliamna: GHCND:USW00025506 (even worse only 2001-2005)
Kodiak airport: GHCND:USW00025501
Cordova airport: GHCND:USW00026410

```{r}
mynoaakey = "LmempjqpgcLSxDQWiLvuaOAGmscrQCrb"


clim_dat <- meteo_pull_monitors(monitors = c('USC00500275', 'USW00025503', 'USW00025506'), date_min = "2001-01-01",
                      date_max = "2019-12-31", var = c("PRCP", "TAVG", "TMAX", "TMIN"))
```


