---
title: "2_ts_model"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(readxl)
library(lubridate)
library(GGally)
library(corrplot)
library(car)
library(nlme)
```


Modeling thermal sensitivities for five regions in Southern AK.

1. read in response data
2. read in and combine covariates
3. reduce covariate list: corrplot, VIF, etc.
4. transform/model covariates as needed. e.g. logit transform for 0-1 percentages and/or lcld ~ wtd_slope model resid.
4. model thermal sensitivities: mixed model or RF with random effects

# Read in and combine model data

New AR(1) model output from Tim.

```{r responses}
ts_dat <- readRDS("DFA/TempSens_Arima_Results2021.12.07.RDS") %>% as_tibble()

# write_csv(ts_dat, "DFA/TempSens_Arima_Results2021.12.07.csv")
```


Read in covariates.

Copied over from Dustin's script, but modified so that lcld is combined with the other covariates.

```{r covariates}
getwd()
#Data from covariates script
cov.df.raw <- read.csv(file = 'data_preparation/sensitivity_drivers/AKSSF_Covariates.csv', header = TRUE) 
#Data from spatial join script
sites.df <- read.csv(file = 'data_preparation/sensitivity_drivers/AKSSF_sites_sj_maxfac.csv', header = TRUE)
#Data from Modis Script
lcld.df <- read.csv(file = 'data_preparation/sensitivity_drivers/AKSSF_wtd_lcld_mn.csv', header = TRUE)

lcld.long <- lcld.df %>% 
  pivot_longer(cols = wtd_lcld_mn_2001:wtd_lcld_mn_2019, names_to = "metric", values_to = "wtd_lcld") %>% 
  mutate(Year = as.numeric(substr(metric, 13, 17)),
         wtd_lcld_jd = wtd_lcld - 365) %>% 
  select(-metric)

#dput(colnames(sites.df))
site_keep_cols <- c("SiteID","region", "cat_ID_con", "site_max_acc", "site_acc_sqKm",
                    "Area_km2","area_diff","size_diff", "lat", "lon",
                    'str_ord','str_slope', "dist_coast_km", "ds_dist_outlet")

sites.df <- sites.df %>% 
  mutate( region = sub("_$","",gsub('[[:digit:]"]+', '', cat_ID_con)),
  size_diff =  case_when(area_diff >0~'Larger', area_diff <0~'Smaller',
                        TRUE~'No Change')) %>% 
  subset(select = site_keep_cols )

#don't keep ds_dist_outlet bc it is incorrect for some sites/networks
# sites.df %>% 
#   ggplot(aes(x = dist_coast_km, y = ds_dist_outlet)) +
#   geom_point()
  
#dput(colnames(cov.df.raw))
wtd_keep_cols <- c("cat_ID_con", "cat_ID", "region", "cat_slope_MIN", "cat_slope_MAX",
  "cat_slope_MEAN", "cat_slope_STD", "cat_slope_SUM", "cat_elev_MIN", 
  "cat_elev_MAX", "cat_elev_MEAN", "cat_elev_STD", "wtd_elev_MIN",
  "wtd_elev_MAX", "wtd_elev_MEAN", "wtd_elev_STD", "wtd_slope_MIN",
  "wtd_slope_MAX", "wtd_slope_MEAN", "wtd_slope_STD", "wtd_north_per", 
  "wtd_wet_per","wtd_lake_per","wtd_glacier_per","wtd_area_sqKM")


cov.df <- cov.df.raw %>%
  mutate(wtd_area_sqKM = wtd_elev_AREA *1e-6) %>% 
  subset(select = wtd_keep_cols )

# Convert na to 0
cov.df[is.na(cov.df)] <- 0

cov.all <- left_join(sites.df %>% select(SiteID, cat_ID_con, Region = region, str_ord, str_slope, dist_coast_km),
                     cov.df)

cov.all <- left_join(cov.all, lcld.long)
# summary(cov.all)


```

Early exploration turned up lots of small problems that Dustin has fixed -- all recorded on [google sheet](https://docs.google.com/spreadsheets/d/1TJrNIwr14HJ4QV6-fB4ubmi0BoTNCG7UxoK-s--IsJM/edit#gid=624864644), QA review tab.

```{r covariate exploration}

names(cov.all)

cov.all %>% 
  group_by(Region) %>% 
  summarize(meanslope = mean(wtd_slope_MEAN))

cov.all %>% 
  group_by(Region) %>% 
  summarize(meanslope = mean(wtd_elev_MEAN))

#2011 all fixed now.
cov.all %>% 
  filter(wtd_lcld_jd <0)

cov.all %>% 
  mutate(yr11 = case_when(Year == 2011 ~ 1, 
                          TRUE ~ 0)) %>% 
  ggplot(aes(x = wtd_slope_MEAN, y = wtd_lcld_jd, color = as.factor(yr11))) +
  geom_point() +
  facet_wrap(~Region)


cov.all %>% 
  # filter(!Year == 2011) %>% 
  ggplot(aes(x = wtd_elev_MEAN, y = wtd_lcld_jd, color = Region)) +
  geom_point() +
  geom_smooth() 
  facet_wrap(~Year)
  

```

Comparing modis output with snowtel.

```{r snow exploration}

#these sort of match up to hillside snowtel (2006-2019) april 1st swe 
# lowest years 2015, 2014, 2019
# biggest years 2012 by a lot, then 2007-2010 all about the same
cov.all %>% 
  group_by(Region, Year) %>% 
  summarize(meanlcld = mean(wtd_lcld_jd)) %>% 
  pivot_wider(names_from = Year, values_from = meanlcld, names_sort = TRUE)

cov.all %>% 
  ggplot() +
  geom_freqpoly(aes(x = wtd_lcld_jd, after_stat(density), color = Region)) +
  facet_wrap(~Year)
```

Stream covariates. 

```{r watershed area}
cov.all %>% 
  ggplot(aes(str_ord, wtd_area_sqKM)) +
  geom_point() +
  scale_y_log10() +
  facet_wrap(~Region)

```

Review the glacier, wetland, and lake covers.

```{r landcover}
cov.all %>% 
  distinct(Region, SiteID, wtd_glacier_per) %>% 
  ggplot() +
  geom_freqpoly(aes(x = wtd_glacier_per, after_stat(density))) +
  facet_wrap(~ Region) 

cov.all %>% 
  distinct(Region, SiteID, cat_ID_con, wtd_glacier_per) %>% 
  filter(wtd_glacier_per > 5) %>% 
  arrange(desc(wtd_glacier_per))

cov.all %>% 
  distinct(Region, SiteID, cat_ID_con, wtd_lake_per) %>% 
  filter(wtd_lake_per > 5) %>% 
  arrange(desc(wtd_lake_per))
```

Generate a data frame that associates the SiteIDs in the response data frame with the covariate data frame. Tim's response data frame drops > 70 sites because they had limited summer data or data were prior to 2011 (he ran 2011-2020). We may go back and include older years now that we aren't running DFA so use the original model data frame sent to Tim (stream + air temps) and generate lookup for merging the SiteIDs. Then apply this to the response data frame that we are using for now.

We added prefixes to most of the SiteIDs when generating the data that didn't get carried over to the spatial data frames. 

* find the SiteIDs that don't match in the two data frames (109) -- make sure all are converted to lower case because that is causing some mismatches.
* remove the prefix -- for just the mismatched sites -- so that that spatial SiteIDs will join.
* 13 sites in the response data frame not in the covariate data frame -- see [google sheet](https://docs.google.com/spreadsheets/d/1TJrNIwr14HJ4QV6-fB4ubmi0BoTNCG7UxoK-s--IsJM/edit#gid=1942759492) for how to fix them. 


```{r create siteid_join data frame}

temp_dat <- readRDS("data_preparation/final_data/summer_data_wair2021-11-23.rds") %>% 
  mutate(site_lower = tolower(SiteID))

temp_md <- readRDS("data_preparation/final_data/md.rds")

cov.all <- cov.all %>% 
  mutate(site_lower = tolower(SiteID))

#sites in temperature data frame not in covariate data frame
remove_prefix <- anti_join(temp_dat %>% distinct(site_lower),
          cov.all %>% distinct(site_lower)) %>%
  arrange(site_lower) %>% 
  pull(site_lower)

siteid_join <- temp_dat %>% 
  mutate(cov_id = case_when(site_lower %in% remove_prefix ~  str_remove(site_lower, pattern = "^.*?(_|-)"),
                            TRUE ~ site_lower)) %>% 
  distinct(ts_id = site_lower, cov_id)

#13 mismatches
# anti_join(siteid_join, cov.all, by = c("cov_id" = "site_lower"))

# temp_dat %>% filter(grepl("mcgeary", site_lower)) # a little big of data in Aug/Sep 16 and June 17
# temp_dat %>% filter(grepl("lost dog", site_lower)) # a little big of data in Aug/Sep 16 and June 17
# temp_dat %>% filter(grepl("_naknek", site_lower)) # a little big of data in Aug/Sep 16 and June 17

siteid_join <- siteid_join %>% 
  mutate(cov_id = case_when(cov_id == "katm_lbrooo" ~ "katm_lbrooo_lvl",
                            cov_id == "katm_naknlo" ~ "katm_naknlo_lvl",
                            cov_id == "lacl_kijilo" ~ "lacl_kijilo_lvl",
                            cov_id == "lslil10" ~ "lslil",
                            cov_id == "lsnlt10" ~ "lsnlt1",
                            cov_id == "lsr93" ~ "lsr93lb",
                            cov_id == "lsr106" ~ "lsr112.5",
                            cov_id == "lacl_lclaro" ~ "lacl_lclaro_lvl",
                            TRUE ~ cov_id))

```

Lots of sites in covariates not in response data frame -- check on how Tim filtered data completeness, which should explain why sites were dropped. For each site and year, Tim dropped any with less than 80% of days in June 1 - Aug 31 (jd 152-243). AND, he only used sites with data from 2011-2020.


```{r sites dropped in arima analysis}

temp_dat <- readRDS("data_preparation/final_data/summer_data_wair2021-11-23.rds")

temp_dat %>% distinct(SiteID)
ts_dat %>% distinct(SiteID) #dropped 74 from data I provided

anti_join(temp_dat %>% distinct(SiteID), ts_dat %>% distinct(SiteID = Site))

#Now showing 73 sites.
temp_dat %>%
  mutate(Year = year(sampleDate), jd = as.numeric(format(sampleDate, "%j"))) %>% 
  filter(jd > 151, jd < 244) %>% 
  group_by(SiteID, Year) %>% 
  summarize(sum_percent = n()/92) %>%
  mutate(keep = case_when(Year < 2011 ~ 0,
                          sum_percent > 0.8 ~ 1,
                          TRUE ~ 0)) %>% 
  group_by(SiteID) %>% 
  summarize(sum_complete = sum(keep)) %>% 
  filter(sum_complete == 0)
```


```{r create model data frame}

# nerka pike creek is only missing site now
left_join(ts_dat %>% mutate(site_lower = tolower(Site)), siteid_join, by = c("site_lower" = "ts_id")) %>% 
  filter(!Year == 2020) %>% 
  left_join(cov.all %>% rename(cov_id = site_lower)) %>% 
  filter(is.na(str_ord))

mod_dat <- left_join(ts_dat %>% mutate(site_lower = tolower(Site)), siteid_join, by = c("site_lower" = "ts_id")) %>% 
  filter(!Year == 2020) %>% 
  left_join(cov.all %>% rename(cov_id = site_lower)) %>% 
  filter(!is.na(str_ord))

mod_dat %>% 
  filter(TempSens < 0)
```

# Covariate correlations

Check slopes first. A couple of outliers -- one for stream slope and two for minimum catchment slope. 

Keep:

* stream slope
* mean catchment slope and elevation
* mean watershed slope and elevation
* watershed percent north facing 
* distance to coast
* watershed area
* wetland, glacier, lake cover
* modis snow date

```{r pair plots for groups of covariates}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

mod_dat %>% 
  select(str_slope, cat_slope_MIN:cat_slope_STD, wtd_slope_MIN:wtd_slope_STD, wtd_north_per) %>% 
  pairs(upper.panel = panel.cor)

mod_dat %>% 
  select(cat_elev_MIN:cat_elev_STD, wtd_elev_MIN:wtd_elev_STD, wtd_area_sqKM, dist_coast_km) %>% 
  pairs(upper.panel = panel.cor)

mod_dat %>% 
  select(wtd_wet_per:wtd_lcld_jd) %>% 
  pairs(upper.panel = panel.cor)

mod_dat %>% 
  filter(cat_slope_MIN > 0.5)

mod_dat %>% 
  filter(str_slope > 0.07)

mod_dat %>% 
  select(str_slope) %>% 
  ggplot() +
  geom_histogram(aes(x = str_slope))
```

Correlations for all covariates in list above. All r < 0.7, which is good.

```{r pair plot for reduced list of 11 cov}
mod_dat %>% 
  select(str_slope, cat_elev_MEAN, cat_slope_MEAN, wtd_north_per,
         wtd_elev_MEAN, wtd_slope_MEAN, wtd_area_sqKM, dist_coast_km,
         wtd_wet_per, wtd_glacier_per, wtd_lake_per, wtd_lcld_jd) %>% 
  pairs(upper.panel = panel.cor)
```

Set up a basic model to get VIF. WOW - all are below 5, could remove mean watershed elevation or mean catchment elevation.

```{r vif for 12 cov}

#all 12
lm1 <- lm(TempSens ~ str_slope + cat_elev_MEAN + cat_slope_MEAN + wtd_north_per +
         wtd_elev_MEAN + wtd_slope_MEAN + wtd_area_sqKM + dist_coast_km + 
         wtd_wet_per + wtd_glacier_per + wtd_lake_per + wtd_lcld_jd, data = mod_dat)

vif(lm1)

#removed mean catchment elevation
lm2 <- lm(TempSens ~ str_slope + cat_slope_MEAN + wtd_north_per +
         wtd_elev_MEAN + wtd_slope_MEAN + wtd_area_sqKM + dist_coast_km + 
         wtd_wet_per + wtd_glacier_per + wtd_lake_per + wtd_lcld_jd, data = mod_dat)

vif(lm2)

#removed mean watershed slope
lm3 <- lm(TempSens ~ str_slope + cat_slope_MEAN + wtd_north_per +
         wtd_elev_MEAN + wtd_area_sqKM + dist_coast_km + 
         wtd_wet_per + wtd_glacier_per + wtd_lake_per + wtd_lcld_jd, data = mod_dat)

vif(lm3)


```

Simple mixed effects model for this dataset.

# stopped here - use liklihood ratio to test for keeping RE and try lme4 and plot results so I can share next week. Keep researching me and forest models.

```{r mixed effects model}

lme1 <- lme(TempSens ~ str_slope + cat_slope_MEAN + wtd_north_per +
         wtd_elev_MEAN + wtd_area_sqKM + dist_coast_km + 
         wtd_wet_per + wtd_glacier_per + wtd_lake_per + wtd_lcld_jd, data = mod_dat, random = ~1 | SiteID)

summary(lme1)

plot(resid(lme1))

plot(lme1)

plot(resid(lme1), mod_dat$cat_elev_MEAN)
plot(resid(lme1), mod_dat$wtd_slope_MEAN)


#try with scaled covariates
mod_dat_sc <- bind_cols(mod_dat %>% select(TempSens, SiteID, Year), as_tibble(scale(mod_dat %>% select(str_slope, cat_slope_MEAN, wtd_north_per,
         wtd_elev_MEAN, wtd_area_sqKM, dist_coast_km,
         wtd_wet_per, wtd_glacier_per, wtd_lake_per, wtd_lcld_jd))))

lme2 <- lme(TempSens ~ str_slope + cat_slope_MEAN + wtd_north_per +
         wtd_elev_MEAN + wtd_area_sqKM + dist_coast_km + 
         wtd_wet_per + wtd_glacier_per + wtd_lake_per + wtd_lcld_jd, data = mod_dat_sc, random = ~1 | SiteID)

summary(lme2)
```

