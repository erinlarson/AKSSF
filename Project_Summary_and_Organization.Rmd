---
title: "Project_Summary_and_Organization"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# AKSSF

## AKSSF project workflow

Project to describe and map thermal sensitivities for Southern Alaska - Cook Inlet, Prince William Sound, and Copper River, Kodiak, and Bristol Bay.

Note: supplemental funding from FWS to include Kodiak temperature data in this project. And, second AKSSF award to do a similar analysis for Bristol Bay. Kodiak data will be cleaned in this repo. Bristol Bay data are being prepared concurrently for another project so final data can be imported from [SWSHP-Bristol-Bay-Thermal-Diversity github repo](https://github.com/rsshaftel/SWSHP-Bristol-Bay-Thermal-Diversity).


1. Data Availability

All metadata and data ready:
NOTE: need to update temperature model datasets to remove redundant CIK sites.

* Deshka temperature model output - CIK and FWS data from 2017-2019
* Kenai temperature model output - Ben Meyers, CIK and KWF
* Anchor temperature model output - CIK, APU, and KBRR
* CIK data for Cook Inlet
* USFS data from Luca
* Larsen Bay Tribe, site on Karluk - Data received from and QC'd by Jeff Davis.
* Alutiiq Tribe, site on Big Creek - Data received from and QC'd by Jeff Davis.
* ADFG data from Heather. Note that she had also archived ~ 1 year of data on KNB, but sent everything over so no longer using that data. DM will format.
* Prince William Sounds Science Center data from Pete Rand, 4 sites. DM
* (in BB repo) NPS data from Krista collected by Dan Young. DM 
* NPS data from Trey. Note that source/data folder is what was on the KNB archive and has been formatted. But need to bring in new data and combine. Becky formatted for QA. nps.data.rds on google drive AKSSF/data/formatted_data.
* Native Village of Eyak - 2 sites. DM 
* UW tc 16 sites data, streams are formatted, but need QA. Becky will format
* ACCS temperature data from Little Susitna watershed - summer 2016 and 2019-2020. Dustin is doing data prep in a separate [Little_Susitna github repo](https://github.com/rsshaftel/Little_Susitna). Grabbed 2016 sites from akoats metadata and combined with 2019 sites from logger database, some might be the same, will need to check in GIS.

Data need QA after formatting:
* UW 5 sites and historic data - DM will format and qc

Data received, need to read in and format new data and create metadata (some may be done already):

* FWS data for Kodiak Refuge, OSM sites on Kodiak (2) and Copper River (2), and WRB sites on Kodiak (2) -- from Meg Perdue, received Oct 2020. Check to see if FWS data archived on KNB is needed to supplement what Meg sent over -- those data are all through 2017.
* USGS sites selected in GIS from akoats working. Get data from dataRetrieval library in R

Outstanding data requests:

* KRAA data for Little Waterfall, Pillar Creeks, and Telrod Creeks - contact is Trenten Dodson, but talked with Nate Weber, he will send over the data, 2015-2019.  

Low priority datasets (wait to see if we decide to only look at sensitivity for sites with a minimum number of years of data before we pursue additional datasets):

* Mat-Su thermal regimes project - daily data were saved as a final product. This would include CIK data already archived on KNB, USGS data that I can grab separately, and ARRI data from Mat-Su. I could just re-import the raw ARRI data, I believe they cleaned it for me.
* AEA data for Susitna-Watana dam. I looked at this data for the thermal regimes project and it wasn't clean. They had temperature arrays on the Susitna River with some pretty anomalous readings between loggers. We could decide to clean this dataset for this project, but it would probably be a week of someone's time. 
* Little Su tributaries as part of a Masters project. I have this data and only used the most downstream locations for the thermal regimes project, I could grab those along with the ARRI data and import.
* ADFG data from Copper River/PWS. Talked to Stormy Haught. He has temperature data for LB/RB of Copper River and also from Coghill River, but both are very limited each summer - generally only June and July. I told him I would call back if we decided we could use this data - 424-3212 (phone is better than email).


Bristol Bay:

All Bristol Bay data are being cleaned in a separate [github repo](https://github.com/rsshaftel/SWSHP-Bristol-Bay-Thermal-Diversity).
(Note that the only data we are missing in Bristol Bay are Dan Young's sites, otherwise everything should be good. Also need to QA UW data - talk to Dan and Jackie.)

2. Data cleaning

All data need to be formatted consistently so that we can combine them for the data analysis step. All datasets should be saved as separate data files and sites files for import to AKTEMP later. 

Steps:

2a. Read in data files and format consistently using variable names and types below. Screen for duplicates. Formatting scripts are in the data_preparation folder and save data files are in data_preparation/formatted_data. 

2b. In a second script, data are reviewed for exposures and burials for data providers that indicated their data review is minimal or incomplete. Note that CIK and FWS are probably the *only* data providers that don't need review. Enter dates for bad data in a google sheet on the ACCS Aquatics Program/Projects/Active Projects/AKTEMP/Data/Data_Temp/0x_xx/Souce folder. Save reviewed data with additional UseData flags in the data_preparation/formatted_data folder as well. This folder is in .gitignore because file sizes are so large.

Note: We can restrict our QA of data to just the months of June-September. So for the output of the QA scripts, we can just filter to these months.

2c. To obtain the data to be used for AKSSF, we need to filter on complete daily values to calculate daily min, mean, and max. And, may also filter to June - September to see how much available data we have across sites and years.

AKTEMP Data file

* SiteID, character
* sampleDate, date
* sampleTime, hms
* Temperature, numeric
* useData, numeric

Optional fields in the data file to assist with QA:

* Waterbody_name, character
* DT, posixct
* duplicate

Daily Data file - see helper functions for creating this from AKTEMP data file

* SiteID, character
* sampelDate, date
* meanDT, numeric
* maxDT, numeric
* minDT, numeric

Sites file. This is a longer list of names from akoats that we should probably keep. We may not know all if sites were not previously submitted to akoats. Not a big deal, just capture what we can for each data provider for now. Could email them later when actually submitting data to AKTEMP.

* Agency_ID, ID as provided from the data provider
* SiteID, unique name that links to the data file, maybe a concatenation of agency_ID + acronym.
* AKOATS_ID
* Waterbody_name
* Latitude
* Longitude
* SourceName
* Contact_person
* Contact_email
* Contact_telephone
* Sensor_Placement
* Sensor_accuracy
* Sensor_QAQC

3. Air temperature extraction

The sites file will be used in ArcGIS to extract catchments for each site. In R, we can calculate an average air temperature across the catchment or a site buffer from DAYMET data for the appropriate months and years of data. We aren't predicting with air temperatures so it only needs to match the empirical data. There is code in the temperature modeling repos (KFHP) to get this started.

4. Thermal sensitivity analysis

Per Daniel, Tim Cline has the correct code for running the DFA. He did a slightly different procedure than what Peter Lisi did in the original paper for Bristol Bay. See Cline's snow paper and DFA.

## Repository Organization

Folders in this repo:

* data_preparation folder: this include scripts for reading in and wrangling data. data_[provider name] are the scripts for reading and formatting metadata and data files, saved as .rds. data_QA_[provider name] are scripts for datasets that required QA. QAed data have a useData flag that indicates data to be removed prior to analysis.
* data_preparation/formatted_data folder: output from data_ and data_QA scripts.
* data_preparation/final_data folder: final dataset for AKSSF analysis. This should be daily min, max, and mean for all sites and days.
* docs folder: for github pages. 


