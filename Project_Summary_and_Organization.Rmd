---
title: "Project_Summary_and_Organization"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# AKSSF

## AKSSF project workflow

Project to describe and map thermal sensitivities for Southern Alaska - Cook Inlet, Prince William Sound, and Copper River, Kodiak, and Bristol Bay.

Note: supplemental funding from FWS to include Kodiak temperature data in this project. And, second AKSSF award to do a similar analysis for Bristol Bay. Kodiak data will be cleaned in this repo. Bristol Bay data are being prepared concurrently for another project so final data can be imported from [SWSHP-Bristol-Bay-Thermal-Diversity github repo](https://github.com/rsshaftel/SWSHP-Bristol-Bay-Thermal-Diversity).


1. Data Availability

All metadata and data ready:
NOTE: need to update temperature model datasets to remove redundant CIK sites.

* Deshka temperature model output - CIK and FWS data from 2017-2019
* Kenai temperature model output - Ben Meyers, CIK and KWF
* Anchor temperature model output - CIK, APU, and KBRR
* CIK data on KNB - this will include all 48 sites in network, but only through 2017
* NPS data on KNB from Trey Simmons
* ADFG data on KNB for Kodiak Island, three streams with one summer of data

Data need QA after formatting:

* USFS data from Luca.
* ADFG data from Heather.
* NPS data from Trey.
* (in BB repo) NPS data from Krista collected by Dan Young.

Data received, need to read in and format new data and create metadata (some may be done already):

* FWS data for Kodiak Refuge, OSM sites on Kodiak (2) and Copper River (2), and WRB sites on Kodiak (2) -- from Meg Perdue, received Oct 2020. Check to see if FWS data archived on KNB is needed to supplement what Meg sent over -- those data are all through 2017.
* USGS sites selected in GIS from akoats working. Get data from dataRetrieval library in R
* ACCS temperature data from Little Susitna watershed - summer 2016 and 2019-2020. Dustin is doing data prep in a separate [Little_Susitna github repo](https://github.com/rsshaftel/Little_Susitna). Grabbed 2016 sites from akoats metadata and combined with 2019 sites from logger database, some might be the same, will need to check in GIS.
* post-2017 data from Sue has been sent over. 
* USFS data for streams in Chugach National Forest from Luca Adelfio. Dustin and I chatted with him and his data probably need a little review. He sent updated data for Kenai and PWS sites. Also new sites from a grad student. Different levels of QA on all of this data so needs a thorough review.
* ADFG data - contact is Heather Finkle. Data through 2017 archived on KNB have already been included. She has sent over all of their data.
* NPS data - Trey Simmons. He submitted data for ~ 4 sites in our study area to KNB that were formatted. He recently sent over some additional years of data and also notes on burials and exposures.


Outstanding data requests:

* Native Village of Eyak data for Cordova. Talked with James Paley and they have data from two creeks in Cordova from level loggers so buried just subsurface. Not the best data so probably not worth waiting on.
* KRAA data for Little Waterfall, Pillar Creeks, and Telrod Creeks - contact is Trenten Dodson, but talked with Nate Weber, he will send over the data, 2015-2019.  
* Larsen Bay Tribe, site on Karluk - called Alexander Panamaroff, and also talked with Jamie Hubbard. Data will come from Jeff Davis.  
* Alutiiq Tribe, site on Big Creek - data request to Lepani Nadore, data will come from Jeff Davis and should be ready end February.
* Additional years of data from Trey Simmons, NPS. Combine with KNB archive.

Low priority datasets (wait to see if we decide to only look at sensitivity for sites with a minimum number of years of data before we pursue additional datasets):

* Mat-Su thermal regimes project - daily data were saved as a final product. This would include CIK data already archived on KNB, USGS data that I can grab separately, and ARRI data from Mat-Su. I could just re-import the raw ARRI data, I believe they cleaned it for me.
* AEA data for Susitna-Watana dam. I looked at this data for the thermal regimes project and it wasn't clean. They had temperature arrays on the Susitna River with some pretty anomalous readings between loggers. We could decide to clean this dataset for this project, but it would probably be a week of someone's time. 
* Little Su tributaries as part of a Masters project. I have this data and only used the most downstream locations for the thermal regimes project, I could grab those along with the ARRI data and import.
* ADFG data from Copper River/PWS. Talked to Stormy Haught. He has temperature data for LB/RB of Copper River and also from Coghill River, but both are very limited each summer - generally only June and July. I told him I would call back if we decided we could use this data - 424-3212 (phone is better than email).


Bristol Bay:

All Bristol Bay data are being cleaned in a separate [github repo](https://github.com/rsshaftel/SWSHP-Bristol-Bay-Thermal-Diversity).
(Note that the only data we are missing in Bristol Bay are Dan Young's sites, otherwise everything should be good. Also need to QA UW data - talk to Dan and Jackie.)


2. Data cleaning

All data need to be formatted consistently so that we can combine them for the data analysis step. All datasets should be saved as separate data files and sites files for import to AKTEMP later. 

Steps:

2a. Read in data files and format consistently using variable names and types below. Screen for duplicates. Formatting scripts are in the data_preparation folder and save data files are in data_preparation/formatted_data. 

2b. In a second script, data are reviewed for exposures and burials for data providers that indicated their data review is minimal or incomplete. Note that CIK and FWS are probably the *only* data providers that don't need review. Enter dates for bad data in a google sheet on the ACCS Aquatics Program/Projects/Active Projects/AKTEMP/Data/Data_Temp/0x_xx/Souce folder. Save reviewed data with additional UseData flags in the data_preparation/formatted_data folder as well. This folder is in .gitignore because file sizes are so large.

Note: We can restrict our QA of data to just the months of June-September. So for the output of the QA scripts, we can just filter to these months.

2c. To obtain the data to be used for AKSSF, we need to filter on complete daily values to calculate daily min, mean, and max. And, may also filter to June - September to see how much available data we have across sites and years.

Data file

* SiteID, character
* sampleDate, date
* sampleTime, hms
* Temperature, numeric
* useData, numeric

Optional fields in the data file to assist with QA:

* Waterbody_name
* DT, posixct
* duplicate 

Sites file

* Agency_ID, ID as provided from the data provider
* SiteID, unique name that links to the data file, maybe a concatenation of agency_ID + lat + long.
* AKOATS_ID
* Waterbody_name
* latitude
* longitude
* Source_Name
* Contact_Name

3. Air temperature extraction

The sites file will be used in ArcGIS to extract catchments for each site. In R, we can calculate an average air temperature across the catchment or a site buffer from DAYMET data for the appropriate months and years of data. We aren't predicting with air temperatures so it only needs to match the empirical data. There is code in the temperature modeling repos (KFHP) to get this started.

4. Thermal sensitivity analysis

Per Daniel, Tim Cline has the correct code for running the DFA. He did a slightly different procedure than what Peter Lisi did in the original paper for Bristol Bay. See Cline's snow paper and DFA.

## Repository Organization

Folders in this repo:

* data_preparation folder: this include scripts for reading in and wrangling data. data_[provider name] are the scripts for reading and formatting metadata and data files, saved as .rds. data_QA_[provider name] are scripts for datasets that required QA. QAed data have a useData flag that indicates data to be removed prior to analysis.
* data_preparation/formatted_data folder: output from data_ and data_QA scripts.
* data_preparation/final_data folder: final dataset for AKSSF analysis. This should be daily min, max, and mean for all sites and days.
* docs folder: for github pages. 


