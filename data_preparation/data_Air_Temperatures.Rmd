---
title: "data_Air_Temperatures"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(sf)
library(tidyverse)
library(zonalDaymet)
library(maptools)
library(sp)
library(rgdal)
library(raster)
library(ncdf4)
library(tictoc)

# if(!require(devtools)){install.packages("devtools")}
# devtools::install_github("bluegreen-labs/daymetr")
library("daymetr")
```


This script is for processing DAYMET air temperatures for the DFA analysis. Possible packages include daymetr, zonalDaymet or spatialEco.

Note: catchments are almost all < 1 km^2 in area so averaging daymet over catchments is not needed. Extracted daymet air temperatures by site using daymetr library below.


# Extract DAYMET

daymetr library on github is using Daymet V4, which has 2020 (There are 99 sites with complete 2020 summer data). Daymet download tool (thredds ncss) should be working now per email from Michele Thornton on 6/10/21. Note that daymet is in Lambert Conformal Conic system. From old scripts for KFHP repo, retransformed daymet before averaging across catchments.

```{r test ncss download 2019 only}
long1 <- range(st_coordinates(sites)[,'X'])
lat1 <- range(st_coordinates(sites)[,'Y'])

#-165 to -140, 56.5 to 64
ggplot() +
  geom_sf(data = sites)

#top left and bottom right, lat, long
download_area_new <- c(max(lat1) + 1, min(long1) - 2, min(lat1) - 1, max(long1) + 1)

tic(msg = "download 2019 daily min and max")
download_daymet_ncss(location = download_area_new,
                     start = 2019,
                     end = 2019,
                     param = c("tmin", "tmax"),
                     frequency = "daily",
                     path = "W:\\GIS\\Daymet\\raw",
                     silent = TRUE)
toc(log = TRUE)

tic.log()

```

Script below for downloading all years not run because moved forward with downloading by sites (next section).

```{r download all years, eval = FALSE}
tic(msg = "download 41 years daily min and max")
download_daymet_ncss(location = download_area_new,
                     start = 1980,
                     end = 2020,
                     param = c("tmin", "tmax"),
                     frequency = "daily",
                     path = "W:\\GIS\\Daymet\\raw",
                     silent = TRUE)
toc(log = TRUE)
```

Check that daymet covers all sites. For some reason, with just a one degree extension of latitude and longitude, there is a site in BB missing on western edge. Retry with -2 degrees longitude.

```{r daymet and sites plot}
tmax19 <- raster::stack("W:/GIS/Daymet/raw/tmax_daily_2019_ncss.nc") 

raster::projection(tmax19) <- "+proj=lcc +lat_1=25 +lat_2=60 +lat_0=42.5 +lon_0=-100 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=km +no_defs"

# tmin19_aa <- projectRaster(tmin19, crs = "+init=epsg:3338") #alaska albers
# tmin19_aa <- projectRaster(tmin19, crs = 4326)

#try original projection to see why some sites are not covered.
sites_lcc <- st_transform(sites, crs = st_crs(tmax19))

st_crs(sites_lcc) == st_crs(tmax19)

plot(tmax19[[1]])
plot(sites_lcc[1], add = TRUE, color = "black")

```

Tried submitting directly:
https://thredds.daac.ornl.gov/thredds/ncss/ornldaac/1840/daymet_v4_daily_na_tmax_2019.nc?var=lat&var=lon&var=tmax&north=64.145&west=-163.7336&east=-141.9427&south=56.0596&disableProjSubset=on&horizStride=1&time_start=2019-01-01T12%3A00%3A00Z&time_end=2019-12-31T12%3A00%3A00Z&timeStride=1&accept=netcdf

```{r checking ncss download area}
#read in ncss download and set projection.
test <- raster::stack("W:/GIS/Daymet/raw/1840_daymet_v4_daily_na_tmax_2019.nc") 
raster::projection(test) <- "+proj=lcc +lat_1=25 +lat_2=60 +lat_0=42.5 +lon_0=-100 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=km +no_defs"

#create bounding box using exact same coordinates in request URL:
df <- data.frame(north_lat = 64.145, south_lat = 56.0596, west_lng = -163.7336, east_lng = -141.9427)
poly_df <- matrix(c(df[1, 'west_lng'], df[1, 'north_lat'], 
         df[1, 'east_lng'], df[1, 'north_lat'], 
         df[1, 'east_lng'], df[1, 'south_lat'], 
          df[1, 'west_lng'], df[1, 'south_lat'],
         df[1, 'west_lng'], df[1, 'north_lat'])  ## need to close the polygon
       , ncol =2, byrow = T) 
poly <- st_polygon(list(poly_df))
poly <- st_sfc(poly)
st_crs(poly) <- 4326

#convert to same lcc projection as ncss download.
poly_llc <- st_transform(poly, crs = st_crs(test))

#plot the two together.
plot(test[[1]])
plot(poly_llc, add = TRUE, color = "red")
```


# Get air temperatures by sites

Get locations (lat/long) for sites from metadata file and save as csv for use with daymetr download function.

Note: this was run off of the metadata lat/long and not the final shp where some sites were shifted, although nothing drastic, mostly just a few km off, which shouldn't make much of a difference. Shifting sites was to get them correctly on the stream networks for hydrologic attributes and not necessarily to correct locations (although in some cases they were off).


```{r read in site locations and save csv}
md <- readRDS("final_data/md_2022-02-08.rds")

write.csv(md %>% distinct(Site, Latitude, Longitude), file = paste0("sites", Sys.Date(), ".csv"),
          row.names = FALSE)
```


Download daymet data for 40 years and all sites.

```{r daymet for all sites}
dm_batch <- download_daymet_batch(file_location = "sites2022-04-22.csv",
                      start = 2011,
                      end = 2019,
                      simplify = TRUE,
                      silent = TRUE)


write_csv(dm_batch, "site_daymet_2011-19.csv")

dm_batch %>% 
  distinct(site, measurement)
dm_batch %>% 
  distinct(site)
dm_batch %>% 
  distinct(measurement)



```

# try feddata library

This could also work, but NOT RUN. (tried this back when daymet ncss had error.)


```{r test feddata functions, eval = FALSE}

akssf_sa <- st_read(dsn = "W:/GIS/AKSSF Southcentral/AKSSF_Hydrography.gdb", layer = "AKSSF_studyarea_HUC8")
akssf_bdy <- st_union(akssf_sa)

mat <- akssf_sa %>% filter(Name == "Matanuska")

ggplot() +
  geom_sf(data = akssf_bdy)

names(akssf_sa)

akssf_bdy_sp <- as_Spatial(akssf_bdy)
mat_sp <- as_Spatial(mat)

ggplot() +
  # geom_sf(data = akssf_bdy) 
  geom_sf(data = mat)

daymet_test <- get_daymet(template = mat_sp, label = "akssf", elements = c("tmin"), years = c(2019), region = "na",
                          tempo = "day", force.redo = TRUE, extraction.dir = "daymet/")
daymet_test <- get_daymet(akssf_bdy_sp, label = "akssf", elements = c("tmin"), years = 2019)


 template_bbox <- mat_sp %>% sf::st_bbox() %>% sf::st_as_sfc() %>% 
        sf::st_transform(4326) %>% sf::st_bbox()
 
 template_bbox
 
 
matPolygon <- polygon_from_extent(raster::extent(-149, -146, 61, 63),
  proj4string = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
)
 
plot(matPolygon)

test20 <- get_daymet(
  template = matPolygon,
  label = "mat",
  elements = c("prcp"),
  years = 2019
)
```

