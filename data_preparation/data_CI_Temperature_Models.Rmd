---
title: "data_CI_Temperature_Models"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(readxl)
library(lubridate)
library(tidyverse)
library(MARSS)
```


Stream temperature models for the Deshka, Anchor, and Kenai watersheds were completed for USFWS in Fall 2020. The reviewed temperature time series can be used for this analysis as the data are ready. The climate data are for 3-day lags, which we decided we won't use for AKSSF so don't worry about linking to climate data. We do need metadata and data data frames.

# Read in data from Deshka, Anchor, and Kenai - v2 raw data only

This version of import - V2, only includes raw temperature data for each project. Get the daily USGS data and daily data from the thermal regimes project separately. This matches the metadata spreadsheet as well.

## Deshka 

For each watershed, temperature data, site data, and climate data are all needed. Read in temperature data first and convert to daily means, link catchment ids to temperature data using sites table, and join climate data to final temperature dataset. 

```{r deshka temp data}

deshka_wd <- "W:/Github/Deshka_temperature/output/data_catalog"

deshka_temp <- read_csv(paste0(deshka_wd, "/deshka_temperature_data.csv", collapse = ""))

deshka_daily <- deshka_temp %>%
  filter(useData == 1) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanTemp = mean(Temperature))

deshka_daily %>% distinct(SiteID)
```

Edits to site table - kroto 5 downstream misspelled, and PMC1 listed twice. In temperature dataset, there is a PMC1 and PMC1-2, which are in the same catchment and just slightly separated. Keep PMC1, which has the longer time series.

```{r deshka catchment ID}

deshka_sites <- read_csv(paste0(deshka_wd, "/deshka_sites.csv", collapse = ""))

deshka_sites %>% 
  arrange(SiteID)

deshka_sites <- deshka_sites %>%
  mutate(SiteID = case_when(SiteID == "Kroto 5 Downstrean" ~ "Kroto 5 Downstream",
                            TRUE ~ SiteID)) %>% 
  filter(!(SiteID == "PMC1" & useSite == 1))

deshka_daily %>% 
  filter(SiteID %in% c("PMC1", "PMC1-2")) %>% 
  summarize(mindate = min(sampleDate),
            maxdate = max(sampleDate))

deshka_daily <- left_join(deshka_daily, deshka_sites %>% select(SiteID, catchmentID))

deshka_daily %>% distinct(SiteID, catchmentID)

#remove pmc1-2
deshka_daily <- deshka_daily %>% 
  filter(!(is.na(catchmentID)))

deshka_daily

```


```{r deshka climate data}

deshka_climate <- read_csv(paste0(deshka_wd, "/deshka_climate_variables.csv", collapse = ""))

deshka_climate

deshka_daily <- left_join(deshka_daily, deshka_climate %>% select(catchmentID, sampleDate, tair3))

deshka_daily

```


## Anchor

```{r anchor temp data}

anchor_wd <- "W:/Github/Temperature_Data/output/data_catalog"

anchor_temp <- read_csv(paste0(anchor_wd, "/anchor_temperature_data.csv", collapse = ""))

anchor_daily <- anchor_temp %>%
  filter(useData == 1) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanTemp = mean(Temperature))

anchor_daily
```



```{r anchor catchment ID}

anchor_sites <- read_csv(paste0(anchor_wd, "/anchor_sites.csv", collapse = ""))

anchor_sites %>% 
  arrange(SiteID)

anchor_daily %>% 
  distinct(SiteID)

anchor_daily <- left_join(anchor_daily, anchor_sites %>% select(SiteID, catchmentID))

anchor_daily %>% distinct(SiteID, catchmentID)

anchor_daily

```


```{r anchor climate data}

anchor_climate <- read_csv(paste0(anchor_wd, "/anchor_climate_variables.csv", collapse = ""))

anchor_climate

anchor_daily <- left_join(anchor_daily, anchor_climate %>% select(catchmentID, sampleDate, tair3))

anchor_daily

```


## Kenai

```{r kenai temp data}

kenai_wd <- "W:/Github/Kenai_temperature/output/data_catalog"

kenai_temp <- read_csv(paste0(kenai_wd, "/kenai_temperature_data.csv", collapse = ""))

kenai_daily <- kenai_temp %>%
  filter(useData == 1) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanTemp = mean(Temperature))

kenai_daily %>% summary
```


```{r kenai catchment ID}

kenai_sites <- read_csv(paste0(kenai_wd, "/kenai_sites.csv", collapse = ""))

kenai_sites %>% 
  arrange(SiteID)

kenai_daily %>% 
  distinct(SiteID)

kenai_daily <- left_join(kenai_daily, kenai_sites %>% select(SiteID, catchmentID))

kenai_daily %>% distinct(SiteID, catchmentID)

kenai_daily %>% summary

```


```{r kenai climate data}

kenai_climate <- read_csv(paste0(kenai_wd, "/kenai_climate_variables.csv", collapse = ""))

kenai_climate <- kenai_climate %>% 
  mutate(sampleDate = as.Date(sampleDate))

str(kenai_daily)
str(kenai_climate)

kenai_daily <- left_join(kenai_daily, kenai_climate %>% select(catchmentID, sampleDate, tair3))

kenai_daily %>% summary

#lots of missing winter airtemps
kenai_daily %>%
  ungroup() %>% 
  filter(is.na(tair3)) %>% 
  count(month(sampleDate))

#missing 2019 summer airtemps that were downloaded for usgs sites
kenai_daily %>%
  ungroup() %>% 
  filter(is.na(tair3), month(sampleDate) == 6)

```


## Combine

Combine the three datasets and see if there are years with overlapping data. No overlap between Anchor or Kenai and Deshka because that project started in 2017.

```{r combine daily data}
ci_daily <- bind_rows(deshka_daily %>% mutate(wtd = "Deshka"), anchor_daily %>% mutate(wtd = "Anchor"), kenai_daily %>% mutate(wtd = "Kenai")) %>%
  ungroup() %>% 
  mutate(year = year(sampleDate))

```



```{r remove extra data}
rm(kenai_temp, kenai_sites, kenai_climate)
rm(anchor_temp, anchor_sites, anchor_climate)
rm(deshka_temp, deshka_sites, deshka_climate)

```


# Read in data from Deshka, Anchor, and Kenai - v1 with daily data included - EDITED

## Deshka 

For each watershed, temperature data, site data, and climate data are all needed. Read in temperature data first and convert to daily means, link catchment ids to temperature data using sites table, and join climate data to final temperature dataset. 

For Deshka and Kenai, there is a separate temperature data csv with daily data.

```{r deshka temp data}

deshka_wd <- "W:/Github/Deshka_temperature/output/data_catalog"

deshka_temp <- read_csv(paste0(deshka_wd, "/deshka_temperature_data.csv", collapse = ""))
deshka_temp2 <- read_csv(paste0(deshka_wd, "/deshka_temperature_data2.csv", collapse = ""))

deshka_temp2 <- deshka_temp2 %>% 
  filter(useData == 1) %>% 
  rename(meanTemp = Temperature) %>% 
  mutate(SiteID = tolower(SiteID))

deshka_daily <- deshka_temp %>%
  filter(useData == 1) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanTemp = mean(Temperature))

deshka_daily <- bind_rows(deshka_daily, deshka_temp2)

deshka_daily %>% distinct(SiteID)
```

Three edits to site table - kroto 5 downstream misspelled, chijik trib should be chijik to match temperature SiteID and PMC1 listed twice. In temperature dataset, there is a PMC1 and PMC1-2, which are in the same catchment and just slightly separated. Keep PMC1, which has the longer time series.

```{r deshka catchment ID}

deshka_sites <- read_csv(paste0(deshka_wd, "/deshka_sites.csv", collapse = ""))

deshka_sites %>% 
  arrange(SiteID)

deshka_sites <- deshka_sites %>%
  mutate(SiteID = case_when(SiteID == "Kroto 5 Downstrean" ~ "Kroto 5 Downstream",
                            SiteID == "chijik trib.arri" ~ "chijik.arri",
                            TRUE ~ SiteID)) %>% 
  filter(!(SiteID == "PMC1" & useSite == 1))

deshka_daily %>% 
  filter(SiteID %in% c("PMC1", "PMC1-2")) %>% 
  summarize(mindate = min(sampleDate),
            maxdate = max(sampleDate))

deshka_daily <- left_join(deshka_daily, deshka_sites %>% select(SiteID, catchmentID))

deshka_daily %>% distinct(SiteID, catchmentID)

#remove pmc1-2
deshka_daily <- deshka_daily %>% 
  filter(!(is.na(catchmentID)))

deshka_daily

```


```{r deshka climate data}

deshka_climate <- read_csv(paste0(deshka_wd, "/deshka_climate_variables.csv", collapse = ""))

deshka_climate

deshka_daily <- left_join(deshka_daily, deshka_climate %>% select(catchmentID, sampleDate, tair3))

deshka_daily

```


## Anchor

```{r anchor temp data}

anchor_wd <- "W:/Github/Temperature_Data/output/data_catalog"

anchor_temp <- read_csv(paste0(anchor_wd, "/anchor_temperature_data.csv", collapse = ""))

anchor_daily <- anchor_temp %>%
  filter(useData == 1) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanTemp = mean(Temperature))

anchor_daily
```



```{r anchor catchment ID}

anchor_sites <- read_csv(paste0(anchor_wd, "/anchor_sites.csv", collapse = ""))

anchor_sites %>% 
  arrange(SiteID)

anchor_daily %>% 
  distinct(SiteID)

anchor_daily <- left_join(anchor_daily, anchor_sites %>% select(SiteID, catchmentID))

anchor_daily %>% distinct(SiteID, catchmentID)

anchor_daily

```


```{r anchor climate data}

anchor_climate <- read_csv(paste0(anchor_wd, "/anchor_climate_variables.csv", collapse = ""))

anchor_climate

anchor_daily <- left_join(anchor_daily, anchor_climate %>% select(catchmentID, sampleDate, tair3))

anchor_daily

```


## Kenai

```{r kenai temp data}

kenai_wd <- "W:/Github/Kenai_temperature/output/data_catalog"

kenai_temp <- read_csv(paste0(kenai_wd, "/kenai_temperature_data.csv", collapse = ""))
kenai_temp2 <- read_csv(paste0(kenai_wd, "/kenai_temperature_data2.csv", collapse = ""))

kenai_temp2 <- kenai_temp2 %>% 
  rename(meanTemp = Temperature) 

kenai_daily <- kenai_temp %>%
  filter(useData == 1) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanTemp = mean(Temperature))

kenai_daily <- bind_rows(kenai_daily, kenai_temp2)

kenai_daily %>% summary
```


```{r kenai catchment ID}

kenai_sites <- read_csv(paste0(kenai_wd, "/kenai_sites.csv", collapse = ""))

kenai_sites %>% 
  arrange(SiteID)

kenai_daily %>% 
  distinct(SiteID)

kenai_daily <- left_join(kenai_daily, kenai_sites %>% select(SiteID, catchmentID))

kenai_daily %>% distinct(SiteID, catchmentID)

kenai_daily %>% summary

```


```{r kenai climate data}

kenai_climate <- read_csv(paste0(kenai_wd, "/kenai_climate_variables.csv", collapse = ""))

kenai_climate <- kenai_climate %>% 
  mutate(sampleDate = as.Date(sampleDate))

str(kenai_daily)
str(kenai_climate)

kenai_daily <- left_join(kenai_daily, kenai_climate %>% select(catchmentID, sampleDate, tair3))

kenai_daily %>% summary

#lots of missing winter airtemps
kenai_daily %>%
  ungroup() %>% 
  filter(is.na(tair3)) %>% 
  count(month(sampleDate))

#missing 2019 summer airtemps that were downloaded for usgs sites
kenai_daily %>%
  ungroup() %>% 
  filter(is.na(tair3), month(sampleDate) == 6)

```


## Combine

Combine the three datasets and see if there are years with overlapping data. No overlap between Anchor or Kenai and Deshka because that project started in 2017.

```{r combine daily data}
ci_daily <- bind_rows(deshka_daily %>% mutate(wtd = "Deshka"), anchor_daily %>% mutate(wtd = "Anchor"), kenai_daily %>% mutate(wtd = "Kenai")) %>%
  ungroup() %>% 
  mutate(year = year(sampleDate))

```



```{r remove extra data}
rm(kenai_temp, kenai_sites, kenai_climate)
rm(anchor_temp, anchor_sites, anchor_climate)
rm(deshka_temp, deshka_sites, deshka_climate)

```
# Run DFA

Decide on best year(s) to try and run.

```{r identify complete summers and best years}

ci_summer_sites <- ci_daily %>% 
  filter(month(sampleDate) %in% 6:8) %>% 
  group_by(wtd, SiteID, year) %>% 
  summarize(n = n()) %>% 
  filter(n > 82) %>% 
  ungroup()

ci_summer_sites %>% 
  distinct(wtd, SiteID, year) %>% 
  count(year, wtd) %>% 
  pivot_wider(names_from = wtd, values_from = n, values_fill = 0) %>% 
  mutate(max_sites = Kenai + Deshka + Anchor)


```

```{r prepare data}

ci_daily_2010 <- left_join(ci_summer_sites %>% filter(year == 2010) %>% select(SiteID, year), 
                           ci_daily %>% filter(month(sampleDate) %in% 6:8)) %>% 
  arrange(SiteID, year)

ci_daily_2010_zsc <- bind_cols(ci_daily_2010 %>% 
  group_by(SiteID, year) %>% 
  summarize(meanTemp_zsc = scale(meanTemp),
            tair3_zsc = scale(tair3)),
  ci_daily_2010 %>% select(sampleDate, meanTemp, tair3))

ci_daily_2010_zsc 

ci_daily_2010 %>% 
  filter(SiteID == "CIK14") %>% 
  summarize(mean = mean(meanTemp),
            sd = sd(meanTemp))

ci_daily_2010 %>% 
  filter(SiteID == "CIK14") %>% 
  mutate(zsc = (meanTemp - 12.18014)/1.902544)



```


```{r zscores and remove na cols}

ci_daily_2010_zsc %>% 
  ggplot() +
  geom_line(aes(x = sampleDate, y = meanTemp, color = "blue")) +
  geom_line(aes(x = sampleDate, y = meanTemp_zsc, color = "red")) +
  facet_wrap(~SiteID)

ci_daily_2010_zsc %>% 
  ggplot() +
  geom_line(aes(x = sampleDate, y = meanTemp_zsc, group = SiteID, color = "grey")) + 
  geom_line(aes(x = sampleDate, y = tair3_zsc, group = SiteID, color = "red")) 

ci_daily_2010_zsc

#convert to n x T: n time series over T time steps

ci_2010_stream <- ci_daily_2010_zsc %>%
  ungroup() %>% 
  select(SiteID, sampleDate, meanTemp_zsc) %>% 
  pivot_wider(names_from = sampleDate, values_from = meanTemp_zsc)

ci_2010_air <- ci_daily_2010_zsc %>%
  ungroup() %>% 
  select(SiteID, sampleDate, tair3_zsc) %>% 
  pivot_wider(names_from = sampleDate, values_from = tair3_zsc)

#remove dates with any missing values for stream temp
nacols <- colnames(ci_2010_stream)[colSums(is.na(ci_2010_stream)) > 0]
nacols

ci_2010_stream <- ci_2010_stream %>% select(-c(nacols, SiteID))
ci_2010_air <- ci_2010_air %>% select(-c(nacols, SiteID))

```

```{r dfa}

#example code line from https://github.com/tjcline/dfaTMB/blob/master/SimulationTesting/TestMARSSvsTMB.R
# mod13_marss<-MARSS(noNA_Stream[streamSet,],model=list(m=1,R='unconstrained'),covariates=matrix(noNA_Air,nrow=1),form='dfa')

x = 200 #set for minit
c = 5000 #set for maxit = (x + c) - to ensure convergence

#model set for 2014 - 16 time series and 83 days in JJA
# 1 - no air temp, R diagonal and equal
# 2 - with air temp, R diagonal and equal


control.ops <- list(minit = x, maxit = c) 

ci_mod1 <- MARSS(as.matrix(ci_2010_stream), model = list(m = 1, R = 'diagonal and equal'),
                  form = "dfa", control = control.ops)
ci_mod2 <- MARSS(as.matrix(ci_2010_stream), model = list(m = 1, R = 'diagonal and equal'), covariates = as.matrix(ci_2010_air),
                  form = "dfa", control = control.ops)
ci_mod3 <- MARSS(as.matrix(ci_2010_stream), model = list(m = 2, R = 'diagonal and equal'),
                  form = "dfa", control = control.ops)
ci_mod4 <- MARSS(as.matrix(ci_2010_stream), model = list(m = 2, R = 'diagonal and equal'), covariates = as.matrix(ci_2010_air),
                  form = "dfa", control = control.ops)

AIC(ci_mod1, ci_mod2, ci_mod3, ci_mod4)

coef(ci_mod4)

fit <- get_DFA_fits(ci_mod2)

```

```{r get_DFA_fits function}

MLEobj <- ci_mod2

get_DFA_fits <- function(MLEobj, dd = NULL, alpha = 0.05) {
    ## empty list for results
    fits <- list()
    ## extra stuff for var() calcs
    Ey <- MARSS:::MARSShatyt(MLEobj)
    ## model params
    ZZ <- coef(MLEobj, type = "matrix")$Z
    ## number of obs ts
    nn <- dim(Ey$ytT)[1]
    ## number of time steps
    TT <- dim(Ey$ytT)[2]
    ## get the inverse of the rotation matrix
    H_inv <- varimax(ZZ)$rotmat
    ## check for covars
    if (!is.null(dd)) {
        DD <- coef(MLEobj, type = "matrix")$D
        ## model expectation
        fits$ex <- ZZ %*% H_inv %*% MLEobj$states + DD %*% dd
    } else {
        ## model expectation
        fits$ex <- ZZ %*% H_inv %*% MLEobj$states
    }
    ## Var in model fits
    VtT <- MARSSkfss(MLEobj)$VtT
    VV <- NULL
    for (tt in 1:TT) {
        RZVZ <- coef(MLEobj, type = "matrix")$R - ZZ %*% VtT[, 
            , tt] %*% t(ZZ)
        SS <- Ey$yxtT[, , tt] - Ey$ytT[, tt, drop = FALSE] %*% 
            t(MLEobj$states[, tt, drop = FALSE])
        VV <- cbind(VV, diag(RZVZ + SS %*% t(ZZ) + ZZ %*% t(SS)))
    }
    SE <- sqrt(VV)
    ## upper & lower (1-alpha)% CI
    fits$up <- qnorm(1 - alpha/2) * SE + fits$ex
    fits$lo <- qnorm(alpha/2) * SE + fits$ex
    return(fits)
}

```

