---
title: "data_CI_Temperature_Models"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
knitr::opts_knit$set(root.dir = normalizePath(".."))
source('W:/Github/AKSSF/helper_functions.R')

library(googledrive)
library(readxl)
library(lubridate)
library(tidyverse)
library(MARSS)
```

3/2/21 finish metadata corrections, trying to get seq-id from akoats where it applies for kenai dataset and getting correct contact info on anchor dataset. resave all three datasets bc I may have accidentally saved over deshka. When done with these three, check on formatting of uw streams data so that priscilla can qa.


Stream temperature models for the Deshka, Anchor, and Kenai watersheds were completed for USFWS in Fall 2020. The reviewed temperature time series can be used for this analysis as the data are ready. The climate data are for 3-day lags, which we decided we won't use for AKSSF so don't worry about linking to climate data. We do need metadata and data data frames.

Note on sites with metadata in AKOATS. All Deshka sites are new. The Kenai sites and CIK14 in the Anchor are included in Sue's other dataset with additional years of data so they are removed from this dataset. The Anchor River watershed sites CIK1-CIK8 are distinct and don't have metadata in AKOATS. 

# Read in data from Deshka, Anchor, and Kenai - v2 raw data only

This version of import - V2, only includes raw temperature data for each project. Get the daily USGS data and daily data from the thermal regimes project separately. This matches the metadata spreadsheet as well.

## Deshka 

For each watershed, temperature data, site data, and climate data are available. Just reading in site metadata and raw temperature data and using helper functions to save for AKSSF. 

```{r deshka temp data}

deshka_wd <- "W:/Github/Deshka_temperature/output/data_catalog"

deshka_temp <- read_csv(paste0(deshka_wd, "/deshka_temperature_data.csv", collapse = ""))

deshka_temp %>% distinct(SiteID)

deshka_daily <- temp_msmt_freq(deshka_temp %>% mutate(UseData = 1)) %>% daily_screen(.) %>% ungroup()

deshka_daily %>% distinct(SiteID)

```

Edits to site table - kroto 5 downstream misspelled, and PMC1 listed twice. In temperature dataset, there is a PMC1 and PMC1-2, which are in the same catchment and just slightly separated. Keep PMC1, which has the longer time series.

```{r deshka metadata}
deshka_sites <- read_csv(paste0(deshka_wd, "/deshka_sites.csv", collapse = ""))

deshka_temp %>% 
  distinct(SiteID) %>% 
  arrange(SiteID) %>% 
  full_join(deshka_sites)

deshka_sites <- deshka_sites %>%
  mutate(SiteID = case_when(SiteID == "Kroto 5 Downstrean" ~ "Kroto 5 Downstream",
                            TRUE ~ SiteID)) %>% 
  filter(!(SiteID == "PMC1" & useSite == 1))

deshka_daily %>% 
  ungroup() %>% 
  filter(SiteID %in% c("PMC1", "PMC1-2")) %>% 
  group_by(SiteID) %>% 
  summarize(mindate = min(sampleDate),
            maxdate = max(sampleDate))

#remove pmc1-2
deshka_daily <- deshka_daily %>% 
  filter(!(SiteID == "PMC1-2"))

deshka_temp <- deshka_temp %>% 
  filter(!(SiteID == "PMC1-2"))

deshka_md <- deshka_daily %>% 
  ungroup() %>% 
  distinct(SiteID) %>% 
  left_join(deshka_sites) %>% 
  mutate(seq_id = NA,
         SourceName = case_when(grepl("Moose|Kroto|Deshka", SiteID) ~ "CIK",
                                TRUE ~ "fwsAnchorage"),
         Contact_person = case_when(SourceName == "CIK" ~ "Sue Mauger",
                                    TRUE ~ "Daniel Rinella"),
         Contact_email = case_when(SourceName == "CIK" ~ "sue@inletkeeper.org",
                                    TRUE ~ "daniel_rinella@fws.gov"),
         Contact_telephone = case_when(SourceName == "CIK" ~ "907-235-4068",
                                    TRUE ~ "970-891-3783"),
         Sensor_Placement = "Main channel",
         Sensor_accuracy = "level 1  ( +/- 0.2 deg C)",
         Sensor_QAQC = 4,
         Waterbody_type = "S",
         Waterbody_name = case_when(grepl("Moose", SiteID) & grepl("Downstream", SiteID) ~ "Moose",
                                    grepl("Moose", SiteID) & grepl("Upstream", SiteID) ~ "Moose",
                                    grepl("Kroto", SiteID) & grepl("Downstream", SiteID) ~ "Kroto",
                                    grepl("Kroto", SiteID) & grepl("Upstream", SiteID) ~ "Kroto",
                                    grepl("Deshka", SiteID) & grepl("Downstream", SiteID) ~ "Deshka",
                                    grepl("Deshka", SiteID) & grepl("Upstream", SiteID) ~ "Deshka",
                                    TRUE ~ NA_character_)) %>% 
  rename(Latitude = latitude,
         Longitude = longitude)

deshka_md %>% select(SiteID, Waterbody_name)
```



NOT using climate data.

```{r deshka climate data, eval = FALSE}

deshka_climate <- read_csv(paste0(deshka_wd, "/deshka_climate_variables.csv", collapse = ""))

deshka_climate

deshka_daily <- left_join(deshka_daily, deshka_climate %>% select(catchmentID, sampleDate, tair3))

deshka_daily

```

Save data files

```{r deshka save}
save_metadata_files(deshka_md, "deshka")

save_aktemp_files(deshka_temp, "deshka")

save_daily_files(deshka_daily, "deshka")

```


## Anchor

```{r anchor temp data}

anchor_wd <- "W:/Github/Temperature_Data/output/data_catalog"

anchor_temp <- read_csv(paste0(anchor_wd, "/anchor_temperature_data.csv", collapse = ""))

anchor_daily <- temp_msmt_freq(anchor_temp %>% mutate(UseData = 1)) %>% daily_screen(.) %>% ungroup()

anchor_daily
```


None of these data were ever entered into AKOATS, except for CIK_14, which is removed before saving because it is in Sue's other dataset.
Need to enter metadata fields for CIK, KBRR and APU.

```{r anchor metadata}

anchor_sites <- read_csv(paste0(anchor_wd, "/anchor_sites.csv", collapse = ""))

anchor_daily %>% 
  distinct(SiteID) %>% 
  full_join(anchor_sites)

anchor_md <- anchor_daily %>% 
  distinct(SiteID) %>% 
  left_join(anchor_sites) %>% 
  mutate(seq_id = NA,
         SourceName = case_when(grepl("APU", SiteID) ~ "APU",
                                grepl("CIK|AR2", SiteID) ~ "CIK",
                                TRUE ~ "KBRR"),
         Contact_person = case_when(SourceName == "CIK" ~ "Sue Mauger",
                                    SourceName == "APU" ~ "John Hagan",
                                    TRUE ~ "Steve Baird"),
         Contact_email = case_when(SourceName == "CIK" ~ "sue@inletkeeper.org",
                                    SourceName == "APU" ~ NA_character_,
                                    TRUE ~ "sjbaird@alaska.edu"),
         Contact_telephone = case_when(SourceName == "CIK" ~ "907-235-4068",
                                    SourceName == "APU" ~ NA_character_,
                                    TRUE ~ "907-235-1594"),
         Sensor_Placement = "Main channel",
         Sensor_accuracy = "level 1  ( +/- 0.2 deg C)",
         Sensor_QAQC = 4,
         Waterbody_type = "S",
         Waterbody_name = NA) %>% 
  rename(Latitude = latitude,
         Longitude = longitude)

```

NOT using climate data.
 
```{r anchor climate data, eval = FALSE}

anchor_climate <- read_csv(paste0(anchor_wd, "/anchor_climate_variables.csv", collapse = ""))

anchor_climate

anchor_daily <- left_join(anchor_daily, anchor_climate %>% select(catchmentID, sampleDate, tair3))

anchor_daily

```

Remove CIK14, which is the same as CIK_14 and we have in Sue's other dataset.

```{r anchor save }
save_metadata_files(anchor_md %>% filter(!SiteID == "CIK14"), "anchor")

save_aktemp_files(anchor_temp, "anchor")

save_daily_files(anchor_daily, "anchor")
```


## Kenai

Read in Kenai data and remove sites that Sue submitted with her other data.

```{r kenai temp data}

kenai_wd <- "W:/Github/Kenai_temperature/output/data_catalog"

kenai_temp <- read_csv(paste0(kenai_wd, "/kenai_temperature_data.csv", collapse = ""))

dup_sites <- readRDS("output/duplicate_CIK_sites.rds") %>% pull(SiteID)

kenai_temp <- kenai_temp %>% 
  filter(!SiteID %in% dup_sites) 

kenai_daily <- temp_msmt_freq(kenai_temp %>% mutate(UseData = 1)) %>% daily_screen(.) %>% ungroup()

kenai_daily %>% summary

```

Create metadata for kwf sites and get metadata info for Ben Meyer from AKOATS. We saved the data with different siteIDs, which can be replaced so that they match what was submitted for AKOATS and what Ben used.

```{r kenai metadata}

kenai_sites <- read_csv(paste0(kenai_wd, "/kenai_sites.csv", collapse = ""))

kenai_daily %>% 
  distinct(SiteID) %>% 
  left_join(kenai_sites)

akoats <- read_excel("S:/EPA AKTEMP/AKOATS_DATA_2020_working.xlsx", sheet = "AKOATS_COMPLETE") 
site_loc <- read_csv("W:/Github/Kenai_temperature/data/Kenai/Kenai_site_locations.csv")

#get akoats ids for Ben's sites.
benM <- site_loc %>% select(Description, siteID) %>% 
  left_join(akoats %>% filter(Contact_person == "Benjamin Meyer"), by = c("Description" = "Agency_ID")) %>% 
  rename(SiteID = siteID)

kenai_md <- kenai_daily %>% 
  distinct(SiteID) %>% 
  left_join(kenai_sites) 

kenai_md <- kenai_md %>% 
  left_join(benM %>% select(SiteID, seq_id, Description)) %>% 
  rename(Agency_ID = Description,
         Latitude = latitude,
         Longitude = longitude) %>% 
  mutate(SourceName = case_when(grepl("KWF1|KWF2|KWF3", SiteID) ~ "KWF",
                                TRUE ~ "UAF"),
         Contact_person = case_when(SourceName == "KWF" ~ "Brandon Borneman",
                                    TRUE ~ "Ben Meyer"))
```

NOT using climate data.

```{r kenai climate data, eval = FALSE}

kenai_climate <- read_csv(paste0(kenai_wd, "/kenai_climate_variables.csv", collapse = ""))

kenai_climate <- kenai_climate %>% 
  mutate(sampleDate = as.Date(sampleDate))

str(kenai_daily)
str(kenai_climate)

kenai_daily <- left_join(kenai_daily, kenai_climate %>% select(catchmentID, sampleDate, tair3))

kenai_daily %>% summary

#lots of missing winter airtemps
kenai_daily %>%
  ungroup() %>% 
  filter(is.na(tair3)) %>% 
  count(month(sampleDate))

#missing 2019 summer airtemps that were downloaded for usgs sites
kenai_daily %>%
  ungroup() %>% 
  filter(is.na(tair3), month(sampleDate) == 6)

```


Save final files.

```{r anchor save }
save_metadata_files(kenai_md, "kenai")

save_aktemp_files(kenai_temp, "kenai")

save_daily_files(kenai_daily, "kenai")

kenai_daily %>% distinct(SiteID)
```

## Combine

Combine the three datasets and see if there are years with overlapping data. No overlap between Anchor or Kenai and Deshka because that project started in 2017.

```{r combine daily data}
ci_daily <- bind_rows(deshka_daily %>% mutate(wtd = "Deshka"), anchor_daily %>% mutate(wtd = "Anchor"), kenai_daily %>% mutate(wtd = "Kenai")) %>%
  ungroup() %>% 
  mutate(year = year(sampleDate))

```



```{r remove extra data}
rm(kenai_temp, kenai_sites, kenai_climate)
rm(anchor_temp, anchor_sites, anchor_climate)
rm(deshka_temp, deshka_sites, deshka_climate)

```


# Read in data from Deshka, Anchor, and Kenai - v1 with daily data included - ARCHIVE NOT RUN.

## Deshka 

For each watershed, temperature data, site data, and climate data are all needed. Read in temperature data first and convert to daily means, link catchment ids to temperature data using sites table, and join climate data to final temperature dataset. 

For Deshka and Kenai, there is a separate temperature data csv with daily data.

```{r deshka temp data}

deshka_wd <- "W:/Github/Deshka_temperature/output/data_catalog"

deshka_temp <- read_csv(paste0(deshka_wd, "/deshka_temperature_data.csv", collapse = ""))
deshka_temp2 <- read_csv(paste0(deshka_wd, "/deshka_temperature_data2.csv", collapse = ""))

deshka_temp2 <- deshka_temp2 %>% 
  filter(useData == 1) %>% 
  rename(meanTemp = Temperature) %>% 
  mutate(SiteID = tolower(SiteID))

deshka_daily <- deshka_temp %>%
  filter(useData == 1) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanTemp = mean(Temperature))

deshka_daily <- bind_rows(deshka_daily, deshka_temp2)

deshka_daily %>% distinct(SiteID)
```

Three edits to site table - kroto 5 downstream misspelled, chijik trib should be chijik to match temperature SiteID and PMC1 listed twice. In temperature dataset, there is a PMC1 and PMC1-2, which are in the same catchment and just slightly separated. Keep PMC1, which has the longer time series.

```{r deshka catchment ID}

deshka_sites <- read_csv(paste0(deshka_wd, "/deshka_sites.csv", collapse = ""))

deshka_sites %>% 
  arrange(SiteID)

deshka_sites <- deshka_sites %>%
  mutate(SiteID = case_when(SiteID == "Kroto 5 Downstrean" ~ "Kroto 5 Downstream",
                            SiteID == "chijik trib.arri" ~ "chijik.arri",
                            TRUE ~ SiteID)) %>% 
  filter(!(SiteID == "PMC1" & useSite == 1))

deshka_daily %>% 
  filter(SiteID %in% c("PMC1", "PMC1-2")) %>% 
  summarize(mindate = min(sampleDate),
            maxdate = max(sampleDate))

deshka_daily <- left_join(deshka_daily, deshka_sites %>% select(SiteID, catchmentID))

deshka_daily %>% distinct(SiteID, catchmentID)

#remove pmc1-2
deshka_daily <- deshka_daily %>% 
  filter(!(is.na(catchmentID)))

deshka_daily

```


```{r deshka climate data}

deshka_climate <- read_csv(paste0(deshka_wd, "/deshka_climate_variables.csv", collapse = ""))

deshka_climate

deshka_daily <- left_join(deshka_daily, deshka_climate %>% select(catchmentID, sampleDate, tair3))

deshka_daily

```


## Anchor

```{r anchor temp data}

anchor_wd <- "W:/Github/Temperature_Data/output/data_catalog"

anchor_temp <- read_csv(paste0(anchor_wd, "/anchor_temperature_data.csv", collapse = ""))

anchor_daily <- anchor_temp %>%
  filter(useData == 1) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanTemp = mean(Temperature))

anchor_daily
```



```{r anchor catchment ID}

anchor_sites <- read_csv(paste0(anchor_wd, "/anchor_sites.csv", collapse = ""))

anchor_sites %>% 
  arrange(SiteID)

anchor_daily %>% 
  distinct(SiteID)

anchor_daily <- left_join(anchor_daily, anchor_sites %>% select(SiteID, catchmentID))

anchor_daily %>% distinct(SiteID, catchmentID)

anchor_daily

```


```{r anchor climate data}

anchor_climate <- read_csv(paste0(anchor_wd, "/anchor_climate_variables.csv", collapse = ""))

anchor_climate

anchor_daily <- left_join(anchor_daily, anchor_climate %>% select(catchmentID, sampleDate, tair3))

anchor_daily

```


## Kenai

```{r kenai temp data}

kenai_wd <- "W:/Github/Kenai_temperature/output/data_catalog"

kenai_temp <- read_csv(paste0(kenai_wd, "/kenai_temperature_data.csv", collapse = ""))
kenai_temp2 <- read_csv(paste0(kenai_wd, "/kenai_temperature_data2.csv", collapse = ""))

kenai_temp2 <- kenai_temp2 %>% 
  rename(meanTemp = Temperature) 

kenai_daily <- kenai_temp %>%
  filter(useData == 1) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meanTemp = mean(Temperature))

kenai_daily <- bind_rows(kenai_daily, kenai_temp2)

kenai_daily %>% summary
```


```{r kenai catchment ID}

kenai_sites <- read_csv(paste0(kenai_wd, "/kenai_sites.csv", collapse = ""))

kenai_sites %>% 
  arrange(SiteID)

kenai_daily %>% 
  distinct(SiteID)

kenai_daily <- left_join(kenai_daily, kenai_sites %>% select(SiteID, catchmentID))

kenai_daily %>% distinct(SiteID, catchmentID)

kenai_daily %>% summary

```


```{r kenai climate data}

kenai_climate <- read_csv(paste0(kenai_wd, "/kenai_climate_variables.csv", collapse = ""))

kenai_climate <- kenai_climate %>% 
  mutate(sampleDate = as.Date(sampleDate))

str(kenai_daily)
str(kenai_climate)

kenai_daily <- left_join(kenai_daily, kenai_climate %>% select(catchmentID, sampleDate, tair3))

kenai_daily %>% summary

#lots of missing winter airtemps
kenai_daily %>%
  ungroup() %>% 
  filter(is.na(tair3)) %>% 
  count(month(sampleDate))

#missing 2019 summer airtemps that were downloaded for usgs sites
kenai_daily %>%
  ungroup() %>% 
  filter(is.na(tair3), month(sampleDate) == 6)

```


## Combine

Combine the three datasets and see if there are years with overlapping data. No overlap between Anchor or Kenai and Deshka because that project started in 2017.

```{r combine daily data}
ci_daily <- bind_rows(deshka_daily %>% mutate(wtd = "Deshka"), anchor_daily %>% mutate(wtd = "Anchor"), kenai_daily %>% mutate(wtd = "Kenai")) %>%
  ungroup() %>% 
  mutate(year = year(sampleDate))

```



```{r remove extra data}
rm(kenai_temp, kenai_sites, kenai_climate)
rm(anchor_temp, anchor_sites, anchor_climate)
rm(deshka_temp, deshka_sites, deshka_climate)

```
# Run DFA

Decide on best year(s) to try and run.

```{r identify complete summers and best years}

ci_summer_sites <- ci_daily %>% 
  filter(month(sampleDate) %in% 6:8) %>% 
  group_by(wtd, SiteID, year) %>% 
  summarize(n = n()) %>% 
  filter(n > 82) %>% 
  ungroup()

ci_summer_sites %>% 
  distinct(wtd, SiteID, year) %>% 
  count(year, wtd) %>% 
  pivot_wider(names_from = wtd, values_from = n, values_fill = 0) %>% 
  mutate(max_sites = Kenai + Deshka + Anchor)


```

```{r prepare data}

ci_daily_2010 <- left_join(ci_summer_sites %>% filter(year == 2010) %>% select(SiteID, year), 
                           ci_daily %>% filter(month(sampleDate) %in% 6:8)) %>% 
  arrange(SiteID, year)

ci_daily_2010_zsc <- bind_cols(ci_daily_2010 %>% 
  group_by(SiteID, year) %>% 
  summarize(meanTemp_zsc = scale(meanTemp),
            tair3_zsc = scale(tair3)),
  ci_daily_2010 %>% select(sampleDate, meanTemp, tair3))

ci_daily_2010_zsc 

ci_daily_2010 %>% 
  filter(SiteID == "CIK14") %>% 
  summarize(mean = mean(meanTemp),
            sd = sd(meanTemp))

ci_daily_2010 %>% 
  filter(SiteID == "CIK14") %>% 
  mutate(zsc = (meanTemp - 12.18014)/1.902544)



```


```{r zscores and remove na cols}

ci_daily_2010_zsc %>% 
  ggplot() +
  geom_line(aes(x = sampleDate, y = meanTemp, color = "blue")) +
  geom_line(aes(x = sampleDate, y = meanTemp_zsc, color = "red")) +
  facet_wrap(~SiteID)

ci_daily_2010_zsc %>% 
  ggplot() +
  geom_line(aes(x = sampleDate, y = meanTemp_zsc, group = SiteID, color = "grey")) + 
  geom_line(aes(x = sampleDate, y = tair3_zsc, group = SiteID, color = "red")) 

ci_daily_2010_zsc

#convert to n x T: n time series over T time steps

ci_2010_stream <- ci_daily_2010_zsc %>%
  ungroup() %>% 
  select(SiteID, sampleDate, meanTemp_zsc) %>% 
  pivot_wider(names_from = sampleDate, values_from = meanTemp_zsc)

ci_2010_air <- ci_daily_2010_zsc %>%
  ungroup() %>% 
  select(SiteID, sampleDate, tair3_zsc) %>% 
  pivot_wider(names_from = sampleDate, values_from = tair3_zsc)

#remove dates with any missing values for stream temp
nacols <- colnames(ci_2010_stream)[colSums(is.na(ci_2010_stream)) > 0]
nacols

ci_2010_stream <- ci_2010_stream %>% select(-c(nacols, SiteID))
ci_2010_air <- ci_2010_air %>% select(-c(nacols, SiteID))

```

```{r dfa}

#example code line from https://github.com/tjcline/dfaTMB/blob/master/SimulationTesting/TestMARSSvsTMB.R
# mod13_marss<-MARSS(noNA_Stream[streamSet,],model=list(m=1,R='unconstrained'),covariates=matrix(noNA_Air,nrow=1),form='dfa')

x = 200 #set for minit
c = 5000 #set for maxit = (x + c) - to ensure convergence

#model set for 2014 - 16 time series and 83 days in JJA
# 1 - no air temp, R diagonal and equal
# 2 - with air temp, R diagonal and equal


control.ops <- list(minit = x, maxit = c) 

ci_mod1 <- MARSS(as.matrix(ci_2010_stream), model = list(m = 1, R = 'diagonal and equal'),
                  form = "dfa", control = control.ops)
ci_mod2 <- MARSS(as.matrix(ci_2010_stream), model = list(m = 1, R = 'diagonal and equal'), covariates = as.matrix(ci_2010_air),
                  form = "dfa", control = control.ops)
ci_mod3 <- MARSS(as.matrix(ci_2010_stream), model = list(m = 2, R = 'diagonal and equal'),
                  form = "dfa", control = control.ops)
ci_mod4 <- MARSS(as.matrix(ci_2010_stream), model = list(m = 2, R = 'diagonal and equal'), covariates = as.matrix(ci_2010_air),
                  form = "dfa", control = control.ops)

AIC(ci_mod1, ci_mod2, ci_mod3, ci_mod4)

coef(ci_mod4)

fit <- get_DFA_fits(ci_mod2)

```

```{r get_DFA_fits function}

MLEobj <- ci_mod2

get_DFA_fits <- function(MLEobj, dd = NULL, alpha = 0.05) {
    ## empty list for results
    fits <- list()
    ## extra stuff for var() calcs
    Ey <- MARSS:::MARSShatyt(MLEobj)
    ## model params
    ZZ <- coef(MLEobj, type = "matrix")$Z
    ## number of obs ts
    nn <- dim(Ey$ytT)[1]
    ## number of time steps
    TT <- dim(Ey$ytT)[2]
    ## get the inverse of the rotation matrix
    H_inv <- varimax(ZZ)$rotmat
    ## check for covars
    if (!is.null(dd)) {
        DD <- coef(MLEobj, type = "matrix")$D
        ## model expectation
        fits$ex <- ZZ %*% H_inv %*% MLEobj$states + DD %*% dd
    } else {
        ## model expectation
        fits$ex <- ZZ %*% H_inv %*% MLEobj$states
    }
    ## Var in model fits
    VtT <- MARSSkfss(MLEobj)$VtT
    VV <- NULL
    for (tt in 1:TT) {
        RZVZ <- coef(MLEobj, type = "matrix")$R - ZZ %*% VtT[, 
            , tt] %*% t(ZZ)
        SS <- Ey$yxtT[, , tt] - Ey$ytT[, tt, drop = FALSE] %*% 
            t(MLEobj$states[, tt, drop = FALSE])
        VV <- cbind(VV, diag(RZVZ + SS %*% t(ZZ) + ZZ %*% t(SS)))
    }
    SE <- sqrt(VV)
    ## upper & lower (1-alpha)% CI
    fits$up <- qnorm(1 - alpha/2) * SE + fits$ex
    fits$lo <- qnorm(alpha/2) * SE + fits$ex
    return(fits)
}

```

