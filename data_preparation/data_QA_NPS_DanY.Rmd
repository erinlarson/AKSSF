---
title: "data_QA_NPS_DanY"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, messages = FALSE)
knitr::opts_knit$set(root.dir = normalizePath("..")) #this sets the root.dir up one level back to the project so that paths are relative to the project directory.

library(readxl)
library(stringr)
library(lubridate)
library(googlesheets4)
library(googledrive)
library(rnoaa)
library(hms)
library(tidyverse)
library(plotly)
library(DT)
library(beepr)
```

```{r Functions}
#setwd("..")
getwd()
source("helper_functions.R")
```

Data QA is done using dynamic plots and reviewing data by site. For AKSSF, we are only reviewing June - September data and flagging obvious air temperatures. Burials are a lot harder to confirm without a duplicate logger.

# QA data for air temperatures
## Pull in Airtemp for comparison from NOAA GHCN stations
Iliamna Airport station seems like a good fit for these data

Go to [GHCN Daily](https://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND)
website and locate stations - try to identify stations with mean/min/max
air temps for period of interest.

DM Notes to PL:  You can adjust the dates of the climate data query to match the data you are working with - Dan's data go back to 2000 so changed line 63 accordingly

```{r NOAA data, message=FALSE, warning=FALSE}
getwd()

#Token obtained from NOAA to access API
noaaTok <- "LmempjqpgcLSxDQWiLvuaOAGmscrQCrb"

# lcd(station = "26410", year = 2013)
# 
# ncdc_locs(locationcategoryid = "CITY")

#Station Codes for area of interest
nps.climStat <- c("USW00025506")

nps.climDat <- tibble( name = c( "ILIAMNA AIRPORT, AK US"),
                 id = nps.climStat)

# Pull Climate data from Iliamna Airport Stations
climDat <- meteo_pull_monitors(nps.climStat)  
str(climDat)

nps.climDat <- nps.climDat %>% 
  left_join( climDat[,c( "id", "date", "tmax", "tmin")], by = 'id') %>% 
  filter( date >= "2000-05-01",
          date <= "2020-12-30",
          name == "ILIAMNA AIRPORT, AK US") %>% 
  # Temperature and Precipitation values are in tenths of degree/mm
  mutate_if( is.numeric, ~ . * 0.1) %>% 
  mutate(year = as.factor(year(date)),
         day = yday(date),
         DT = as.POSIXct(paste(date), format = "%Y-%m-%d"))

nps.climDat

```

## Load Data
Pull in formatted data and make a copy for QA 
Becky has already formatted these data for qa purposes - Data file = nps_dy_5sites.rds

**DM Notes to PL:  Updated SiteID prefix calculation to more simple conversion & added UseSite to remove lake data and calculate UseData = 0 for Little Kijik measurements taken every 2 hours in 2003**


```{r Load Dataset for QA}

# Choose temperature data ouput formatted for qc from data prep script
filename <- file.choose(new = FALSE)
npsDanY.data <- readRDS(filename)

# Check SiteIDs 
dput(unique(npsDanY.data$SiteID))

# Correct SiteIDs if missing NPS_ prefix and  add year column and populate date
npsDanY.data.qc <- npsDanY.data %>% 
  mutate(year = year(sampleDate),
         # Add NPS_ Prefix to site ids
         SiteID = paste0("NPS_", SiteID),
         # Remove Hardenburg Bay and Six Mile Outlet from review as these data are from a lake
         UseSite = case_when(SiteID %in% c("NPS_Hardenburg_Bay","NPS_Six_Mile_Outlet") ~ 0,
                             TRUE~ 1 ),
         # Little Kijik have data from second logger recording ever 2 hours on 2 minutes past the hour - New Halen also have multiple loggers that need to be filtered out starting in July of 2009 with one recording on the hour and the second on the hour + 16 seconds past
         UseData = case_when( SiteID %in% "NPS_Little_Kijik_River_Dan" & year == 2003 & minute(dt) == 02 ~ 0,
                              SiteID %in% "NPS_Newhalen_River" & year == 2009 & second(dt) == 16 ~ 0,
                              TRUE ~ UseData)) %>% 
  filter(month(sampleDate) %in% 6:9,
         UseData == 1, UseSite == 1)

summary(npsDanY.data.qc)

```

```{r Print PDF of Raw Data}

npsd2_sites <- npsDanY.data.qc %>% filter(UseSite == 1) %>% 
  distinct(SiteID, year) %>% arrange(SiteID, year)

pdf("data_preparation/NPS DY 5 Raw Data.pdf", width = 11, height = 8.5)
# Get limits of temp data 
for(i in 1:nrow(npsd2_sites)) {
  dat <- left_join(npsd2_sites %>% slice(i), npsDanY.data.qc)
  subtitle <- dat %>% distinct(UseSite) %>% pull(UseSite)
  xmin <- as.POSIXct(min(dat$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
  xmax <- as.POSIXct(max(dat$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
  p1 <- dat %>% 
    ggplot(aes(x = dt, y = Temperature)) +
    geom_line( data = nps.climDat, aes(x = DT, y = tmin, color = "Air min")) +
    geom_line( data = nps.climDat, aes(x = DT, y = tmax, color = "Air max")) +
    geom_line() +
    scale_x_datetime(limits = c(xmin, xmax), labels = waiver()) +
    scale_y_continuous(limits = c(-5, 30), labels = waiver()) +
    labs(title = npsd2_sites %>% slice(i) %>% unite(site_year) %>%
           pull(site_year),
         subtitle = paste0("Use Site: ", subtitle)) +
    theme(legend.position = "bottom")
  print(p1)
}

dev.off()

```

## Interactive Plot

Walk through list of Sites and note on google sheet any anomalous measurements and leave note for DM to check

Review Sites with interactive plot and note Airtemps/Burials etc on data flag worksheet stored here [google sheet](https://docs.google.com/spreadsheets/d/1PsDk7xZHQo6qknzZibVOVO4eFdk-I8PqtFHRDTStKnc/edit#gid=0).

Most sites also visible in [online mapper](https://accsmaps.maps.arcgis.com/home/webmap/viewer.html?webmap=364d7ce98dd64b469fce23b06751f989)  - look/filter NPS sites and Dan Young as contact

Sites:
"NPS_Chulitna_SiteB"

"NPS_Little_Kijik_River_Dan" 
**DM Notes: Looks like data from 2 loggers here in 2003 - 1 recording on the hour and one every 2 hours - filtered these out**

"NPS_Newhalen_River"
**DM Notes: Multiple loggers recording in 2009 so I have filtered out the duplicate series above - Loggers appear to have been shifted around so I have flagged them in the google sheet for removal where this seems apparent - may want to check with data provider** 

**DM Notes: "NPS_Six_Mile_Outlet" & "NPS_Hardenburg_Bay" are lake data and should not be included.  Most of the older temp data are very "choppy", possibly due to different logger models?**

```{r Interactive Plot With Air Temp}
# get site ids
#dput(unique(npsDanY.data.qc$SiteID))

p <- npsDanY.data.qc %>% 
  filter(SiteID == "NPS_Chulitna_SiteB") #filtered by site

xmin <- as.POSIXct(min(p$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
xmax <- as.POSIXct(max(p$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")

p <- p %>% 
  ggplot() +
  geom_line(data = nps.climDat, aes(x = DT, y = tmin, color = "Air min")) +
  geom_line(data = nps.climDat, aes(x = DT, y = tmax, color = "Air max")) +
  geom_line(aes(x = dt, y = Temperature)) +
  scale_x_datetime(limits = c(xmin, xmax), labels = waiver()) +
  #coord_cartesian(ylim = c(-5, 30)) + 
  facet_wrap(~SiteID) +
  theme(legend.title = element_blank()) +
  labs(title = "Temperature by Site",
       y = "Temperature degrees C",
       x = "Time of Measurement")
ggplotly(p)
```

# Remove sites

UseSites to record sites for AKSSF analysis

Recalculate UseData based on visual examination

Filter to remove any sites that are not being used for the AKSSF analysis. These could be sites with very incomplete time series or sites that have hydrologic inputs that affect stream temperatures -- e.g. tidally-influenced sites. Note that a list of these sites may be developed at the top of this script so that we don't spend time reviewing data for these sites. That is fine, just note that the sites not used for the AKSSF analysis were NOT reviewed.

Temperature data flagged for UseData = 0 based on visual examination of plots 
and stored in [google sheet](https://docs.google.com/spreadsheets/d/1usYGIlMMQzTqzIqx0r872xo7_aIYth0P_z47YgQoDEc/edit#gid=0).


```{r Pull In Googlesheet Flags, message=FALSE, warning=FALSE}
#Must activate a token to read in this sheet. This is done by running the chunk and following the prompts that populate in the Console area below. You'll want to enter '1' for Yes to give authorization to read in the sheet. It should then open up a Google page in the web browser and if you follow the prompts, it will provide an authorization code to paste/enter into the Console.

gs4_auth() #select 1 (yes, to give authorization)
#gs4_user()
temp_log_db_gs = "https://docs.google.com/spreadsheets/d/1PsDk7xZHQo6qknzZibVOVO4eFdk-I8PqtFHRDTStKnc/edit#gid=0"

#Read in flag data sheet
flagDb <- read_sheet(temp_log_db_gs,sheet = "AKSSF_Data_Flags",
                    col_names = TRUE,
                    col_types = "c")

#create cols variable 
cols <- c("SiteID", "FlagStart", "FlagEnd","Days", "FlagReason", "UseSite", "UseData", "Notes")

# Transform and drop unnecessary columns
flagDb <- flagDb %>%
  select(all_of(cols)) %>% 
  transform(SiteID = as.character(SiteID),
            FlagStart = as_date(ymd(FlagStart)),
            FlagEnd= as_date(ymd(FlagEnd)),
            Days = as.numeric(Days),
            FlagReason = as.character(FlagReason),
            UseSite = as.numeric(UseSite),
            UseData = as.numeric(UseData),
            Notes = as.character(Notes))
           
#convert date range to dates 
flagDb2 <- flagDb %>% 
  mutate(flagDate = map2(FlagStart, FlagEnd, ~seq(from = .x, to = .y,
                                                  by = "day"))) %>% 
  unnest() %>%
  select(SiteID, flagDate, FlagReason) %>%
  distinct()

str(flagDb2)
```
```{r Recalculate UseData}
#Tibble for final qc data
npsDanY.data.finalqc <- tibble()

#Recalculate UseData values using information stored in flagDb tibble

npsDanY.data.finalqc <- npsDanY.data.qc %>% 
  left_join( flagDb2, by = c("SiteID" = "SiteID", "sampleDate" = "flagDate")) %>% 
  mutate(UseData = case_when(FlagReason == "Air Temperature" ~ 0,
                             FlagReason == "Burial" ~ 0,
                             FlagReason == "Logger Failure" ~ 0,
                             FlagReason == "Other" ~ 0,
                             FlagReason == "Tidal Influence" ~ 0,
                             TRUE ~ UseData),
         year=year(sampleDate))
  

npsDanY.data.finalqc %>% count(SiteID, year, UseData, FlagReason)

summary(npsDanY.data.finalqc)

summary(npsDanY.data.qc)
```


# Compare raw data (npsDanY.data.qc) to QC'd data (npsDanY.data.finalqc)
Use rolling pdf code to print out pdf of QC'd data by site/year and compare side by side with original pdf of these data.

```{r plot of recalculated data by site-year}

#Filters out recalculated UseData from google sheet
npsDanY.data.finalqc <- npsDanY.data.finalqc %>% 
  filter(UseData == 1)

npsd_sites <- npsDanY.data.finalqc %>% distinct(SiteID, year) %>% arrange(SiteID, year)

pdf("data_preparation/NPS DanY QC Data by Site and Year.pdf", width = 11, height = 8.5)
# Get limits of temp data 
for(i in 1:nrow(npsd_sites)) {
  dat <- left_join(npsd_sites %>% slice(i), npsDanY.data.finalqc)
  #subtitle <- dat %>% distinct(UseSite) %>% pull(UseSite)
  xmin <- as.POSIXct(min(dat$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
  xmax <- as.POSIXct(max(dat$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
  p1 <- dat %>% 
    ggplot(aes(x = dt, y = Temperature)) +
    geom_line( data = nps.climDat, aes(x = DT, y = tmin, color = "Air min")) +
    geom_line( data = nps.climDat, aes(x = DT, y = tmax, color = "Air max")) +
    geom_line() +
    scale_x_datetime(limits = c(xmin, xmax), labels = waiver()) +
    scale_y_continuous(limits = c(-5, 30), labels = waiver()) +
    labs(title = npsd_sites %>% slice(i) %>% unite(site_year) %>%
           pull(site_year))#,
         #subtitle = paste0("Use Site: ", subtitle)) +
    theme(legend.position = "bottom")
  print(p1)
}

dev.off()

getwd()

```

# Check measurement frequency

```{r Temperature Measuremnt Frequency}
msmt_freq <- temp_msmt_freq(npsDanY.data.finalqc)

summary(msmt_freq)

# Check mode_diff
msmt_freq[order(msmt_freq$mode_diff, decreasing = FALSE),]
```

# Daily screen

```{r Daily Screen}
daily_screen <- daily_screen(msmt_freq)

summary(daily_screen)
```

# Save daily data

Save a copy of the daily statistics in the final data folder for the AKSSF analysis. There are two helper functions that add the mode of the time difference for each day and calculate the daily min, mean, and max and removed days with less than 90% of measurements.

* temp_msmt_freq
* daily_screen

```{r Save Daily}
# Calculate temp measurement frequency and daily summaries for qc'd data only for Usedata = 1
acronym = "npsDanY_"

# Save Daily Summaries
save_daily_files(daily_screen, acronym = acronym)
getwd()
```

