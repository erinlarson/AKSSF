---
title: "data_QA_NPS_TreyS"
output: html_document
---

Notes to Dustin: Take a look at the yaml and the setup chunk too. I think all of this could be fixed across the different datasets and it would make it go quicker.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, messages = FALSE)
knitr::opts_knit$set(root.dir = normalizePath("..")) #this sets the root.dir up one level back to the project so that paths are relative to the project directory.

library(readxl)
library(stringr)
library(lubridate)
library(googlesheets4)
library(googledrive)
library(rnoaa)
library(hms)
library(tidyverse)
library(plotly)
library(DT)
library(beepr)
```

```{r Functions}
#setwd("..")
getwd()
source("helper_functions.R")
```

Data QA is done using dynamic plots and reviewing data by site. For AKSSF, we are only reviewing June - September data and flagging obvious air temperatures. Burials are a lot harder to confirm without a duplicate logger.

# QA data for air temperatures
## Pull in Airtemp for comparison from NOAA GHCN stations
Glennallen and Chistochina Stations seem like a good fit for these data

Go to [GHCN Daily](https://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND)
website and locate stations - try to identify stations with mean/min/max
air temps for period of interest.

Note - have had trouble with lcd function recently

```{r NOAA data, message=FALSE, warning=FALSE}
getwd()

#Token obtained from NOAA to access API
noaaTok <- "LmempjqpgcLSxDQWiLvuaOAGmscrQCrb"

#Station Codes for area of interest
nps.climStat <- c("USW00056401","USR0000ACHS")

nps.climDat <- tibble( name = c( "GLENNALLEN 64 N, AK US", "CHISTOCHINA ALASKA, AK US"),
                 id = nps.climStat)

# Pull Climate data from Glennallen or Chistochina Stations
climDat <- meteo_pull_monitors(nps.climStat)  
str(climDat)

nps.climDat <- nps.climDat %>% 
  left_join( climDat[,c( "id", "date", "tmax", "tmin")], by = 'id') %>% 
  filter( date >= "2008-05-01",
          date <= "2020-12-30",
          name == "CHISTOCHINA ALASKA, AK US") %>% 
  # Temperature and Precipitation values are in tenths of degree/mm
  mutate_if( is.numeric, ~ . * 0.1) %>% 
  mutate(year = as.factor(year(date)),
         day = yday(date),
         DT = as.POSIXct(paste(date), format = "%Y-%m-%d"))

nps.climDat

```

## Load Data
Pull in formatted data and make a copy for QA 
Becky has already formatted these data for qa purposes - Data file = nps.data.rds

```{r Load Dataset for QA}
# Choose temperature data ouput formatted for qc from data prep script
filename <- file.choose(new = FALSE)
npsTreyS.data <- readRDS(filename)

# Copy and format for qc - limit to use site and months of interest
npsTreyS.data.qc <- npsTreyS.data %>% 
  mutate(SiteID = paste0("NPS_", SiteID),
         UseData = case_when( SiteID == "NPS_Caribou Creek" & second(DT) == 29 ~ 0,
                             TRUE ~ UseData)) %>% 
           filter(month(sampleDate) %in% 6:9,
                  UseData == 1)
  
summary(npsTreyS.data.qc)

```
## Interactive Plot

Walk through list of Sites and note on google sheet any anomalous measurements and leave note for DM to check

Review Sites with interactive plot and note Airtemps/Burials etc on data flag worksheet stored here [google sheet](https://docs.google.com/spreadsheets/d/1usYGIlMMQzTqzIqx0r872xo7_aIYth0P_z47YgQoDEc/edit#gid=0).

Most sites also visible in [online mapper](https://accsmaps.maps.arcgis.com/home/webmap/viewer.html?webmap=364d7ce98dd64b469fce23b06751f989)  - look/filter NPS sites and Trey Simmons as contact

Sites:
"NPS_Caribou Creek"

"NPS_Crystal Creek"

"NPS_Gilahina River"

"NPS_Lakina River" 

"NPS_Long Lake Creek"

"NPS_Rock Creek WRST"
**DM Notes: Trey notes Burial in Rock Creek on 2012-07-19 & 07-21 but data this is not evident to me in the temp data where this burial may have started - Data very erratic in June 2008/2009**

"NPS_Rufus Creek"


```{r Interactive Plot With Air Temp}
# get site ids
#dput(unique(npsTreyS.data.qc$SiteID))
#Change to bb.data.qc to examine qc'd data
p <- npsTreyS.data.qc %>% 
  filter(SiteID == "NPS_Rock Creek WRST", UseData == 1)

xmin <- as.POSIXct(min(p$DT),format = "%Y-%m-%d %H:%M", tz = "GMT")
xmax <- as.POSIXct(max(p$DT),format = "%Y-%m-%d %H:%M", tz = "GMT")

p <- p %>% 
  ggplot() +
  geom_line(data = nps.climDat, aes(x = DT, y = tmin, color = "Air min")) +
  geom_line(data = nps.climDat, aes(x = DT, y = tmax, color = "Air max")) +
  geom_line(aes(x = DT, y = Temperature)) +
  scale_x_datetime(limits = c(xmin, xmax), labels = waiver()) +
  #coord_cartesian(ylim = c(-5, 30)) + 
  facet_wrap(~SiteID) +
  theme(legend.title = element_blank()) +
  labs(title = "Temperature by Site",
       y = "Temperature degrees C",
       x = "Time of Measurement")
ggplotly(p)
```

# Remove sites

UseSites to record sites for AKSSF analysis

Recalculate UseData based on visual examination

Filter to remove any sites that are not being used for the AKSSF analysis. These could be sites with very incomplete time series or sites that have hydrologic inputs that affect stream temperatures -- e.g. tidally-influenced sites. Note that a list of these sites may be developed at the top of this script so that we don't spend time reviewing data for these sites. That is fine, just note that the sites not used for the AKSSF analysis were NOT reviewed.

Temperature data flagged for UseData = 0 based on visual examination of plots 
and stored in [google sheet](https://docs.google.com/spreadsheets/d/1usYGIlMMQzTqzIqx0r872xo7_aIYth0P_z47YgQoDEc/edit#gid=0).


```{r Pull In Googlesheet Flags, message=FALSE, warning=FALSE}
#Must activate a token to read in this sheet. This is done by running the chunk and following the prompts that populate in the Console area below. You'll want to enter '1' for Yes to give authorization to read in the sheet. It should then open up a Google page in the web browser and if you follow the prompts, it will provide an authorization code to paste/enter into the Console.

gs4_auth() #select 1 (yes, to give authorization)
#gs4_user()
temp_log_db_gs = "https://docs.google.com/spreadsheets/d/1usYGIlMMQzTqzIqx0r872xo7_aIYth0P_z47YgQoDEc/edit#gid=0"

#Read in flag data sheet
flagDb <- read_sheet(temp_log_db_gs,sheet = "AKSSF_Data_Flags",
                    col_names = TRUE,
                    col_types = "c")

#create cols variable 
cols <- c("SiteID", "FlagStart", "FlagEnd","Days", "FlagReason", "UseSite", "UseData", "Notes")

# Transform and drop unnecessary columns
flagDb <- flagDb %>%
  select(all_of(cols)) %>% 
  transform(SiteID = as.character(SiteID),
            FlagStart = as_date(ymd(FlagStart)),
            FlagEnd= as_date(ymd(FlagEnd)),
            Days = as.numeric(Days),
            FlagReason = as.character(FlagReason),
            UseSite = as.numeric(UseSite),
            UseData = as.numeric(UseData),
            Notes = as.character(Notes))
           
#convert date range to dates 
flagDb2 <- flagDb %>% 
  mutate(flagDate = map2(FlagStart, FlagEnd, ~seq(from = .x, to = .y,
                                                  by = "day"))) %>% 
  unnest() %>%
  select(SiteID, flagDate, FlagReason) %>%
  distinct()

str(flagDb2)
```
```{r Recalculate UseData}
#Tibble for final qc data
npsTreyS.data.finalqc <- tibble()

#Recalculate UseData values using information stored in flagDb tibble

npsTreyS.data.finalqc <- npsTreyS.data.qc %>% 
  left_join( flagDb2, by = c("SiteID" = "SiteID", "sampleDate" = "flagDate")) %>% 
  mutate(UseData = case_when(FlagReason == "Air Temperature" ~ 0,
                             FlagReason == "Burial" ~ 0,
                             FlagReason == "Logger Failure" ~ 0,
                             FlagReason == "Other" ~ 0,
                             FlagReason == "Tidal Influence" ~ 0,
                             TRUE ~ UseData),
         year=year(sampleDate))
  

npsTreyS.data.finalqc %>% count(SiteID, year, UseData, FlagReason)

summary(npsTreyS.data.finalqc)

summary(npsTreyS.data.qc)
```
# Compare raw data (npsTreyS.data.qc) to QC'd data (npsTreyS.data.finalqc)
Use rolling pdf code to print out pdf of QC'd data by site/year and compare side by side with original pdf of these data.  Code starts line 226 pf data_QA_USFS_Chugach.rmd 

```{r plot of recalculated data by site-year}

#Filters out recalculated UseData from google sheet
npsTreyS.data.finalqc <- npsTreyS.data.finalqc %>% 
  filter(UseData == 1)

npst_sites <- npsTreyS.data.finalqc %>% distinct(SiteID, year) %>% arrange(SiteID, year)

pdf("data_preparation/NPS Treys QC Data by Site and Year.pdf", width = 11, height = 8.5)
# Get limits of temp data 
for(i in 1:nrow(npst_sites)) {
  dat <- left_join(npst_sites %>% slice(i), npsTreyS.data.finalqc)
  #subtitle <- dat %>% distinct(UseSite) %>% pull(UseSite)
  xmin <- as.POSIXct(min(dat$DT),format = "%Y-%m-%d %H:%M", tz = "GMT")
  xmax <- as.POSIXct(max(dat$DT),format = "%Y-%m-%d %H:%M", tz = "GMT")
  p1 <- dat %>% 
    ggplot(aes(x = DT, y = Temperature)) +
    geom_line( data = nps.climDat, aes(x = DT, y = tmin, color = "Air min")) +
    geom_line( data = nps.climDat, aes(x = DT, y = tmax, color = "Air max")) +
    geom_line() +
    scale_x_datetime(limits = c(xmin, xmax), labels = waiver()) +
    scale_y_continuous(limits = c(-5, 30), labels = waiver()) +
    labs(title = npst_sites %>% slice(i) %>% unite(site_year) %>%
           pull(site_year))#,
         #subtitle = paste0("Use Site: ", subtitle)) +
    theme(legend.position = "bottom")
  print(p1)
}

dev.off()

```

# Check measurement frequency

```{r Temperature Measuremnt Frequency}
msmt_freq <- temp_msmt_freq(npsTreyS.data.finalqc)

summary(msmt_freq)

# Check mode_diff
msmt_freq[order(msmt_freq$mode_diff, decreasing = FALSE),]
```

# Daily screen

```{r Daily Screen}
daily_screen <- daily_screen(msmt_freq)

summary(daily_screen)
```

# Save daily data

Save a copy of the daily statistics in the final data folder for the AKSSF analysis. There are two helper functions that add the mode of the time difference for each day and calculate the daily min, mean, and max and removed days with less than 90% of measurements.

* temp_msmt_freq
* daily_screen

```{r Save Daily}
# Calculate temp measurement frequency and daily summaries for qc'd data only for Usedata = 1
acronym = "npsTreyS_"

# Save Daily Summaries
save_daily_files(daily_screen, acronym = acronym)
getwd()
```

