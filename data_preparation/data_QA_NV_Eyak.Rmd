---
title: "data_QA_NV_Eyak"
output: html_document
---

Notes to Dustin: Take a look at the yaml and the setup chunk too. I think all of this could be fixed across the different datasets and it would make it go quicker.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, messages = FALSE)
knitr::opts_knit$set(root.dir = normalizePath("..")) #this sets the root.dir up one level back to the project so that paths are relative to the project directory.

library(readxl)
library(stringr)
library(lubridate)
library(googlesheets4)
library(googledrive)
library(rnoaa)
library(hms)
library(tidyverse)
library(plotly)
library(DT)
library(beepr)
```

```{r Functions}
#setwd("..")
getwd()
source("helper_functions.R")
```

Data QA is done using dynamic plots and reviewing data by site. For AKSSF, we are only reviewing June - September data and flagging obvious air temperatures. Burials are a lot harder to confirm without a duplicate logger.

# QA data for air temperatures
## Pull in Airtemp for comparison from NOAA GHCN stations
Cordova Station seems like a good fit for these data

Go to [GHCN Daily](https://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND)
website and locate stations - try to identify stations with mean/min/max
air temps for period of interest.

Note - have had trouble with lcd function recently

```{r NOAA data, message=FALSE, warning=FALSE}
getwd()
#Token obtained from NOAA to access API
noaaTok <- "LmempjqpgcLSxDQWiLvuaOAGmscrQCrb"
# lcd(station = "26410", year = 2013)
# 
# ncdc_locs(locationcategoryid = "CITY")
#Station Codes for area of interest
eyak.climStat <- c("USW00026410")

eyak.climDat <- tibble( name = c( "CORDOVA AIRPORT, AK US"),
                 id = eyak.climStat)

# Pull Climate data from Cordova Airport
climDat <- meteo_pull_monitors(eyak.climStat)  
str(climDat)

eyak.climDat <- eyak.climDat %>% 
  left_join( climDat[,c( "id", "date", "tmax", "tmin")], by = 'id') %>% 
  filter( date >= "2008-05-01",
          date <= "2020-12-30",
          name == "CORDOVA AIRPORT, AK US") %>% 
  # Temperature and Precipitation values are in tenths of degree/mm
  mutate_if( is.numeric, ~ . * 0.1) %>% 
  mutate(year = as.factor(year(date)),
         day = yday(date),
         DT = as.POSIXct(paste(date), format = "%Y-%m-%d"))
  
eyak.climDat
```

## Load Data
Pull in formatted data and make a copy for QA 
Becky has already formatted these data for qa purposes - Data file = eyak.data.rds

```{r Load Dataset for QA}
# Choose temperature data ouput formatted for qc from data prep script
filename <- file.choose(new = FALSE)
NVEyak.data <- readRDS(filename)

# Copy and format for qc - limit to use site and months of interest
NVEyak.data.qc <- NVEyak.data %>% 
  filter(month(sampleDate) %in% 6:9)
summary(NVEyak.data.qc)
```
## Interactive Plot

Walk through list of Sites and note on google sheet any anomalous measurements and leave note for DM to check

Review Sites with interactive plot and note Airtemps/Burials etc on data flag worksheet stored here [google sheet](https://docs.google.com/spreadsheets/d/1XDUhe8hf08epm8yH9vVxZ92DyIAB-TnJGGFV5wRPu8c/edit#gid=0).

Most sites also visible in [online mapper](https://accsmaps.maps.arcgis.com/home/webmap/viewer.html?webmap=364d7ce98dd64b469fce23b06751f989)  - look/filter Eyak Native Village sites and James Paley as contact

Sites:
"nv_Eyak_Hartney Creek", "nv_Eyak_Heney Creek"

```{r Interactive Plot With Air Temp}
# get site ids
#dput(unique(NVEyak.data.qc$SiteID))
#Change to bb.data.qc to examine qc'd data
p <- NVEyak.data.qc %>% 
  filter(SiteID == "nv_Eyak_Hartney Creek", UseData == 1)
xmin <- as.POSIXct(min(p$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
xmax <- as.POSIXct(max(p$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
p <- p %>% 
  ggplot() +
  geom_line( data = eyak.climDat, aes(x = DT, y = tmin, color = "Air min")) +
  geom_line( data = eyak.climDat,aes(x = DT, y = tmax, color = "Air max")) +
  geom_line(aes(x = dt, y = Temperature)) +
  scale_x_datetime(limits = c(xmin, xmax), labels = waiver()) +
  #coord_cartesian(ylim = c(-5, 30)) + 
  facet_wrap(~SiteID) +
  theme(legend.title = element_blank()) +
  labs(title = "Temperature by Site",
       y = "Temperature degrees C",
       x = "Time of Measurement")
ggplotly(p)
```

# Remove sites

Recalculate UseData values using flagged data stored in google sheet

# Remove sites

UseSites to record sites for AKSSF analysis

Recalculate UseData based on visual examination

Filter to remove any sites that are not being used for the AKSSF analysis. These could be sites with very incomplete time series or sites that have hydrologic inputs that affect stream temperatures -- e.g. tidally-influenced sites. Note that a list of these sites may be developed at the top of this script so that we don't spend time reviewing data for these sites. That is fine, just note that the sites not used for the AKSSF analysis were NOT reviewed.

Temperature data flagged for UseData = 0 based on visual examination of plots 
and stored in https://docs.google.com/spreadsheets/d/1XDUhe8hf08epm8yH9vVxZ92DyIAB-TnJGGFV5wRPu8c/edit#gid=0.


```{r Pull In Googlesheet Flags, message=FALSE, warning=FALSE}
#Must activate a token to read in this sheet. This is done by running the chunk and following the prompts that populate in the Console area below. You'll want to enter '1' for Yes to give authorization to read in the sheet. It should then open up a Google page in the web browser and if you follow the prompts, it will provide an authorization code to paste/enter into the Console.

gs4_auth() #select 1 (yes, to give authorization)
#gs4_user()
temp_log_db_gs = "https://docs.google.com/spreadsheets/d/1XDUhe8hf08epm8yH9vVxZ92DyIAB-TnJGGFV5wRPu8c/edit#gid=0"

#Read in flag data sheet
flagDb <- read_sheet(temp_log_db_gs,sheet = "AKSSF_Data_Flags",
                    col_names = TRUE,
                    col_types = "c")
#create cols variable 
cols <- c("SiteID", "FlagStart", "FlagEnd","Days", "FlagReason", "UseSite", "UseData", "Notes")
# Transform and drop unnecessary columns
flagDb <- flagDb %>%
  select(all_of(cols)) %>% 
  transform(SiteID = as.character(SiteID),
            FlagStart = as_date(ymd(FlagStart)),
            FlagEnd= as_date(ymd(FlagEnd)),
            Days = as.numeric(Days),
            FlagReason = as.character(FlagReason),
            UseSite = as.numeric(UseSite),
            UseData = as.numeric(UseData),
            Notes = as.character(Notes))

#convert date range to dates 
flagDb2 <- flagDb %>% 
  mutate(flagDate = map2(FlagStart, FlagEnd, ~seq(from = .x, to = .y,
                                                  by = "day"))) %>% 
  unnest() %>%
  select(SiteID, flagDate, FlagReason) %>%
  distinct()
str(flagDb2)
```

```{r Recalculate UseData}
#Tibble for final qc data
NVEyak.data.finalqc <- tibble()

#Recalculate UseData values using information stored in flagDb tibble
NVEyak.data.finalqc <- NVEyak.data.qc %>% 
  left_join( flagDb2, by = c("SiteID" = "SiteID", "sampleDate" = "flagDate")) %>% 
  mutate(UseData = case_when(FlagReason == "Air Temperature" ~ 0,
                             FlagReason == "Burial" ~ 0,
                             FlagReason == "Logger Failure" ~ 0,
                             FlagReason == "Other" ~ 0,
                             FlagReason == "Tidal Influence" ~ 0,
                             TRUE ~ UseData))
  
NVEyak.data.finalqc %>% count(SiteID, year, UseData, FlagReason)

summary(NVEyak.data.finalqc)

summary(NVEyak.data.qc)
```

# Compare raw data (NVEyak.data.qc) to QC'd data (NVEyak.data.finalqc)
Use rolling pdf code to print out pdf of QC'd data by site/year and compare side by side with original pdf of these data.  Code starts line 226 pf data_QA_USFS_Chugach.rmd 

```{r plot of recalculated data by site-year}

#Filters out recalculated UseData from google sheet
NVEyak.data.finalqc <- NVEyak.data.finalqc %>% 
  filter(UseData == 1)

nve_sites <- NVEyak.data.finalqc %>% distinct(SiteID, year) %>% arrange(SiteID, year)

#pdf("data_preparation/NV Eyak QC Data by Site and Year.pdf", width = 11, height = 8.5) #original
pdf(file = "NV Eyak QC Data by Site and Year.pdf", width = 11, height = 8.5) #new but works
# Get limits of temp data
for(i in 1:nrow(nve_sites)) {
  dat <- left_join(nve_sites %>% slice(i), NVEyak.data.finalqc)
  #subtitle <- dat %>% distinct(UseSite) %>% pull(UseSite)
  xmin <- as.POSIXct(min(dat$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
  xmax <- as.POSIXct(max(dat$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
  p1 <- dat %>% 
    ggplot(aes(x = dt, y = Temperature)) +
    geom_line( data = eyak.climDat, aes(x = DT, y = tmin, color = "Air min")) +
    geom_line( data = eyak.climDat, aes(x = DT, y = tmax, color = "Air max")) +
    geom_line() +
    scale_x_datetime(limits = c(xmin, xmax), labels = waiver()) +
    scale_y_continuous(limits = c(-5, 30), labels = waiver()) +
    labs(title = nve_sites %>% slice(i) %>% unite(site_year) %>%
           pull(site_year))#,
         #subtitle = paste0("Use Site: ", subtitle)) +
    theme(legend.position = "bottom")
  print(p1)
}

dev.off()

getwd()

```

# Check measurement frequency

```{r Temperature Measuremnt Frequency}
msmt_freq <- temp_msmt_freq(NVEyak.data.finalqc)

summary(msmt_freq)

# Check mode_diff
msmt_freq[order(msmt_freq$mode_diff, decreasing = FALSE),]
```

# Daily screen

```{r Daily Screen}
daily_screen <- daily_screen(msmt_freq)

summary(daily_screen)
```

# Save daily data

Save a copy of the daily statistics in the final data folder for the AKSSF analysis. There are two helper functions that add the mode of the time difference for each day and calculate the daily min, mean, and max and removed days with less than 90% of measurements.

* temp_msmt_freq
* daily_screen

```{r Save Daily}
# Calculate temp measurement frequency and daily summaries for qc'd data only for Usedata = 1
acronym = "nv_Eyak_"

# Save Daily Summaries
save_daily_files(daily_screen, acronym = acronym)
getwd()
```
