---
title: "data_QA_USFS_Chugach"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, messages = FALSE)
knitr::opts_knit$set(root.dir = normalizePath(".."))
#this sets the root.dir up one level back to the project so that paths are relative to the project directory.

library(readxl)
library(stringr)
library(lubridate)
library(googlesheets4)
library(rnoaa)
library(hms)
library(tidyverse)
library(plotly)
library(DT)

```

```{r Source Helper Functions}
# Check dir
getwd()
# Source helper functions
source("helper_functions.R")

```

Data QA is done using dynamic plots and reviewing data by site. For AKSSF,
we are only reviewing June - September data and flagging obvious air
temperatures. Burials are a lot harder to confirm without a duplicate logger.


# QA data for air temperatures
## Pull in Airtemp for comparison from NOAA GHCN stations

Go to [GHCN Daily](https://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND)
website and locate stations - try to identify stations with mean/min/max
air temps for period of interest.

Note - have had trouble with lcd function recently

```{r NOAA data, message=FALSE, warning=FALSE}
getwd()

#Token obtained from NOAA to access API
noaaTok <- "LmempjqpgcLSxDQWiLvuaOAGmscrQCrb"

# lcd(station = "26410", year = 2013)
# 
# ncdc_locs(locationcategoryid = "CITY")

#Station Codes for area of interest
cd.climStat <- c( "USC00502179","USW00026410", "USW00096405")


cd.climDat <- tibble( name = c( "CORDOVA WWTP", "CORDOVA AIRPORT",
"CORDOVA 14"),
                 id = cd.climStat)

# Pull Climate data from Cordova Airport
climDat <- meteo_pull_monitors(cd.climStat)  
str(climDat)
  

cd.climDat <- cd.climDat %>% 
  left_join( climDat[,c( "id", "date", "tmax", "tmin")], by = 'id') %>% 
  filter( date >= "2008-06-01",
          date <= "2020-12-30",
          name == "CORDOVA AIRPORT") %>% 
  # Temperature and Precipitation values are in tenths of degree/mm
  mutate_if( is.numeric, ~ . * 0.1) %>% 
  mutate(year = as.factor(year(date)),
         day = yday(date),
         dt = as.POSIXct(paste(date), format = "%Y-%m-%d"))
  

cd.climDat

```
# Begin QA

```{r Select Formatted Data Output from data script}

# Choose temperature data ouput formatted for qc from data prep script

filename <- file.choose(new = FALSE)
usfsChug.data <- readRDS(filename)

# Copy and format for qc - limit to usesite and months of interest
usfsChug.data.qc <- usfsChug.data %>% 
  filter(useSite==1, month(sampleDate) %in% 6:9)

summary(usfsChug.data.qc)

```


??MAKE REVIEW GROUPS - TIDAL/NOT - Time period of interest

## Summary Table
Start with interactive data summary table

```{r Summary Table, message=TRUE, warning=FALSE}

# create data summary table
usfsChug.data.summary <- usfsChug.data.qc %>%
  filter(useSite == 1) %>%
  group_by(SiteID, year) %>%
  summarize(meanTemp = mean(Temperature, na.rm = T),
            maxTemp = max(Temperature, na.rm = T),
            minTemp = min(Temperature, na.rm = T),
            sdTemp = sd(Temperature, na.rm = T),
            n_obs = n())

usfsChug.data.summary %>%
  datatable() %>%
  formatRound(columns=c("meanTemp","maxTemp","minTemp","sdTemp"), digits=2)

```

## Check measurement frequency
DM Notes: Something going on with quartz creek data - have two measurements?? 
Dataset QuartzCreek_Surf_2019-11-13 has duplicate measurements.  Not sure if these
are data from two loggers or if something else happened to them.


```{r Temp measuremnt frequency}
msmt_freq <- temp_msmt_freq(usfsChug.data.qc)

summary(msmt_freq)

# Check mode_diff
msmt_freq[order(msmt_freq$mode_diff, decreasing = FALSE),]


```

# Daily screen
Daily screen also suggests something wrong with USFS_Chickaloon Headwaters
and USFS_24.9 Mile Creek data in addition to the quartz creek issues


```{r Daily Screen}

daily_screen <- daily_screen(msmt_freq)

summary(daily_screen)

```

Quick plots of data to make sure they read in ok. Start by summarizing daily means because quicker to plot. Looks like some bad winter temps well below zero that could be clipped later. No obvious air temps in summer as everything is < = 20 or so.

```{r plot of daily means, message=FALSE, warning=FALSE}

usfsChug.data.qc %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meant = mean(Temperature)) %>% 
  ggplot(aes(x = sampleDate, y = meant)) +
  geom_line() +
  facet_wrap(~SiteID)

```

Wrong! The sub-daily temps show more errors, definitely some air temps that need to be removed. This will be a good dataset for testing scripts, although it will probably need cleaning sooner rather than later for AKSSF.

```{r plot of raw data}

usfsChug.data.qc %>% 
  ggplot(aes(x = dt, y = Temperature)) +
  #geom_line( data = cd.climDat, aes(x = dt, y = tmin, color = "Air min")) +
  #geom_line( data = cd.climDat,aes(x = dt, y = tmax, color = "Air max")) +
  geom_line() +
  facet_wrap(~SiteID)


```

Rolling pdf of raw data to send to Luca and check on status of data QA.


```{r plot of raw data by site-year}
fs_sites <- usfsChug.data.qc %>% distinct(SiteID, year) %>% arrange(SiteID, year)

pdf("data_preparation/USFS Raw Data by Site and Year.pdf", width = 11, height = 8.5)
# Get limits of temp data 
for(i in 1:nrow(fs_sites)) {
  dat <- left_join(fs_sites %>% slice(i), usfsChug.data.qc)
  subtitle <- dat %>% distinct(useSite) %>% pull(useSite)
  xmin <- as.POSIXct(min(dat$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
  xmax <- as.POSIXct(max(dat$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
  p1 <- dat %>% 
    ggplot(aes(x = dt, y = Temperature)) +
    geom_line( data = cd.climDat, aes(x = dt, y = tmin, color = "Air min")) +
    geom_line( data = cd.climDat,aes(x = dt, y = tmax, color = "Air max")) +
    geom_line() +
    scale_x_datetime(limits = c(xmin, xmax), labels = waiver()) +
    scale_y_continuous(limits = c(-5, 30), labels = waiver()) +
    labs(title = fs_sites %>% slice(i) %>% unite(site_year) %>%
           pull(site_year),
         subtitle = paste0("Use Site: ", subtitle)) +
    theme(legend.position = "bottom")
  print(p1)
}

dev.off()

```

# Interactive plot to compare against air temps

site IDs:
"USFS_Bench Creek", "USFS_Blackhole Creek", "USFS_Cabin Lake Outlet", 
"USFS_Center Creek","USFS_Chickaloon Headwaters","USFS_Crescent Creek", 
"USFS_Daves Creek", "USFS_Eagle Creek", "USFS_East Fork 18 Mile",
"USFS_Middle Arm Eyak", "USFS_Hell's Hole Trib", "USFS_Hook Point", 
"USFS_Jackpot River", "USFS_Juneau Creek", "USFS_Koppen Creek",
"USFS_Little Martin River", "USFS_Martin Lake- Inlet", "USFS_Olsen Creek", 
"USFS_Pigot Bay Spawn Channel", "USFS_Power Creek", "USFS_Quartz Creek",
"USFS_Resurrection Creek", "USFS_Rude River SC", "USFS_Salmon Creek",
"USFS_Sheep River", "USFS_Shelter Bay Trib", "USFS_Solf Lake Fish Pass",
"USFS_Stump Lake Outlet", "USFS_25 Mile", "USFS_24.9 Mile Creek", 
"USFS_Ibeck Creek-Low", "USFS_18 Mile Middle Fork", "USFS_18 Mile West Fork",
"USFS_Ibeck Creek-Lower Side Channel", "USFS_18 Mile", "USFS_Eyak Lake Tributary", 
"USFS_ERB Creek", "USFS_Solf Lake Outlet Creek"

Double Check:
USFS_Eagle Creek 2016-07-20 - end of year for possible burial - tidal influence too not sure?
USFS_Middle Arm Eyak - 2015 & 2016 end of summer burials? 2017 end of summer air temps?
USFS_Little Martin - Suspicious but shallow lake upstream
USFS_Pigot Bay Spawn Channel & USFS_Rude River SC & Sheep River - tidal influence seems likely - inclined to remove entire series
USFS_Quartz Creek - 2018 duplicates all year some triplicates around 2018-08-20/22
USFS_Solf Lake Fish pass - looks strange
USFS_Ibeck Creek-Low - 2015, 2016 & 2017 missing days and large daily range 2018-2020 look ok
USFS_18 Mile, USFS_Eyak Lake Tributary, USFS_Solf Lake Outlet Creek - only one full year data
USFS_ERB Creek - partial 2018 and 2020 full season 2019


```{r Interactive Plot With Air Temp}
# get site ids
#dput(unique(usfsChug.data.qc$SiteID))

#Change to bb.data.qc to examine qc'd data
p <- usfsChug.data.qc %>% 
  filter(SiteID == "USFS_ERB Creek", UseData == 1)

xmin <- as.POSIXct(min(p$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
xmax <- as.POSIXct(max(p$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")

p <- p %>% 
  ggplot() +
  geom_line( data = cd.climDat, aes(x = dt, y = tmin, color = "Air min")) +
  geom_line( data = cd.climDat,aes(x = dt, y = tmax, color = "Air max")) +
  geom_line(aes(x = dt, y = Temperature)) +
  scale_x_datetime(limits = c(xmin, xmax), labels = waiver()) +
  coord_cartesian(ylim = c(-5, 30)) + 
  facet_wrap(~SiteID) +
  theme(legend.title = element_blank()) +
  labs(title = "Temperature by Site",
       y = "Temperature degrees C",
       x = "Time of Measurement")

ggplotly(p)

```
# Remove sites

UseSites to record sites for AKSSF analysis

Recalculate UseData based on visual examination

Filter to remove any sites that are not being used for the AKSSF analysis. These could be sites with very incomplete time series or sites that have hydrologic inputs that affect stream temperatures -- e.g. tidally-influenced sites. Note that a list of these sites may be developed at the top of this script so that we don't spend time reviewing data for these sites. That is fine, just note that the sites not used for the AKSSF analysis were NOT reviewed.

Temperature data flagged for UseData = 0 based on visual examination of plots 
and stored in [google sheet](https://docs.google.com/spreadsheets/d/1SijMtKMuU2MB_vx9DCcCXYYeOGqrv6VG-Eidx_pyfkk/edit#gid=0).


```{r Recalculate UseData}
â—˜
```

# Save File for summary report
Save file for summary report.

```{r Save}
usfsChug.data

names(usfsChug.data)

usfsChug.data %>% 
  select(SiteID, useSite, sampleDate, sampleTime, dt, year, Temperature) %>% 
  saveRDS("output/usfsChug.data.rds")
```

# Save QAed dataset for AKTEMP

Save a copy of the final data with the QA flags for AKTEMP as a .csv.



# Save daily data

Save a copy of the daily statistics in the final data folder for the AKSSF analysis. There are two helper functions that add the mode of the time difference for each day and calculate the daily min, mean, and max and removed days with less than 90% of measurements.

* temp_msmt_freq
* daily_screen

```{r}
source("day_and_month_functions.R")

```

