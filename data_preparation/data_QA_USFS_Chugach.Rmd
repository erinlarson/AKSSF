---
title: "data_QA_USFS_Chugach"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, messages = FALSE)
knitr::opts_knit$set(root.dir = normalizePath(".."))
#this sets the root.dir up one level back to the project so that paths are relative to the project directory.

library(readxl)
library(stringr)
library(lubridate)
library(googlesheets4)
library(rnoaa)
library(hms)
library(tidyverse)
library(plotly)
library(DT)
library(beepr)

```

```{r Source Helper Functions}
# Check dir
getwd()
# Source helper functions
source("helper_functions.R")

```

Data QA is done using dynamic plots and reviewing data by site. For AKSSF,
we are only reviewing June - September data and flagging obvious air
temperatures. Burials are a lot harder to confirm without a duplicate logger.


# QA data for air temperatures
## Pull in Airtemp for comparison from NOAA GHCN stations

Go to [GHCN Daily](https://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND)
website and locate stations - try to identify stations with mean/min/max
air temps for period of interest.

Note - have had trouble with lcd function recently

```{r NOAA data, message=FALSE, warning=FALSE}
getwd()

#Token obtained from NOAA to access API
noaaTok <- "LmempjqpgcLSxDQWiLvuaOAGmscrQCrb"

#Station Codes for area of interest
cd.climStat <- c( "USC00502179","USW00026410", "USW00096405")

cd.climDat <- tibble( name = c( "CORDOVA WWTP", "CORDOVA AIRPORT",
"CORDOVA 14"),
                 id = cd.climStat)

# Pull Climate data from Cordova Airport
climDat <- meteo_pull_monitors(cd.climStat)  
str(climDat)

cd.climDat <- cd.climDat %>% 
  left_join( climDat[,c( "id", "date", "tmax", "tmin")], by = 'id') %>% 
  filter( date >= "2008-06-01",
          date <= "2020-12-30",
          name == "CORDOVA AIRPORT") %>% 
  # Temperature and Precipitation values are in tenths of degree/mm
  mutate_if( is.numeric, ~ . * 0.1) %>% 
  mutate(year = as.factor(year(date)),
         day = yday(date),
         dt = as.POSIXct(paste(date), format = "%Y-%m-%d"))

cd.climDat
```


## Pull in Tide data from NOAA

Find station @ [NOAA Tides and Currents](https://tidesandcurrents.noaa.gov/map/index.shtml?id=9454050)
for tide and current records


```{r NOAA Tides}


# Tibble to hold tide data
cd.tideDat <- tibble()
datalist = list()

# Start and end dates
years <- tibble( start = c( 20080101, 20090101, 20100101, 20110101, 20120101,
                            20130101, 20140101, 20150101, 20160101, 20170101,
                            20180101, 20190101, 20200101),
                 stop = c( 20081231, 20091231, 20101231, 20111231, 20121231,
                           20131231, 20141231, 20151231, 20161231, 20171231,
                           20181231, 20191231, 20201231))

# Can only access 1 years worth of measurements at a time
for (row in 1:nrow(years)){
  yr <- gsub('.{4}$', '', paste(years[row,"start"]))
  nam <- paste("cd.tideDat", yr, sep = "")
  startdate <- years[row,"start"]
  stopdate <- years[row, "stop"]
  # Get hourly height data one by year for Cordova station 9454050
  tideDat <- coops_search(station_name = 9454050 , begin_date = startdate,
                             end_date = stopdate, product = "hourly_height",
                             datum = "stnd", units = "metric", time_zone = "gmt")
  datalist[[row]] <- tideDat$data
  
}

cd.tideDat <-  do.call(rbind,datalist)

cd.tideDat <- cd.tideDat %>% 
  rename("dt" = "t", "verified_meters" = "v")

```

# Begin QA

```{r Select Formatted Data Output from data script}

# Choose temperature data ouput formatted for qc from data prep script
usfsChug.data.noqc <- tibble()

filename <- file.choose(new = FALSE)
usfsChug.data <- readRDS(filename)

# Copy and format for qc - limit to usesite and months of interest
usfsChug.data.qc <- usfsChug.data %>% 
  filter(useSite == 1, month(sampleDate) %in% 6:9)

# Select records that will not be QC'd and bind together - Used for AKTEMP export
# Use Sites = 1 outside of period of interest
usfsChug.data.noqc1 <- usfsChug.data %>% 
  filter(useSite == 1, !month(sampleDate) %in% 6:9)
# All UseSite = 0
usfsChug.data.noqc2 <- usfsChug.data %>% 
  filter(useSite == 0)

# Bind together
usfsChug.data.noqc <- rbind(usfsChug.data.noqc1, usfsChug.data.noqc2)

summary(usfsChug.data.qc)

```

??MAKE REVIEW GROUPS - TIDAL/NOT - Time period of interest

## Summary Table
Start with interactive data summary table

```{r Summary Table, message=TRUE, warning=FALSE}

# create data summary table
usfsChug.data.summary <- usfsChug.data.qc %>%
  filter(useSite == 1) %>%
  group_by(SiteID, year) %>%
  summarize(meanTemp = mean(Temperature, na.rm = T),
            maxTemp = max(Temperature, na.rm = T),
            minTemp = min(Temperature, na.rm = T),
            sdTemp = sd(Temperature, na.rm = T),
            n_obs = n())

usfsChug.data.summary %>%
  datatable() %>%
  formatRound(columns=c("meanTemp","maxTemp","minTemp","sdTemp"), digits=2)

```

## Check measurement frequency
DM Notes: Something going on with quartz creek data - have two measurements?? 
Dataset QuartzCreek_Surf_2019-11-13 has duplicate measurements.  Not sure if these
are data from two loggers or if something else happened to them.


```{r Temp measuremnt frequency}
msmt_freq <- temp_msmt_freq(usfsChug.data.qc)

summary(msmt_freq)

# Check mode_diff
msmt_freq[order(msmt_freq$mode_diff, decreasing = FALSE),]


```

# Daily screen
Daily screen also suggests something wrong with USFS_Chickaloon Headwaters
and USFS_24.9 Mile Creek data in addition to the quartz creek issues


```{r Daily Screen}

daily_screen <- daily_screen(msmt_freq)

summary(daily_screen)

```

Quick plots of data to make sure they read in ok. Start by summarizing daily means because quicker to plot. Looks like some bad winter temps well below zero that could be clipped later. No obvious air temps in summer as everything is < = 20 or so.

```{r plot of daily means, message=FALSE, warning=FALSE}

usfsChug.data.qc %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meant = mean(Temperature)) %>% 
  ggplot(aes(x = sampleDate, y = meant)) +
  geom_line() +
  facet_wrap(~SiteID)

```

Wrong! The sub-daily temps show more errors, definitely some air temps that need to be removed. This will be a good dataset for testing scripts, although it will probably need cleaning sooner rather than later for AKSSF.

```{r plot of raw data}

usfsChug.data.qc %>% 
  ggplot(aes(x = dt, y = Temperature)) +
  #geom_line( data = cd.climDat, aes(x = dt, y = tmin, color = "Air min")) +
  #geom_line( data = cd.climDat,aes(x = dt, y = tmax, color = "Air max")) +
  geom_line() +
  facet_wrap(~SiteID)


```

Rolling pdf of raw data to send to Luca and check on status of data QA.


```{r plot of raw data by site-year}
fs_sites <- usfsChug.data.qc %>% distinct(SiteID, year) %>% arrange(SiteID, year)

pdf("data_preparation/USFS Raw Data by Site and Year.pdf", width = 11, height = 8.5)
# Get limits of temp data 
for(i in 1:nrow(fs_sites)) {
  dat <- left_join(fs_sites %>% slice(i), usfsChug.data.qc)
  subtitle <- dat %>% distinct(useSite) %>% pull(useSite)
  xmin <- as.POSIXct(min(dat$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
  xmax <- as.POSIXct(max(dat$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
  p1 <- dat %>% 
    ggplot(aes(x = dt, y = Temperature)) +
    geom_line( data = cd.climDat, aes(x = dt, y = tmin, color = "Air min")) +
    geom_line( data = cd.climDat,aes(x = dt, y = tmax, color = "Air max")) +
    geom_line() +
    scale_x_datetime(limits = c(xmin, xmax), labels = waiver()) +
    scale_y_continuous(limits = c(-5, 30), labels = waiver()) +
    labs(title = fs_sites %>% slice(i) %>% unite(site_year) %>%
           pull(site_year),
         subtitle = paste0("Use Site: ", subtitle)) +
    theme(legend.position = "bottom")
  print(p1)
}

dev.off()

```

# Interactive plot to compare against air temps

site IDs:
"USFS_Bench Creek", "USFS_Blackhole Creek", "USFS_Cabin Lake Outlet", 
"USFS_Center Creek","USFS_Chickaloon Headwaters","USFS_Crescent Creek", 
"USFS_Daves Creek", "USFS_Eagle Creek", "USFS_East Fork 18 Mile",
"USFS_Middle Arm Eyak", "USFS_Hell's Hole Trib", "USFS_Hook Point", 
"USFS_Jackpot River", "USFS_Juneau Creek", "USFS_Koppen Creek",
"USFS_Little Martin River", "USFS_Martin Lake- Inlet", "USFS_Olsen Creek", 
"USFS_Pigot Bay Spawn Channel", "USFS_Power Creek", "USFS_Quartz Creek",
"USFS_Resurrection Creek", "USFS_Rude River SC", "USFS_Salmon Creek",
"USFS_Sheep River", "USFS_Shelter Bay Trib", "USFS_Solf Lake Fish Pass",
"USFS_Stump Lake Outlet", "USFS_25 Mile", "USFS_24.9 Mile Creek", 
"USFS_Ibeck Creek-Low", "USFS_18 Mile Middle Fork", "USFS_18 Mile West Fork",
"USFS_Ibeck Creek-Lower Side Channel", "USFS_18 Mile", "USFS_Eyak Lake Tributary", 
"USFS_ERB Creek", "USFS_Solf Lake Outlet Creek"

Double Check:

~~USFS_Middle Arm Eyak - noted a possible burial in google sheet but leaving UseData = 1 for now~~
~~USFS_Little Martin River - Notes say shallow lake upstream, may be reason for strange patterns~~
~~USFS_Quartz Creek - 2018 duplicates all year some triplicates around 2018-08-20/22~~
~~USFS_Solf Lake Fish Pass - looks strange~~
~~USFS_18 Mile, USFS_Eyak Lake Tributary, USFS_Solf Lake Outlet Creek - only one full year data~~
~~USFS_ERB Creek - partial 2018 and 2020 full season 2019~~

```{r Interactive Plot With Air Temp and Tide}
# get site ids
#dput(unique(usfsChug.data.qc$SiteID))

#Change to bb.data.qc to examine qc'd data
p <- usfsChug.data.qc %>% 
  filter(SiteID == "USFS_Rude River SC")

xmin <- as.POSIXct(min(p$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
xmax <- as.POSIXct(max(p$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")

p <- p %>% 
  ggplot() +
  geom_line( data = cd.climDat, aes(x = dt, y = tmin, color = "Air min")) +
  geom_line( data = cd.climDat,aes(x = dt, y = tmax, color = "Air max")) +
  geom_line( data = cd.tideDat,aes(x = dt, y = verified_meters, color = "Tide (m)")) +
  geom_line(aes(x = dt, y = Temperature)) +
  scale_x_datetime(limits = c(xmin, xmax), labels = waiver()) +
  coord_cartesian(ylim = c(-5, 30)) + 
  facet_wrap(~SiteID) +
  theme(legend.title = element_blank()) +
  labs(title = "Temperature by Site",
       y = "Temperature degrees C",
       x = "Time of Measurement")

ggplotly(p)

```
# Remove sites

UseSites to record sites for AKSSF analysis

Recalculate UseData based on visual examination

Filter to remove any sites that are not being used for the AKSSF analysis. These could be sites with very incomplete time series or sites that have hydrologic inputs that affect stream temperatures -- e.g. tidally-influenced sites. Note that a list of these sites may be developed at the top of this script so that we don't spend time reviewing data for these sites. That is fine, just note that the sites not used for the AKSSF analysis were NOT reviewed.

Temperature data flagged for UseData = 0 based on visual examination of plots 
and stored in [google sheet](https://docs.google.com/spreadsheets/d/1SijMtKMuU2MB_vx9DCcCXYYeOGqrv6VG-Eidx_pyfkk/edit#gid=0).


```{r Pull In Googlesheet Flags, message=FALSE, warning=FALSE}

#Must activate a token to read in this sheet - need to investigate why read only
#access is not working
gs4_auth()

#gs4_user()

temp_log_db_gs = "https://docs.google.com/spreadsheets/d/1SijMtKMuU2MB_vx9DCcCXYYeOGqrv6VG-Eidx_pyfkk/edit#gid=0"

#Read in flag data sheet
flagDb <- read_sheet(temp_log_db_gs,sheet = "AKSSF_Data_Flags",
                    col_names = TRUE,
                    col_types = "c")

#create cols variable 
cols <- c("SiteID", "FlagStart", "FlagEnd","Days", "FlagReason", "UseSite", "UseData", "Notes")

# Transform and drop unecessary columns
flagDb <- flagDb %>%
  select(all_of(cols)) %>% 
  transform(SiteID = as.character(SiteID),
            FlagStart = as_date(ymd(FlagStart)),
            FlagEnd= as_date(ymd(FlagEnd)),
            Days = as.numeric(Days),
            FlagReason = as.character(FlagReason),
            UseSite = as.numeric(UseSite),
            UseData = as.numeric(UseData),
            Notes = as.character(Notes))

#convert date range to dates 
flagDb2 <- flagDb %>% 
  mutate(flagDate = map2(FlagStart, FlagEnd, ~seq(from = .x, to = .y,
                                                  by = "day"))) %>% 
  unnest() %>%
  select(SiteID, flagDate, FlagReason) %>%
  distinct()

str(flagDb2)
```
```{r Recalculate UseData}
#Tibble for final qc data
usfsChug.data.finalqc <- tibble()

#Recalculate UseData values using information stored in flagDb tibble

usfsChug.data.finalqc <- usfsChug.data.qc %>% 
  left_join( flagDb2, by = c("SiteID" = "SiteID", "sampleDate" = "flagDate")) %>% 
  mutate(UseData = case_when(FlagReason == "Air Temperature" ~ 0,
                             FlagReason == "Burial" ~ 0,
                             FlagReason == "Logger Failure" ~ 0,
                             FlagReason == "Other" ~ 0,
                             FlagReason == "Tidal Influence" ~ 0,
                             TRUE ~ UseData))
  

usfsChug.data.finalqc %>% count(SiteID, year, UseData, FlagReason)


```
# Save Data
## Save reviewed data with additional UseData flags

```{r Save QA }
acronym = "usfsChug"
# Save copy of qa data with data flags
usfsChug.data.finalqc %>% 
  select(SiteID, Agency_ID , sampleDate, sampleTime, dt, year, Temperature, UseData, FlagReason) %>% 
  saveRDS("formatted_data/usfsChug.data.finalqc.rds")

```

## Save QAed dataset for AKTEMP
AKTEMP Data file

* SiteID, character
* sampleDate, date
* sampleTime, hms
* Temperature, numeric
* useData, numeric

```{r Save For AKTEMP}
# Tibble for AKTEMP data copy of all data - QC'd Period of interest and data excluded from qc process
AKTEMP_Data <- bind_rows(usfsChug.data.noqc, usfsChug.data.finalqc)

# save copy of data formatted for AKTEMP
save_aktemp_files(AKTEMP_Data, acronym = acronym )

```

## Save daily data

Save a copy of the daily statistics in the final data folder for the AKSSF analysis. There are two helper functions that add the mode of the time difference for each day and calculate the daily min, mean, and max and removed days with less than 90% of measurements.

* temp_msmt_freq
* daily_screen

```{r Save Daily}
# Calculate temp measurement frequency and daily summaries for qc'd data only for Usedata = 1
AKSSF_Data <- usfsChug.data.finalqc %>% 
  filter(UseData == 1)

daily_data <- temp_msmt_freq(AKSSF_Data) %>% 
  daily_screen()

daily_data

# Save Daily Summaries
save_daily_files(daily_data, acronym = acronym )

```

