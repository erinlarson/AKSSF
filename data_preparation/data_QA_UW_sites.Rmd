---
title: "data_QA_UW_sites"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, messages = FALSE)
knitr::opts_knit$set(root.dir = normalizePath("..")) #this sets the root.dir up one level back to the project so that paths are relative to the project directory.

library(readxl)
library(stringr)
library(lubridate)
library(googlesheets4)
library(googledrive)
library(rnoaa)
library(hms)
library(tidyverse)
library(plotly)
library(DT)
library(rnaturalearth)
library(rnaturalearthdata)
library(ggrepel)
library(tmap)
library(tmaptools)
library(sf)
library(mapview)

```

```{r Functions}
#setwd("..")
getwd()
source("helper_functions.R")
```

Data QA is done using dynamic plots and reviewing data by site. For AKSSF, we are only reviewing June - September data and flagging obvious air temperatures. Burials are a lot harder to confirm without a duplicate logger.

# QA data for air temperatures
## Pull in Airtemp for comparison from NOAA GHCN stations
Kilbuck station seems like a good fit for these data

Go to [GHCN Daily](https://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND)
website and locate stations - try to identify stations with mean/min/max
air temps for period of interest.

Note - have had trouble with lcd function recently

```{r NOAA data, message=FALSE, warning=FALSE}
getwd()

#Token obtained from NOAA to access API
noaaTok <- "LmempjqpgcLSxDQWiLvuaOAGmscrQCrb"

#Station Codes for area of interest
uw.climStat <- "USW00026562"

uw.climDat <- tibble( name = "Iliamna Airport",
                 id = uw.climStat)

# Pull Climate data from Kilbuck Station
climDat <- meteo_pull_monitors(uw.climStat)  
str(climDat)

# Temp records from Iliamna Airport
uw.climDat1 <- uw.climDat %>% 
  left_join( climDat[,c( "id", "date", "tmax", "tmin")], by = 'id') %>% 
  filter( date >= "2017-05-01",
          date <= "2020-12-30",
          name == "Iliamna Airport") %>% 
  # Temperature and Precipitation values are in tenths of degree/mm
  mutate_if( is.numeric, ~ . * 0.1) %>% 
  mutate(year = as.factor(year(date)),
         day = yday(date),
         DT = as.POSIXct(paste(date), format = "%Y-%m-%d"))

```

## Load Data
Pull in formatted data and make a copy for QA 
Becky has already formatted these data for qa purposes - Data file = uw.post16.tcsites.rds

```{r Load Dataset for QA}
# Choose temperature data ouput formatted for qc from data prep script
filename <- file.choose(new = FALSE)
UWsites.data <- readRDS(filename)

# If needed, add the agency prefix to data in the SiteID column.
acronym <- "UW_"
UWsites.data <- mutate(UWsites.data, SiteID = paste0(acronym, SiteID))

# Copy and format for qc - limit to use site and months of interest
UWsites.data.qc <- UWsites.data %>% 
  filter(month(sampleDate) %in% 6:9)
  
summary(UWsites.data.qc)

```

## Map of sites
Map of site locations

```{r Map of sites}

# Choose md file to plot sites (RS uploaded draft md file to googledrive)
filename2 <- file.choose(new = FALSE)
UWsites.md.data <- read_csv(filename2) 

sites <- UWsites.md.data %>% 
  select(SiteID, Latitude, Longitude) %>% 
  st_as_sf(coords = c("Longitude", "Latitude"),
           crs= "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0") %>% 
  st_cast("POINT")

tmap_mode("view")
sitemap <- sites %>% 
  tm_shape() +
  tm_dots(id = "SiteID", size = 0.05) +
  tm_text("SiteID", size = 1.5, shadow = TRUE, auto.placement = TRUE,
          just = "bottom", remove.overlap = TRUE, clustering = TRUE )#+
  #tm_basemap(server = "Esri.WorldTopoMap")

sitemap

```

## Rolling PDF 
Pdf containing plots of raw water temp data and daily Air temperatures for each site by year.

```{r Print PDF of Raw Data}

UW_sites <- UWsites.data.qc %>% distinct(SiteID, year) %>% arrange(SiteID, year)

pdf("data_preparation/UW 16 Raw Data with Iliamna Air Temp.pdf", width = 11, height = 8.5)
# Get limits of temp data 
for(i in 1:nrow(UW_sites)) {
  dat <- left_join(UW_sites %>% slice(i), UWsites.data.qc)
  #subtitle <- dat %>% distinct(UseSite) %>% pull(UseSite)
  xmin <- as.POSIXct(min(dat$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
  xmax <- as.POSIXct(max(dat$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
  p1 <- dat %>% 
    ggplot(aes(x = dt, y = Temperature)) +
    geom_line( data = uw.climDat1, aes(x = DT, y = tmin, color = "Air min")) +
    geom_line( data = uw.climDat1, aes(x = DT, y = tmax, color = "Air max")) +
    geom_line() +
    scale_x_datetime(limits = c(xmin, xmax), labels = waiver()) +
    scale_y_continuous(limits = c(-5, 30), labels = waiver()) +
    labs(title = UW_sites %>% slice(i) %>% unite(site_year) %>%
           pull(site_year))#,
         #subtitle = paste0("Use Site: ", subtitle)) +
    theme(legend.position = "bottom")
  print(p1)
}

dev.off()

```

## Daily plots

```{r Plot of daily means, message=FALSE, warning=FALSE}

UWsites.data.qc %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meant = mean(Temperature)) %>% 
  ggplot(aes(x = sampleDate, y = meant)) +
  geom_line() +
  facet_wrap(~SiteID)

```

```{r plot of raw data}

UWsites.data.qc %>% 
  ggplot(aes(x = dt, y = Temperature)) +
  #geom_line( data = uw.climDat, aes(x = dt, y = tmin, color = "Air min")) +
  #geom_line( data = uw.climDat,aes(x = dt, y = tmax, color = "Air max")) +
  geom_line() +
  facet_wrap(~SiteID)

```

## Interactive Plot

Walk through list of Sites and note on google sheet any anomalous measurements and leave note for DM to check

Review Sites with interactive plot and note Airtemps/Burials etc on data flag worksheet stored here [google sheet](https://docs.google.com/spreadsheets/d/1Iex7gTFejRkJHZf2BWYQtsjEx7HWtPfUiLQQ1qz3AfY/edit#gid=466638279).

Most sites also visible in [online mapper](https://accsmaps.maps.arcgis.com/home/webmap/viewer.html?webmap=364d7ce98dd64b469fce23b06751f989)  - look/filter UW sites

Sites:
"UW_Aleknagik Bear Creek"

"UW_Aleknagik Big Whitefish Creek"

"UW_Aleknagik Eagle Creek"

"UW_Aleknagik Hansen Creek"
**DM Notes - 2020-7-30 - 2020-8-31 Looks like burial**

"UW_Aleknagik Happy Creek"

"UW_Aleknagik Ice Creek", "UW_Aleknagik Mission Creek", "UW_Aleknagik Pfifer Creek", "UW_Aleknagik Silver Salmon Creek", "UW_Aleknagik Squaw Creek", "UW_Aleknagik Yako Creek", "UW_Beverley Hope Creek", "UW_Beverley Moose Creek", "UW_Beverley Uno Creek", "UW_Kulik Grant River", "UW_Kulik Kâˆ’3 Creek", "UW_Kulik Kulik Creek", "UW_Little Togiak C Creek", "UW_Little Togiak Creek", "UW_Nerka Allah Creek", "UW_Nerka Bear Creek", "UW_Nerka Berm Creek", "UW_Nerka Cabin Creek", "UW_Nerka Chamee Creek", "UW_Nerka Cottonwood Creek", "UW_Nerka Elva Creek", "UW_Nerka Fenno Creek", "UW_Nerka Hidden Lake Creek", "UW_Nerka Joe Creek", "UW_Nerka Kema Creek", "UW_Nerka Lynx Creek", "UW_Nerka Lynx Creek Cold Tributary", "UW_Nerka N4 Creek", "UW_Nerka Pick Creek", "UW_Nerka Rainbow Creek", "UW_Nerka Sam Creek", "UW_Nerka Seventh Creek", "UW_Nerka Stovall Creek", "UW_Nerka Teal Creek"

```{r Interactive Plot With Air Temp}
# get site ids
#dput(unique(UWsites.data.qc$SiteID))

#Change to bb.data.qc to examine qc'd data
p <- UWsites.data.qc %>% 
  filter(SiteID == "UW_Nerka Stovall Creek", UseData == 1) # Filtered by site and use data

xmin <- as.POSIXct(min(p$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
xmax <- as.POSIXct(max(p$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")

p <- p %>% 
  ggplot() +
  geom_line(data = uw.climDat1, aes(x = DT, y = tmin, color = "Air min")) +
  geom_line(data = uw.climDat1, aes(x = DT, y = tmax, color = "Air max")) +
  geom_line(aes(x = dt, y = Temperature)) +
  scale_x_datetime(limits = c(xmin, xmax), labels = waiver()) +
  coord_cartesian(ylim = c(-5, 30)) + 
  facet_wrap(~SiteID) +
  theme(legend.title = element_blank()) +
  labs(title = "Temperature by Site",
       y = "Temperature degrees C",
       x = "Time of Measurement")
ggplotly(p)
```

# Remove sites

Recalculate UseData based on visual examination

Filter to remove any sites that are not being used for the AKSSF analysis. These could be sites with very incomplete time series or sites that have hydrologic inputs that affect stream temperatures -- e.g. tidally-influenced sites. Note that a list of these sites may be developed at the top of this script so that we don't spend time reviewing data for these sites. That is fine, just note that the sites not used for the AKSSF analysis were NOT reviewed.

Temperature data flagged for UseData = 0 based on visual examination of plots 
and stored in [google sheet](https://docs.google.com/spreadsheets/d/1usYGIlMMQzTqzIqx0r872xo7_aIYth0P_z47YgQoDEc/edit#gid=0).


```{r Pull In Googlesheet Flags, message=FALSE, warning=FALSE}

#Must activate a token to read in this sheet. This is done by running the chunk and following the prompts that populate in the Console area below. You'll want to enter '1' for Yes to give authorization to read in the sheet. It should then open up a Google page in the web browser and if you follow the prompts, it will provide an authorization code to paste/enter into the Console.

gs4_auth() #select 1 (yes, to give authorization)
#gs4_user()
temp_log_db_gs = "https://docs.google.com/spreadsheets/d/1Iex7gTFejRkJHZf2BWYQtsjEx7HWtPfUiLQQ1qz3AfY/edit#gid=0"

#Read in flag data sheet
flagDb <- read_sheet(temp_log_db_gs,sheet = "AKSSF_Data_Flags",
                    col_names = TRUE,
                    col_types = "c")

#create cols variable 
cols <- c("SiteID", "FlagStart", "FlagEnd","Days", "FlagReason", "UseSite", "UseData", "Notes")

# Transform and drop unnecessary columns
flagDb <- flagDb %>%
  select(all_of(cols)) %>% 
  transform(SiteID = as.character(SiteID),
            FlagStart = as_date(ymd(FlagStart)),
            FlagEnd= as_date(ymd(FlagEnd)),
            Days = as.numeric(Days),
            FlagReason = as.character(FlagReason),
            UseSite = as.numeric(UseSite),
            UseData = as.numeric(UseData),
            Notes = as.character(Notes))
           
#convert date range to dates 
flagDb2 <- flagDb %>% 
  mutate(flagDate = map2(FlagStart, FlagEnd, ~seq(from = .x, to = .y,
                                                  by = "day"))) %>% 
  unnest() %>%
  select(SiteID, flagDate, FlagReason) %>%
  distinct()

str(flagDb2)
```
```{r Recalculate UseData}
#Tibble for final qc data
UWsites.data.finalqc <- tibble()

#Recalculate UseData values using information stored in flagDb tibble

UWsites.data.finalqc <- UWsites.data.qc %>% 
  left_join( flagDb2, by = c("SiteID" = "SiteID", "sampleDate" = "flagDate")) %>% 
  mutate(UseData = case_when(FlagReason == "Air Temperature" ~ 0,
                             FlagReason == "Burial" ~ 0,
                             FlagReason == "Logger Failure" ~ 0,
                             FlagReason == "Other" ~ 0,
                             FlagReason == "Tidal Influence" ~ 0,
                             TRUE ~ UseData),
         year=year(sampleDate))
  

UWsites.data.finalqc %>% count(SiteID, year, UseData, FlagReason)

summary(UWsites.data.finalqc)

summary(UWsites.data.qc)
```


# Compare raw data (UWsites.data.qc) to QC'd data (UWsites.data.finalqc)
Use rolling pdf code to print out pdf of QC'd data by site/year and compare side by side with original pdf of these data.

```{r plot of recalculated data by site-year}

#Filters out recalculated UseData from google sheet
UWsites.data.finalqc <- UWsites.data.finalqc %>% 
  filter(UseData == 1)

uw_sites <- UWsites.data.finalqc %>% distinct(SiteID, year) %>% arrange(SiteID, year)

pdf("data_preparation/UW 16 QC Data by Site and Year.pdf", width = 11, height = 8.5)
# Get limits of temp data 
for(i in 1:nrow(uw_sites)) {
  dat <- left_join(uw_sites %>% slice(i), UWsites.data.finalqc)
  #subtitle <- dat %>% distinct(UseSite) %>% pull(UseSite)
  xmin <- as.POSIXct(min(dat$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
  xmax <- as.POSIXct(max(dat$dt),format = "%Y-%m-%d %H:%M", tz = "GMT")
  p1 <- dat %>% 
    ggplot(aes(x = dt, y = Temperature)) +
    geom_line( data = uw.climDat, aes(x = DT, y = tmin, color = "Air min")) +
    geom_line( data = uw.climDat, aes(x = DT, y = tmax, color = "Air max")) +
    geom_line() +
    scale_x_datetime(limits = c(xmin, xmax), labels = waiver()) +
    scale_y_continuous(limits = c(-5, 30), labels = waiver()) +
    labs(title = uw_sites %>% slice(i) %>% unite(site_year) %>%
           pull(site_year))#,
         #subtitle = paste0("Use Site: ", subtitle)) +
    theme(legend.position = "bottom")
  print(p1)
}

dev.off()

getwd()

```

# Check measurement frequency

```{r Temperature Measuremnt Frequency}
msmt_freq <- temp_msmt_freq(UWsites.data.finalqc)

summary(msmt_freq)

# Check mode_diff
msmt_freq[order(msmt_freq$mode_diff, decreasing = FALSE),]
```

# Daily screen

```{r Daily Screen}
daily_screen <- daily_screen(msmt_freq)

summary(daily_screen)
```

# Save daily data

Save a copy of the daily statistics in the final data folder for the AKSSF analysis. There are two helper functions that add the mode of the time difference for each day and calculate the daily min, mean, and max and removed days with less than 90% of measurements.

* temp_msmt_freq
* daily_screen

```{r Save Daily}
# Calculate temp measurement frequency and daily summaries for qc'd data only for Usedata = 1
acronym = "UWsites_"

# Save Daily Summaries
save_daily_files(daily_screen, acronym = acronym)
getwd()
```

