---
title: "data_TEMPLATE"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, messages = FALSE)
knitr::opts_knit$set(root.dir = normalizePath("..")) #this sets the root.dir up one level back to the project so that paths are relative to the project directory.

library(readxl)
library(stringr)
library(lubridate)
library(googlesheets4)
library(rnoaa)
library(hms)
library(tidyverse)
```


Notes to Dustin: Take a look at the yaml and the setup chunk too. I think all of this could be fixed across the different datasets and it would make it go quicker.

This file is for reading in data from a single provider. I usually read in files separately by folder depending on how the data were provided and formatted. This might not be needed. The final saved data frame can be .rds for importing into the data QA report. If data have been QAed, then the final data file, metadata file, and daily data can be saved as .csv. Add a UseData == 1 for data that have already been reviewed.

# Define Functions
Define any functions that are used for data formatting or duplicate measurement identification 


```{r Functions}


```

# Read in data and format

## Metadata

Sites file. This is a longer list of names from akoats that we should probably keep. Some basic ones should be filled in for providers with data not entered into akoats.

Notes on new names:

* Agency_ID should equal the SiteID in the data table. So during left_join, use by = c("SiteID" = "Agency_ID"). If this is a problem because a data provided used stream names, concatenate the agency acronym to the Agency_ID. E.g. USFS_Cold Creek.
* AKOATS_ID = seq_id.


```{r}


```



## Data

# Review data

## Duplicate measurements

## Save data 

If data have been reviewed by the data provider, these should be .csv of the final data, metadata, and daily data (see data_QA_TEMPLATE). Otherwise, an .rds or .csv to be passed to the data QA script.
