---
title: "data_TEMPLATE"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, messages = FALSE)
knitr::opts_knit$set(root.dir = normalizePath("..")) #this sets the root.dir up one level back to the project so that paths are relative to the project directory.

library(readxl)
library(stringr)
library(lubridate)
library(googlesheets4)
library(rnoaa)
library(hms)
library(tidyverse)
```


Notes to Dustin: Take a look at the yaml and the setup chunk too. I think all of this could be fixed across the different datasets and it would make it go quicker.

This file is for reading in data from a single provider. I usually read in files separately by folder depending on how the data were provided and formatted. This might not be needed. The final saved data frame can be .rds for importing into the data QA report. If data have been QAed, then the final data file, metadata file, and daily data can be saved as .csv. Add a UseData == 1 for data that have already been reviewed.

# Define Functions
Define any functions that are used for data formatting or duplicate measurement identification



```{r Functions}
# I had to set working directory up one level evan after knitr::opts_knit$set(root.dir = normalizePath(".."))  in order to source the helper function 
# setwd('..')
# Source functions in helper function script to ensure proper format of saved data
source("helper_functions.R")

```

# Read in data and format

## Metadata

Sites file. This is a longer list of names from akoats that we should probably keep. Some basic ones should be filled in for providers with data not entered into akoats.

Notes on new names:

* Agency_ID should equal the SiteID in the data table. So during left_join, use by = c("SiteID" = "Agency_ID"). If this is a problem because a data provided used stream names, concatenate the agency acronym to the Agency_ID. E.g. USFS_Cold Creek.
* AKOATS_ID = seq_id.


```{r}


```



## Data

# Review data


## Duplicate measurements

?? Sufficient to just use distinct() on site/temp/dt or more robust method?

## Save data 
Save copies of metadata file and temp data formatted for QA to local drive
Save copies of metadata file and temp data formatted for QA to google drive

```{r Save Outputs}
# Save copy of formatted data for qc (UseSite vs UseData?)
# Save metadata - this will save a local copy and upload to google drive
save_metadata_files(data.in, acronym = "NAME OF OUTPUT")


```


If data have been reviewed by the data provider, these should be .csv of the final data, metadata, and daily data (see data_QA_TEMPLATE). Otherwise, an .rds or .csv to be passed to the data QA script.
