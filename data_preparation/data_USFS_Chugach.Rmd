---
title: "data_USFS_Chugach"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath(".."))


library(tidyverse)
library(readxl)
library(stringr)
library(lubridate)
library(googlesheets4)
library(rnoaa)


```


# Get air temperatures from GHCN sites using rnoaa

Not working, I can't find a site with daily summaries in cordova. The airport is part of the local climatological data online - WBAN:26410, but it's not working with the lcd function.

```{r NOAA data}

#Token obtained from NOAA to access API
noaaTok <- "LmempjqpgcLSxDQWiLvuaOAGmscrQCrb"

lcd(station = "26410", year = 2013)

ncdc_locs(locationcategoryid = "CITY")

#Station Codes for area of interest
bb.climStat <- c( "USW00026562","USW00025506", "USR0000ASNI", "USW00025503")


bb.climDat <- tibble( name = c( "Iliamna Airport", "Port Alsworth 1",
"Snipe Lake","King Salmon Airport"),
                 id = bb.climStat)

#Pull Climate data from Bristol Bay 
climDat <- meteo_pull_monitors(bb.climStat)  
str(climDat)
  

bb.climDat <- bb.climDat %>% 
  left_join( climDat[,c( "id", "date", "prcp", "tmax", "tmin", "tavg")], by = 'id') %>% 
  filter( date >= "2013-06-01",
          date <= "2019-10-30",
          name == "Port Alsworth 1") %>% 
  # Temperature and Precipitation values are in tenths of degree/mm
  mutate_if( is.numeric, ~ . * 0.1) %>% 
  mutate(year = as.factor(year(date)),
         day = yday(date),
         dt = as.POSIXct(paste(date), format = "%Y-%m-%d"))
  

bb.climDat

```


# USFS

## Data

Export his site names for Priscilla or use his metadata workbook -- it's a little confusing. Check if it matches the actual data files.

```{r file names}
fs_files <- list.files("S:\\Stream Temperature Data\\Luca Adelfio\\ChugachNF_wtdat", full.names = TRUE)

#remove metadata sheet
fs_files <- fs_files[!grepl("siteinfo", fs_files)]

#remove air temperature data sheets
fs_files <- fs_files[!grepl("AirT", fs_files)]

#save names so Priscilla can look up AKOATS ID
fs_site_names <- sapply(fs_files, function(x) str_match(x, "\\/\\s*(.*?)\\s*\\.csv")[2], simplify = TRUE)
# as.data.frame(fs_site_names) %>% write_csv(path = "output/usfs_site_names.csv")

```

```{r read in data from csv}

read_csv_and_name <- function(csv_file_path) {
  sheet_name <- str_match(csv_file_path, "\\/\\s*(.*?)\\s*\\.csv")[2]
  dat <- read_csv(csv_file_path) %>% 
      mutate(file_name = sheet_name)
}

fsdat <- fs_files %>% 
  map_df(function(x) read_csv_and_name(x)) 

```

Some problems with dates - some are 2 digit and some are 4 digit. In as.Date function, tryFormats won't fix this problem, which is unfortunate. I'll need to get number of digits using regexp and then manually tell it what format it is in.

```{r fix dates}
fsdat %>% mutate(year1 = sub(".*/.*/", "", Date)) %>% distinct(year1)

fsdat <- fsdat %>% 
  mutate(year1 = sub(".*/.*/", "", Date),
         sampleDate = case_when(nchar(year1) == 4 ~ as.Date(Date, format = "%m/%d/%Y"),
                                TRUE ~ as.Date(Date, format = "%m/%d/%y"))) %>% 
  rename(sampleTime = Time,
         Temperature = Temp)

fsdat %>% 
  group_by(file_name) %>% 
  summarize(min(sampleDate), max(sampleDate))

fsdat %>% 
  distinct(year(sampleDate))

```

Priscilla was able to link most of these file names to site names in AKOATS. For the four sites not in AKOATS, she converted the UTM 
coordinates to latitude/longitude. Read in that google sheet so that we can get simple site names on the data sheet and also start a metadata file with the locations.

```{r read in metadata}

depracated_metadata <- "https://docs.google.com/spreadsheets/d/1ymyhRyAomnJZTZqr4IbRmCnJjVaOIpCQYGscJDbCk-w/edit#gid=0"

usfs_md <- read_sheet(depracated_metadata, sheet = "Sheet1",
                    col_names = TRUE,
                    col_types = "c") %>% 
  filter(ACCS_Project == "USFS_Chugach")


usfs_md

```

Additional fields to add to data frame:

* Add SiteIDs to the data. Unfortunately, Luca is using waterbody names for site ids, which can complicate having unique station names down the line - e.g. additional sites on the same stream or another agency monitoring the same stream.
* In Luca's siteinfo worksheet, he mentions some sites that may not be useful for our thermal sensitivity analysis. Flag those here.
* Add date-time and year for plotting.


```{r add new fields}
fsdat <- left_join(fsdat, usfs_md %>% select(data_SiteID, Agency_ID), by = c("file_name" = "data_SiteID")) %>% 
  mutate(useSite = case_when(Agency_ID %in% c("Clear Creek", "Hatchery Creek", "Ibeck Creek", "Jack Bay River",
                                           "McKinley Lake", "NF Williwaw Creek", "Pigot Bay Spawn Channel", "Rude River SC",
                                           "SF Williwaw Creek", "Solf Lake Inlet", "Steller Jay Creek") ~ 0,
                             TRUE ~ 1),
         dt = as.POSIXct(paste(sampleDate, sampleTime, sep = " "), format = "%Y-%m-%d %H:%M"),
         year = year(sampleDate)) 

```


Quick plots of data to make sure they read in ok. Start by summarizing daily means because quicker to plot. Looks like some bad winter temps well below zero that could be clipped later. No obvious air temps in summer as everything is < = 20 or so.

```{r plot of dialy means}

fsdat %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(meant = mean(Temperature)) %>% 
  ggplot(aes(x = sampleDate, y = meant)) +
  geom_line() +
  facet_wrap(~SiteID)

```

Wrong! The sub-daily temps show more errors, definitely some air temps that need to be removed. This will be a good dataset for testing scripts, although it will probably need cleaning sooner rather than later for AKSSF.

```{r plot of raw data}

fsdat %>% 
  ggplot(aes(x = dt, y = Temperature)) +
  geom_line() +
  facet_wrap(~SiteID)


```

Rolling pdf of raw data to send to Luca and check on status of data QA.

```{r plot of raw data by site-year}
fs_sites <- fsdat %>% distinct(Agency_ID, year) %>% arrange(Agency_ID, year)

pdf("data_preparation/USFS Raw Data by Site and Year.pdf", width = 11, height = 8.5)

for(i in 1:nrow(fs_sites)) {
  dat <- left_join(fs_sites %>% slice(i), fsdat)
  subtitle <- dat %>% distinct(useSite) %>% pull(useSite)
  p1 <- dat %>% 
    ggplot(aes(x = dt, y = Temperature)) +
    geom_line() +
    labs(title = fs_sites %>% slice(i) %>% unite(site_year) %>%
           pull(site_year),
         subtitle = paste0("Use Site: ", subtitle)) +
    theme(legend.position = "bottom")
  print(p1)
}

dev.off()

```



