---
title: "data_USFS_Chugach"
output: html_document
---

The <knitr::opts_knit$set(root.dir = normalizePath(".."))> only works when run as a chunk and resets to the data prep folder for all following chunks?  Not sure why this is happening.
1

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = setwd(normalizePath("..")))
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, messages = FALSE)

library(tidyverse)
library(readxl)
library(stringr)
library(lubridate)
library(googlesheets4)
library(rnoaa)
library(hms)
library(googledrive) 
source("helper_functions.R")

```


# Review Data
## Metadata

DM - I updated the depracated metadata sheet to correct a few sites that were in AKOATS and add new sites from Luca that were not in AKOATS. 

```{r Metadata not in AKOATS}
getwd()
# Created a spreadsheet to link AKOATS IDs and Site names/Coords using data filename----
depracated_metadata <- ".\\formatted_data\\USFS_Chugach_Working_deprecated_md.xlsx"

usfs_dpmd <- read_excel(depracated_metadata, sheet = "deprecated_metadata",
                    col_names = TRUE) %>% 
  filter(ACCS_Project == "USFS_Chugach") %>% 
  mutate(AKOATS_ID = as.numeric(AKOATS_ID),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude)) 

usfs_dpmd <- usfs_dpmd %>%
  select(Agency_ID, AKOATS_ID, Latitude, Longitude, data_SiteID)  
  

```

DM Notes: Use GOOGLE SHEET Copy for consistency

```{r AKOATS MD}

# Find and copy AKOATS working excel file in Google Drive
# x <- drive_find(pattern = "<NAME OF EXCEL FILE STORED IN GOOGLE DRIVE>", type = "xlsx", n_max = 1)
# akoats_cont <- drive_download(x, overwrite = TRUE)
# akoats_path <- akoats_cont$local_path
# akoats_fsch <- read_excel( akoats_path, sheet = "CONTINUOUS_DATA",
#                         col_names = TRUE
#                         )  %>%

# Access Google Sheet copy of AKOATS working sheet and use AKOATS Complete sheet

akoats_2020wk <- "https://docs.google.com/spreadsheets/d/1SPZXNGm_Tc39-GuJXY8j7Eb1lX6DwXTQ1en2_LvCI1I/edit#gid=1281874712"

akoats_fsch <- read_sheet(akoats_2020wk, sheet = "AKOATS_COMPLETE",
                    col_names = TRUE,
                    col_types = "c") %>% 
  # select(seq_id,Agency_ID,Contact_person,SourceName,Contact_email,
  #        Contact_telephone,Latitude,Longitude,Sensor_accuracy) %>%
  rename(AKOATS_ID = seq_id) %>% 
         #SiteID = Agency_ID) %>% 
  mutate(AKOATS_ID = as.numeric(AKOATS_ID),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude))

akoats_fsch <- akoats_fsch %>% 
  filter(SourceName == "usfsakChugach")
akoats_fsch


```
Should we create some AKOATS_IDs for the sites that do not have corresponding values?

```{r Combine Metadata, message=FALSE, warning=FALSE}

# Create tibble for combined akoats and depracated md
usfs_md <- tibble()
# Create tibble to copy depracated md
fs_akoats_sites <- tibble()

fs_akoats_sites <- usfs_dpmd %>% 
  select(Agency_ID, AKOATS_ID) %>% 
  left_join(akoats_fsch) %>% 
  filter(!is.na(AKOATS_ID))
  
keep <- intersect(names(fs_akoats_sites), names(usfs_dpmd))

usfs_md <- bind_rows(fs_akoats_sites,
  usfs_dpmd %>% filter(is.na(AKOATS_ID)) %>%
    select(one_of(keep))) %>% 
  distinct()

usfs_md

```

## Data

Extract file name of data set and in new column

```{r Collect temperature data files}

# Folder containing source datasets
source_fol <- ".\\source\\06_USFS_Chugach"

files <- list.files(source_fol, full.names = T, recursive = T, pattern = ".*.csv|.*.xlsx")

# Remove metadata sheets and air temp datasets
patterns <- c("siteinfo", "selectedsites", "AirT", "geospatial", "UTM", "USFS_Chugach_Metada")

fs_files <- files
for (pattern in patterns){
  fs_files <- fs_files[!grepl(pattern, fs_files)]
}

basename(fs_files)
```

Bind all input datasets together after removing Airtemp and site info CSVs

```{r Bind all data, message=FALSE, warning=FALSE}

# Create tibble for fs temperature data
fsdat <- tibble()

# Create counters to check data file formats
t1 <- 0
t2 <- 0

for ( i in fs_files){
  # CSV Data
  if( endsWith (i,".csv")){
    dat <- read_csv( file = i)
    name = str_sub(basename(i), end = -5)
    
    if(grepl("/", dat[,1]) == TRUE) {
      dat <- dat[1:3]
      colnames(dat) <- c("sampleDate", "sampleTime", "Temperature")
      print( paste0( name, " has Date as first column"))
      print(colnames(dat))
      # Add to correctly formatted counter
      t1= t1 + 1
      
      # Format data
      dat <- dat %>%  mutate(data_filename = name) %>% 
      #                       UseData = case_when( Temperature < -0.25 ~ 0,
      #                                            Temperature > 30 ~ 0,
      #                                            TRUE ~ 1)) %>%
        transform(sampleDate = as_date(mdy(sampleDate)),
                  # Some times in HMS and some in HM format
                  sampleTime = format(lubridate::parse_date_time(sampleTime,c('HMS','HM')),'%H:%M:%S'),
                  Temperature = as.numeric(Temperature))
      fsdat <- bind_rows(fsdat, dat[!is.na(dat$Temperature),]) %>%
        select(data_filename, sampleDate, sampleTime, Temperature)

    }else{
      print(paste0( name,  " has Different format "))
      # Add to incorrectly formatted counter
      t2= t2 + 1
    }
   
    # Excel Data  
  }else if( endsWith ( i,".xlsx")){
    dat <- read_excel( path = i)
    name = str_sub( basename( i), end = -6)
    if(grepl("-", dat[,2]) == TRUE) {
      dat <- dat[1:3]
      colnames(dat) <- c("sampleDate", "sampleTime", "Temperature")
      print( paste0( name, " has Date as first column"))
      print(colnames(dat))
      # Add to correctly formatted counter
      t1= t1 + 1
      
      # Format data 
      dat <- dat %>% mutate(data_filename = name) %>% 
#                            UseData = case_when( Temperature < -0.25 ~ 0,
#                                                 Temperature > 30 ~ 0,
#                                                 TRUE ~ 1)) %>% 
        transform(sampleDate = as.Date(sampleDate, format = "%m/%d/%Y"),
                  sampleTime = hms::as.hms(sampleTime),
                  Temperature = as.numeric(Temperature))
      fsdat <- bind_rows(fsdat, dat[!is.na(dat$Temperature),]) %>% 
        select(data_filename, sampleDate, sampleTime, Temperature)
      
    }else{
      print(paste0( name,  " has Different format "))
      # Add to incorrectly formatted counter
      t2= t2 + 1
    }
    
  }
}

print( paste0(t1, " Formatted correctly" ," | ",t2, " Incorrectly formatted"))

summary(fsdat)
```

```{r Export csv of temp data filenames}

# Export CSV with list of filenames to identify measurement location and link AKOATS
# IDs where possible

# unique(fsdat$data_filename)

# fs_site_names <- unique(fsdat$data_filename)
# fs_site_names
#  
# as.data.frame(fs_site_names) %>% 
#   write_csv(path = "data_preparation/formatted_data/usfs_site_names.csv")

```

Some problems with dates - some are 2 digit and some are 4 digit. In as.Date function, tryFormats won't fix this problem, which is unfortunate. I'll need to get number of digits using regexp and then manually tell it what format it is in.

```{r fix dates, message=FALSE, warning=FALSE}
# Setting tz = "GMT" removes errors associated with dt not calculating for daylight savings time dates in March
fsdat <- fsdat %>% 
  mutate(year = lubridate::year(sampleDate),
         dt = as.POSIXct(paste(sampleDate, sampleTime, sep = " "),
                    format = "%Y-%m-%d %H:%M", tz = "GMT"))

summary(fsdat)

```

Additional fields to add to data frame:

* Add SiteIDs to the data. Unfortunately, Luca is using waterbody names for site ids, which can complicate having unique station names down the line - e.g. additional sites on the same stream or another agency monitoring the same stream.

* Add date-time and year for plotting.

```{r add new fields, message=FALSE, warning=FALSE}

# Add Agency ID
fsdat <- left_join(fsdat, usfs_dpmd %>%
                     select(data_SiteID, Agency_ID),
                   by = c("data_filename" = "data_SiteID")) 

fsdat
summary(fsdat)
```

# Remove Duplicates and format output
## Drop Sites based on Luca Notes in worksheet/email convos

* In Luca's siteinfo worksheet, he mentions some sites that may not be useful
for our thermal sensitivity analysis. Flag those here.

Worksheet drops  include: 
"Clear Creek", "Hatchery Creek", "Ibeck Creek", "Jack Bay River",
"McKinley Lake", "NF Williwaw Creek", "Pigot Bay Spawn Channel",
"Rude River SC", "SF Williwaw Creek", "Solf Lake Inlet" & "Steller Jay Creek"

Luca Notes Update 02/24/2021 :
"L.A.- My initial hunch is that that for Olsen, Eagle, Rude, Pigot, Shelter,
and maybe Sheep, the data will useable for the DFA with very limited
tidal impacts in the summer"

DM Notes: Data from Quartz Creek overlap and there are duplicate measurements 
in the <QuartzCreek_Surf_2019-11-13> Dataset. Unsure if the two datasets are from the same location and where the duplicate measure came from?

```{r Remove Duplicates and Classify initial useSite}
# Tibble for all formatted data with duplicates removed
fschdat <- tibble()

# Drop sites based on email conversations and notes in site info workbooks
fschdat <- fsdat %>% 
  mutate(useSite = case_when(
    Agency_ID %in% c("Clear Creek", "Hatchery Creek", "Ibeck Creek",
                     "Jack Bay River", "McKinley Lake", "NF Williwaw Creek",
                     "SF Williwaw Creek", "Solf Lake Inlet",
                     "Steller Jay Creek") ~ 0, TRUE ~ 1),
    # Need to make new site id because AGENCY_ID for these data = waterbody_name
    SiteID = paste0("USFS_", Agency_ID)) %>%
  # Drop duplicate records - QUARTZ CREEK STILL HAS DUPLICATES POSSIBLY DUE TO MEASUREMENTS FROM 2 LOGGERS BEING INCLUDED IN THE QuartzCreek_Surf_2019-11-13 dataset and overlapping measurements between from the second dataset
  distinct()

#Reorder and save output
colorder <- c("data_filename", "SiteID","Agency_ID", "sampleDate", "sampleTime",
              "Temperature", "dt", "year", "useSite")

fschdat <- fschdat[,colorder]

summary(fschdat)
```
Average temps for Quartz Creek and drop measurements from datasets in Kenai_2018-20 Batch for Quartz and Daves Creeks for day of overlapping measurements 2018-08-22 and Bench on 2018-09-20


```{r Average any duplicate measurements and keep distinct records}

fschdat1 <- tibble()
fschquartz1 <- tibble()
fschquartz0 <-  tibble()
fschdat_nodups <- tibble()

# Datasets with overlapping measurements on 2018-08-22
lap1 <- c("R10_CNF_KPZ_DavesCrk_surfaceH2O_cz_2018_08_22_2020_08_18_10497829_QAQC",
"R10_CNF_KPZ_QuartzCrk_surfaceH2O_cz_2018_08_22_2020_08_21_20163535_QAQC")

# Dataset with overlapping measurements on 2018-09-20
lap2 <- "R10_CNF_KPZ_BENCH_surfaceH2O_cz_2018_09_20_2020_09_09_10515702_QAQC"

# Add UseData field and and assign 0 to most recent dataset on day of overlap
fschdat <- fschdat %>%
  mutate(UseData = case_when( data_filename %in% lap1 &
    sampleDate == "2018-08-22" ~ 0,
  data_filename == lap2 & sampleDate == "2018-09-20" ~ 0,
  TRUE ~ 1))

# Copy of all sites but quartz creek
fschdat1 <- fschdat %>% 
  filter(SiteID != "USFS_Quartz Creek")
# Copy of quartz creek data on day of overlap from most recent dataset
fschquartz0 <- fschdat %>% 
  filter(SiteID == "USFS_Quartz Creek", UseData == 0)

# Calculate Avg Temperature for duplicate measurements at quartz creek
fschquartz1 <- fschdat %>% 
  filter(UseData == 1, SiteID == "USFS_Quartz Creek") %>% 
  group_by(SiteID, dt) %>% 
  mutate(avgTemp = mean(Temperature, na.rm = TRUE)) %>% 
  ungroup() %>% 
  mutate(Temperature = avgTemp) %>% 
  select(-avgTemp)

# Merge back together and select distinct records
fschdat_nodups <- rbind(fschdat1, fschquartz1, fschquartz0) %>% 
  select(-data_filename) %>% 
  distinct()

```

```{r Format metada for saving}
# Rename 
usfs_md <- usfs_md %>% 
  rename("seq_id" = "AKOATS_ID") 

```

# Save Data
## Save Temp Data
Save formatted versions of temp data as rds prior to QAQC

```{r Save Formatted USFS Temp Data}
# Save formatted data for input into qa script
usfsChug.data <- saveRDS(fschdat_nodups, "formatted_data/usfsChug.data.rds")

```

## Save Metadata
Save a copy of the the metadata locally and upload to google drive

```{r Save USFS Metadata}

# Knittr setup command not working so must reset and run as chunk here
setwd(paste0(rprojroot::find_rstudio_root_file()))
getwd()

# Resolve NA values for sites not in AKOATS
usfs_md_udpate <- usfs_md %>% 
  mutate( SiteID = paste0("USFS_",Agency_ID),
    Contact_person = case_when (is.na(Contact_person) ~ "Luca Adelfio",
          TRUE ~ Contact_person),
    # Calculate use site 
    useSite = case_when(
    Agency_ID %in% c("Clear Creek", "Hatchery Creek", "Ibeck Creek",
                     "Jack Bay River", "McKinley Lake", "NF Williwaw Creek",
                     "SF Williwaw Creek", "Solf Lake Inlet",
                     "Steller Jay Creek") ~ 0, TRUE ~ 1))

usfs_md_udpate

# Save metadata 
# Must use absolute path?? save stopped working with relative
usfsChugmd.data <- saveRDS(usfs_md_udpate, ".//formatted_data//usfsChugmd.data.rds")

# Filter out sites that have not been qa'd for AKSSF
usfs_md_udpate %>% 
  filter(useSite == 1) %>% 
  save_metadata_files("usfsChug")

```
