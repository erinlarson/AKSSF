---
title: "data_USFS_Chugach"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, messages = FALSE)
knitr::opts_knit$set(root.dir = normalizePath(".."))

library(tidyverse)
library(readxl)
library(stringr)
library(lubridate)
library(googlesheets4)
library(rnoaa)
library(hms)
#library(googledrive) #Only need if accessing file stored in drive to download

```

# Define Functions
Functions not needed for this dataset but included as example

```{r Define Functions}

# Read in csv and add column with filename
read_csv_and_name <- function(csv_file_path) {
  sheet_name <- str_match(csv_file_path, "\\/\\s*(.*?)\\s*\\.csv")[2]
  dat <- read_csv(csv_file_path) %>% 
      mutate(file_name = sheet_name)
}

# Function to pull logger serial# from temp column
log_sn_fun <- function(string){ 
  
  str_extract(string, "\\-*\\d+\\.*\\d*")
} 

```

# Review Data
## Metadata

DM - I updated the depracated metadata sheet to correct a few sites that were in AKOATS and add new sites from Luca that were not in AKOATS.  Sheet is stored locally but I can upload a copy if needed.  

```{r Metadata not in AKOATS}

# Access google sheet with metadata for usfs chugach sites

# depracated_metadata <- "https://docs.google.com/spreadsheets/d/1ymyhRyAomnJZTZqr4IbRmCnJjVaOIpCQYGscJDbCk-w/edit#gid=0"

# Created a spreadsheet to link AKOATS IDs and Site names/Coords using data filename----
depracated_metadata <- "C:\\Users\\dwmerrigan\\Documents\\GitHub\\AKSSF\\data_preparation\\formatted_data\\USFS_Chugach_Working_deprecated_md.xlsx"

usfs_dpmd <- read_excel(depracated_metadata, sheet = "deprecated_metadata",
                    col_names = TRUE) %>% 
  filter(ACCS_Project == "USFS_Chugach") %>% 
  mutate(AKOATS_ID = as.numeric(AKOATS_ID),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude)) 

usfs_dpmd %>% select(Agency_ID, AKOATS_ID, Latitude, Longitude, data_SiteID)  
  

```

DM Notes: Use GOOGLE SHEET Copy for consistency

```{r AKOATS MD}

# Find and copy AKOATS working excel file in Google Drive
# x <- drive_find(pattern = "<NAME OF EXCEL FILE STORED IN GOOGLE DRIVE>", type = "xlsx", n_max = 1)
# akoats_cont <- drive_download(x, overwrite = TRUE)
# akoats_path <- akoats_cont$local_path
# akoats_fsch <- read_excel( akoats_path, sheet = "CONTINUOUS_DATA",
#                         col_names = TRUE
#                         )  %>%

# Access Google Sheet copy of AKOATS working sheet and use AKOATS Complete sheet

akoats_2020wk <- "https://docs.google.com/spreadsheets/d/1SPZXNGm_Tc39-GuJXY8j7Eb1lX6DwXTQ1en2_LvCI1I/edit#gid=1281874712"

akoats_fsch <- read_sheet(akoats_2020wk, sheet = "AKOATS_COMPLETE",
                    col_names = TRUE,
                    col_types = "c") %>% 
  select(seq_id,Agency_ID,Contact_person,SourceName,Contact_email,
         Contact_telephone,Latitude,Longitude,Sensor_accuracy) %>%
  rename(AKOATS_ID = seq_id) %>% 
         #SiteID = Agency_ID) %>% 
  mutate(AKOATS_ID = as.numeric(AKOATS_ID),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude))

akoats_fsch <- akoats_fsch %>% 
  filter(SourceName == "usfsakChugach")
akoats_fsch


```
Should we create some AKOATS_IDs for the sites that do not have corresponding values?

```{r Combine Metadata, message=FALSE, warning=FALSE}

usfs_md <- tibble()
fs_akoats_sites <- tibble()

fs_akoats_sites <- usfs_dpmd %>% 
  select(Agency_ID, AKOATS_ID) %>% 
  left_join(akoats_fsch) %>% 
  filter(!is.na(AKOATS_ID))
  
keep <- intersect(names(fs_akoats_sites), names(usfs_dpmd))

usfs_md <- bind_rows(fs_akoats_sites,
  usfs_dpmd %>% filter(is.na(AKOATS_ID)) %>%
    select(one_of(keep)))

# saveRDS(usfs_md, "output/usfs_md.rds")

usfs_md

```

## Data

Extract file name of data set and in new column

```{r Collect temperature data files}

# Folder containing source datasets
source_fol <- "C:\\Users\\dwmerrigan\\Documents\\GitHub\\AKSSF\\data_preparation\\source"

files <- list.files(source_fol, full.names = T, recursive = T, pattern = ".*.csv|.*.xlsx")

# Remove metadata sheets and air temp datasets
patterns <- c("siteinfo", "selectedsites", "AirT", "geospatial", "UTM", "USFS_Chugach_Metada")

fs_files <- files
for (pattern in patterns){
  fs_files <- fs_files[!grepl(pattern, fs_files)]
}

basename(fs_files)
```

Bind all input datasets together after removing Airtemp and site info CSVs

```{r Bind all data, message=FALSE, warning=FALSE}

# Create tibble for fs temperature data
fsdat <- tibble()

# Create counters to check data file formats
t1 <- 0
t2 <- 0

for ( i in fs_files){
  # CSV Data
  if( endsWith (i,".csv")){
    dat <- read_csv( file = i)
    name = str_sub(basename(i), end = -5)
    
    if(grepl("/", dat[,1]) == TRUE) {
      dat <- dat[1:3]
      colnames(dat) <- c("sampleDate", "sampleTime", "Temperature")
      print( paste0( name, " has Date as first column"))
      print(colnames(dat))
      # Add to correctly formatted counter
      t1= t1 + 1
      
      # Format data
      dat <- dat %>% mutate(data_filename = name) %>%
        transform(sampleDate = as_date(mdy(sampleDate)),
                  sampleTime = hms::as.hms(sampleTime),
                  Temperature = as.numeric(Temperature))
      fsdat <- bind_rows(fsdat, dat[!is.na(dat$Temperature),]) %>%
        select(data_filename, sampleDate, sampleTime, Temperature)

    }else{
      print(paste0( name,  " has Different format "))
      # Add to incorrectly formatted counter
      t2= t2 + 1
    }
   
    # Excel Data  
  }else if( endsWith (i,".xlsx")){
    dat <- read_excel(path = i)
    name = str_sub(basename(i), end = -6)
    
    if(grepl("-", dat[,2]) == TRUE) {
      dat <- dat[1:3]
      colnames(dat) <- c("sampleDate", "sampleTime", "Temperature")
      print( paste0( name, " has Date as first column"))
      print(colnames(dat))
      # Add to correctly formatted counter
      t1= t1 + 1
      
      # Format data 
      dat <- dat %>% mutate(data_filename = name) %>% 
        transform(sampleDate = as.Date(sampleDate, format = "%m/%d/%Y"),
                  sampleTime = hms::as.hms(sampleTime),
                  Temperature = as.numeric(Temperature))
      fsdat <- bind_rows(fsdat, dat[!is.na(dat$Temperature),]) %>% 
        select(data_filename, sampleDate, sampleTime, Temperature)
      
    }else{
      print(paste0( name,  " has Different format "))
      # Add to incorrectly formatted counter
      t2= t2 + 1
    }
    
  }
}

print( paste0(t1, " Formatted correctly" ," | ",t2, " Incorrectly formatted"))

```

```{r Export csv of temp data filenames}

# Export CSV with list of filenames to identify measurement location and link AKOATS
# IDs where possible

unique(fsdat$data_filename)

# fs_site_names <- unique(fsdat$data_filename)
# fs_site_names
#  
# as.data.frame(fs_site_names) %>% 
#   write_csv(path = "data_preparation/formatted_data/usfs_site_names.csv")

```

Some problems with dates - some are 2 digit and some are 4 digit. In as.Date function, tryFormats won't fix this problem, which is unfortunate. I'll need to get number of digits using regexp and then manually tell it what format it is in.

```{r fix dates, message=FALSE, warning=FALSE}

fsdat2 <- fsdat %>% 
  mutate(year1 = sub(".*/.*/", "", sampleDate),
         sampleDate2 = case_when(nchar(year1) == 4 ~ as.Date(sampleDate,
                                                             format = "%m/%d/%Y"),
                                 TRUE ~ as.Date(sampleDate, format = "%m/%d/%y")),
         sampleDate = sampleDate2,
         dt = as.POSIXct(paste(sampleDate, sampleTime, sep = " "),
                    format = "%Y-%m-%d %H:%M"))

# fsdat %>%
#   group_by(data_filename) %>%
#   summarize(min(sampleDate), max(sampleDate))
# 
# fsdat %>%
#   distinct(year = year(sampleDate)) %>%
#   arrange(year)
# 
# fsdat %>%
#   distinct(data_filename, year = year(sampleDate)) %>%
#   count(data_filename) %>%
#   arrange(n) %>%
#   summarize(median(n))

fsdat2

```

Additional fields to add to data frame:

* Add SiteIDs to the data. Unfortunately, Luca is using waterbody names for site ids, which can complicate having unique station names down the line - e.g. additional sites on the same stream or another agency monitoring the same stream.
* In Luca's siteinfo worksheet, he mentions some sites that may not be useful for our thermal sensitivity analysis. Flag those here.
* Add date-time and year for plotting.

```{r add new fields, message=FALSE, warning=FALSE}
fsdat2 <- left_join(fsdat2, usfs_dpmd %>%
                     select(data_SiteID, Agency_ID),
                   by = c("data_filename" = "data_SiteID")) 
fsdat2
```


# Remove Duplicates and format output
## Drop Sites based on Luca Notes in worksheet/email convos
Drop sites highlighted in yellow on worksheet and any additional sites covered in conversation with Luca A.

Worksheet drops  include: 
"Clear Creek", "Hatchery Creek", "Ibeck Creek", "Jack Bay River", "McKinley Lake", "NF Williwaw Creek", "Pigot Bay Spawn Channel", "Rude River SC",
"SF Williwaw Creek", "Solf Lake Inlet" & "Steller Jay Creek"

Luca Notes Update 02/24/2021 :
"L.A.- My initial hunch is that that for Olsen, Eagle, Rude, Pigot, Shelter, and maybe Sheep, the data will useable for the DFA with very limited tidal impacts in the summer"

```{r Remove Duplicates and Classify initial useSite}
# Tibble for all formatted data with duplicates removed
fschdat <- tibble()

# Drop sites based on email conversations and notes in site info workbooks
fschdat <- fsdat2 %>% 
  mutate(useSite = case_when(
    Agency_ID %in% c("Clear Creek", "Hatchery Creek", "Ibeck Creek",
                     "Jack Bay River", "McKinley Lake", "NF Williwaw Creek",
                     # Removing pigot and rude based on Luca Email 02/24/2021
                     #"Pigot Bay Spawn Channel", "Rude River SC",
                     "SF Williwaw Creek", "Solf Lake Inlet",
                     "Steller Jay Creek") ~ 0, TRUE ~ 1),
    SiteID = Agency_ID,
    year = year(sampleDate)) %>% 
  select( -data_filename, -year1, -sampleDate2, -Agency_ID) %>% 
  # Drop duplicate records
  distinct()

#Reorder and save output
colorder <- c("SiteID","sampleDate", "sampleTime", "Temperature", "dt", "year",
              "useSite")

fschdat <- fschdat[,colorder]

```

# Save Data
## Save Temp Data
Save formatted version prior to QAQC
```{r Save Formatted USFS Data}

# Save formatted data for input into qa script
usfsChug.data <- saveRDS(fschdat, "data_preparation/formatted_data/usfsChug.data.rds")

```
## Save Metadata

```{r Save USFS Metadata}
# Save metadata 
usfsChugmd.data <- saveRDS(usfs_md, "data_preparation/formatted_data/usfsChugmd.data.rds")

```


