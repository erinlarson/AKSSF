---
title: "data_USFS_Chugach"
output: html_document
---

The <knitr::opts_knit$set(root.dir = normalizePath(".."))> only works when run as a chunk and resets to the data prep folder for all following chunks?  Not sure why this is happening.

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = setwd(normalizePath("..")))
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, messages = FALSE)

library(tidyverse)
library(readxl)
library(stringr)
library(lubridate)
library(googlesheets4)
library(rnoaa)
library(hms)
library(googledrive) #Only need if accessing file stored in drive to download
source("helper_functions.R")

```


# Review Data
## Metadata

DM - I updated the depracated metadata sheet to correct a few sites that were in AKOATS and add new sites from Luca that were not in AKOATS. 

```{r Metadata not in AKOATS}
# Created a spreadsheet to link AKOATS IDs and Site names/Coords using data filename----
depracated_metadata <- "C:\\Users\\dwmerrigan\\Documents\\GitHub\\AKSSF\\data_preparation\\formatted_data\\USFS_Chugach_Working_deprecated_md.xlsx"

usfs_dpmd <- read_excel(depracated_metadata, sheet = "deprecated_metadata",
                    col_names = TRUE) %>% 
  filter(ACCS_Project == "USFS_Chugach") %>% 
  mutate(AKOATS_ID = as.numeric(AKOATS_ID),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude)) 

usfs_dpmd <- usfs_dpmd %>%
  select(Agency_ID, AKOATS_ID, Latitude, Longitude, data_SiteID)  
  

```

DM Notes: Use GOOGLE SHEET Copy for consistency

```{r AKOATS MD}

# Find and copy AKOATS working excel file in Google Drive
# x <- drive_find(pattern = "<NAME OF EXCEL FILE STORED IN GOOGLE DRIVE>", type = "xlsx", n_max = 1)
# akoats_cont <- drive_download(x, overwrite = TRUE)
# akoats_path <- akoats_cont$local_path
# akoats_fsch <- read_excel( akoats_path, sheet = "CONTINUOUS_DATA",
#                         col_names = TRUE
#                         )  %>%

# Access Google Sheet copy of AKOATS working sheet and use AKOATS Complete sheet

akoats_2020wk <- "https://docs.google.com/spreadsheets/d/1SPZXNGm_Tc39-GuJXY8j7Eb1lX6DwXTQ1en2_LvCI1I/edit#gid=1281874712"

akoats_fsch <- read_sheet(akoats_2020wk, sheet = "AKOATS_COMPLETE",
                    col_names = TRUE,
                    col_types = "c") %>% 
  # select(seq_id,Agency_ID,Contact_person,SourceName,Contact_email,
  #        Contact_telephone,Latitude,Longitude,Sensor_accuracy) %>%
  rename(AKOATS_ID = seq_id) %>% 
         #SiteID = Agency_ID) %>% 
  mutate(AKOATS_ID = as.numeric(AKOATS_ID),
         Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude))

akoats_fsch <- akoats_fsch %>% 
  filter(SourceName == "usfsakChugach")
akoats_fsch


```
Should we create some AKOATS_IDs for the sites that do not have corresponding values?

```{r Combine Metadata, message=FALSE, warning=FALSE}

# Create tibble for combined akoats and depracated md
usfs_md <- tibble()
# Create tibble to copy depracated md
fs_akoats_sites <- tibble()

fs_akoats_sites <- usfs_dpmd %>% 
  select(Agency_ID, AKOATS_ID) %>% 
  left_join(akoats_fsch) %>% 
  filter(!is.na(AKOATS_ID))
  
keep <- intersect(names(fs_akoats_sites), names(usfs_dpmd))

usfs_md <- bind_rows(fs_akoats_sites,
  usfs_dpmd %>% filter(is.na(AKOATS_ID)) %>%
    select(one_of(keep))) %>% 
  distinct()

usfs_md

```

## Data

Extract file name of data set and in new column

```{r Collect temperature data files}

# Folder containing source datasets
source_fol <- "C:\\Users\\dwmerrigan\\Documents\\GitHub\\AKSSF\\data_preparation\\source"

files <- list.files(source_fol, full.names = T, recursive = T, pattern = ".*.csv|.*.xlsx")

# Remove metadata sheets and air temp datasets
patterns <- c("siteinfo", "selectedsites", "AirT", "geospatial", "UTM", "USFS_Chugach_Metada")

fs_files <- files
for (pattern in patterns){
  fs_files <- fs_files[!grepl(pattern, fs_files)]
}

basename(fs_files)
```

Bind all input datasets together after removing Airtemp and site info CSVs
Add UseData field and calculated temp threshold values - set upper max to 30 for these data


```{r Bind all data, message=FALSE, warning=FALSE}

# Create tibble for fs temperature data
fsdat <- tibble()

# Create counters to check data file formats
t1 <- 0
t2 <- 0

for ( i in fs_files){
  # CSV Data
  if( endsWith (i,".csv")){
    dat <- read_csv( file = i)
    name = str_sub(basename(i), end = -5)
    
    if(grepl("/", dat[,1]) == TRUE) {
      dat <- dat[1:3]
      colnames(dat) <- c("sampleDate", "sampleTime", "Temperature")
      print( paste0( name, " has Date as first column"))
      print(colnames(dat))
      # Add to correctly formatted counter
      t1= t1 + 1
      
      # Format data
      dat <- dat %>% mutate(data_filename = name,
                            UseData = case_when( Temperature < -0.25 ~ 0,
                                                 Temperature > 30 ~ 0,
                                                 TRUE ~ 1)) %>%
        transform(sampleDate = as_date(mdy(sampleDate)),
                  # Some times in HMS and some in HM format
                  sampleTime = format(lubridate::parse_date_time(sampleTime,c('HMS','HM')),'%H:%M:%S'),
                  Temperature = as.numeric(Temperature))
      fsdat <- bind_rows(fsdat, dat[!is.na(dat$Temperature),]) %>%
        select(data_filename, sampleDate, sampleTime, Temperature, UseData)

    }else{
      print(paste0( name,  " has Different format "))
      # Add to incorrectly formatted counter
      t2= t2 + 1
    }
   
    # Excel Data  
  }else if( endsWith (i,".xlsx")){
    dat <- read_excel(path = i,col_types = )
    name = str_sub(basename(i), end = -6)
    
    if(grepl("-", dat[,2]) == TRUE) {
      dat <- dat[1:3]
      colnames(dat) <- c("sampleDate", "sampleTime", "Temperature")
      print( paste0( name, " has Date as first column"))
      print(colnames(dat))
      # Add to correctly formatted counter
      t1= t1 + 1
      
      # Format data 
      dat <- dat %>% mutate(data_filename = name,
                            UseData = case_when( Temperature < -0.25 ~ 0,
                                                 Temperature > 30 ~ 0,
                                                 TRUE ~ 1)) %>% 
        transform(sampleDate = as.Date(sampleDate, format = "%m/%d/%Y"),
                  sampleTime = hms::as.hms(sampleTime),
                  Temperature = as.numeric(Temperature))
      fsdat <- bind_rows(fsdat, dat[!is.na(dat$Temperature),]) %>% 
        select(data_filename, sampleDate, sampleTime, Temperature, UseData)
      
    }else{
      print(paste0( name,  " has Different format "))
      # Add to incorrectly formatted counter
      t2= t2 + 1
    }
    
  }
}

print( paste0(t1, " Formatted correctly" ," | ",t2, " Incorrectly formatted"))

summary(fsdat)
```

```{r Export csv of temp data filenames}

# Export CSV with list of filenames to identify measurement location and link AKOATS
# IDs where possible

# unique(fsdat$data_filename)

# fs_site_names <- unique(fsdat$data_filename)
# fs_site_names
#  
# as.data.frame(fs_site_names) %>% 
#   write_csv(path = "data_preparation/formatted_data/usfs_site_names.csv")

```

Some problems with dates - some are 2 digit and some are 4 digit. In as.Date function, tryFormats won't fix this problem, which is unfortunate. I'll need to get number of digits using regexp and then manually tell it what format it is in.

```{r fix dates, message=FALSE, warning=FALSE}
# Setting tz = "GMT" removes errors associated with dt not calculating for daylight savings time dates in March
fsdat <- fsdat %>% 
  mutate(year = lubridate::year(sampleDate),
         dt = as.POSIXct(paste(sampleDate, sampleTime, sep = " "),
                    format = "%Y-%m-%d %H:%M", tz = "GMT"))

summary(fsdat)

```

Additional fields to add to data frame:

* Add SiteIDs to the data. Unfortunately, Luca is using waterbody names for site ids, which can complicate having unique station names down the line - e.g. additional sites on the same stream or another agency monitoring the same stream.

* Add date-time and year for plotting.

```{r add new fields, message=FALSE, warning=FALSE}

# Add Agency ID
fsdat <- left_join(fsdat, usfs_dpmd %>%
                     select(data_SiteID, Agency_ID),
                   by = c("data_filename" = "data_SiteID")) 

fsdat
summary(fsdat)
```

# Remove Duplicates and format output
## Drop Sites based on Luca Notes in worksheet/email convos

* In Luca's siteinfo worksheet, he mentions some sites that may not be useful
for our thermal sensitivity analysis. Flag those here.

Worksheet drops  include: 
"Clear Creek", "Hatchery Creek", "Ibeck Creek", "Jack Bay River",
"McKinley Lake", "NF Williwaw Creek", "Pigot Bay Spawn Channel",
"Rude River SC", "SF Williwaw Creek", "Solf Lake Inlet" & "Steller Jay Creek"

Luca Notes Update 02/24/2021 :
"L.A.- My initial hunch is that that for Olsen, Eagle, Rude, Pigot, Shelter,
and maybe Sheep, the data will useable for the DFA with very limited
tidal impacts in the summer"

DM Notes: Data from Quartz Creek overlap and there are duplicate measurements 
in the <QuartzCreek_Surf_2019-11-13> Dataset. Unsure if the two datasets are from
the same location and where the duplicate measure came from?

```{r Remove Duplicates and Classify initial useSite}
# Tibble for all formatted data with duplicates removed
fschdat <- tibble()

# Drop sites based on email conversations and notes in site info workbooks
fschdat <- fsdat %>% 
  mutate(useSite = case_when(
    Agency_ID %in% c("Clear Creek", "Hatchery Creek", "Ibeck Creek",
                     "Jack Bay River", "McKinley Lake", "NF Williwaw Creek",
                     # Removing pigot and rude based on Luca Email 02/24/2021
                     #"Pigot Bay Spawn Channel", "Rude River SC",
                     "SF Williwaw Creek", "Solf Lake Inlet",
                     "Steller Jay Creek") ~ 0, TRUE ~ 1),
    # Need to make new site id because AGENCY_ID for these data = waterbody_name
    SiteID = paste0("USFS_", Agency_ID)) %>%
  select( -data_filename) %>% 
  # Drop duplicate records - QUARTZ CREEK STILL HAS DUPLICATES POSSIBLY DUE TO MEASUREMENTS FROM 2 LOGGERS BEING INCLUDED IN THE QuartzCreek_Surf_2019-11-13 dataset and overlapping measurements between from the second dataset
  distinct()

#Reorder and save output
colorder <- c("SiteID","Agency_ID", "sampleDate", "sampleTime", "Temperature",
              "dt", "year", "useSite","UseData")

fschdat <- fschdat[,colorder]

summary(fschdat)

```
```{r Format metada for saving}
# Rename 
usfs_md <- usfs_md %>% 
  rename("seq_id" = "AKOATS_ID") 

```

# Save Data
Still having trouble with the knitr::opts_knit$set(root.dir = ....) command not applying to subsequent chunks.Must setwd(paste0(rprojroot::find_rstudio_root_file())) in order to run helper save functions

## Save Temp Data
Save formatted versions of temp data as rds prior to QAQC

```{r Save Formatted USFS Temp Data}

# Save formatted data for input into qa script
usfsChug.data <- saveRDS(fschdat, "formatted_data/usfsChug.data.rds")

save_aktemp_files(fschdat, "usfsChug")
```

## Save Metadata
Save a copy of the the metadata locally and upload to google drive

```{r Save USFS Metadata}
# Knittr setup command not working so must reset and run as chunk here
setwd(paste0(rprojroot::find_rstudio_root_file()))
# Save metadata 

usfsChugmd.data <- saveRDS(usfs_md, "data_preparation/formatted_data/usfsChugmd.data.rds")

save_metadata_files(usfs_md, "usfsChug")

```

