---
title: "data_combine_all"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
knitr::opts_knit$set(root.dir = normalizePath("..")) 

# load packages
library(googledrive)
# library(googlesheets4)
library(lubridate)
library(readr)
library(hms)
# library(ggmap)
library(sf)
library(stars)
# library(leaflet)
# library(osmdata)
library(broom)
# library(caTools)
library(tidyverse)
library(daymetr)

# install.packages("devtools")
# devtools::install_github("yutannihilation/ggsflabel")
# library(ggsflabel)

```


# Metadata

Bring in the metadata for each dataset. Export a shapefile so that we can check site locations in ArcGIS and also intersect to get catchment IDs. The site locations are used to extract DAYMET air temperatures in the air tempertures rmd.

```{r read in metadata}
gd.akssf.files <- drive_ls(path = "https://drive.google.com/drive/u/0/folders/1_qtmORSAow1fIxvh116ZP7oKd_PC36L0")

gd.akssf.files %>% 
  arrange(name)

gd.metadata.files <- gd.akssf.files %>% 
  filter(grepl("Metadata", name) & grepl(".csv", name))

gd.metadata.files %>% 
  arrange(name)

folder <- "data_preparation/final_data/Metadata/"

#note may need to redirect to root dir to get this to run. Doesn't always work after just running line in setup chunk.
for (i in seq_along(gd.metadata.files$name)) {
  drive_download(as_id(gd.metadata.files$id[i]),
                 path = str_c(folder, gd.metadata.files$name[i]),
                 overwrite = TRUE)
}

local.md.files <- list.files(folder, full.names = TRUE)

md <- map_df(local.md.files, function(x) read_csv(x, col_types = "ccccccccccccccc") %>%
                      mutate(file_name = basename(x))) %>% 
  mutate(Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude))

md <- md %>% 
  mutate(SiteID = case_when(is.na(SiteID) ~ Agency_ID,
                            TRUE ~ SiteID))

md %>% dplyr::select(SiteID, file_name)
```


Convert to sf object and save as a shapefile.
NOT RUN, this was exported previously so we could shift sites to correct locations in ArcGIS.

```{r eval = FALSE}
md_sf <- st_as_sf(md, coords = c("Longitude", "Latitude"), crs = "wgs84")

akssf_sa <- st_read(dsn = "W:/GIS/AKSSF Southcentral/AKSSF_Hydrography.gdb", layer = "AKSSF_studyarea_HUC8")
akssf_sa_wgs84 <- st_transform(akssf_sa, crs = "wgs84")
st_crs(akssf_sa_wgs84) == st_crs(md_sf)
md_sf <- st_join(md_sf, akssf_sa_wgs84 %>% select(HUC8, Name))

ggplot() +
  geom_sf(data = akssf_sa) +
  geom_sf(data = md_sf, aes(color = HUC8 > 1903)) 

#remove sites in bb and kusko
md_sf_nobb <- md_sf %>% filter(!grepl("1903", HUC8)) %>% filter(!is.na(HUC8)) 

#check
ggplot() +
  geom_sf(data = akssf_sa, aes(color = HUC8 > 1903)) +
  geom_sf(data = md_sf_nobb) 

st_write(md_sf_nobb, "output/sites_outside_bb.shp", append = FALSE)
```

# Get regions and huc8 on metadata file

```{r create metadata sf}
md_sf <- st_as_sf(md, coords = c("Longitude", "Latitude"), crs = "WGS84")
```

Read in HUC8s and reproject.

```{r add huc8 names to md_sf}
huc8 <- st_read(dsn = "S:/Leslie/GIS/NHD Harmonized/WBD_19_GDB.gdb", layer = "WBDHU8")
st_crs(huc8)
huc8_wgs84 <- st_transform(huc8, crs = "WGS84")

st_crs(md_sf) == st_crs(huc8_wgs84)

md_sf <- st_join(md_sf, huc8_wgs84)

ggplot() +
  geom_sf(data = md_sf, aes(color = Name))

```

Remove Kuskokwim Delta sites from md and md_sf.

```{r remove kuskokwim sites}
kusko <- md_sf %>% filter(Name == "Kuskokwim Delta") %>% pull(SiteID)

md <- md %>% 
  filter(!SiteID %in% kusko)

md_sf <- md_sf %>% 
  filter(!SiteID %in% kusko)

```

Add in a region - Bristol Bay, Cook Inlet, PWS, Kodiak, or Copper. 

* Copper 19020101-19020104
* PWS    19020201-19020203
* CI     19020301-19020602
* Kodiak 19020701 - just this one?
* BB     19030202-19030306


```{r add region to md}
md_sf %>% 
  st_drop_geometry() %>% 
  distinct(Name, HUC8) %>% 
  arrange(HUC8)

md_sf <- md_sf %>% 
  mutate(Region = case_when(HUC8 %in% 19020301:19020602 ~ "Cook Inlet",
                            HUC8 == 19020701 ~ "Kodiak",
                            HUC8 %in% 19020101:19020104 ~ "Copper",
                            HUC8 %in% 19020201:19020203 ~ "Prince William Sound",
                            TRUE ~ "Bristol Bay"))

md <- left_join(md, st_drop_geometry(md_sf) %>% distinct(SiteID, HUC8, Name, Region))
```



# Data

Bring in the data files and exclude files associated with lakes.

```{r create daily data frame}

gd.daily.files <- gd.akssf.files %>% 
  filter(grepl("Daily_Data", name) & grepl(".csv", name), !grepl("lakes", name))

gd.daily.files %>% 
  arrange(name)

folder <- "data_preparation/final_data/Daily_Data/"

for (i in seq_along(gd.daily.files$name)) {
  drive_download(as_id(gd.daily.files$id[i]),
                 path = str_c(folder, gd.daily.files$name[i]),
                 overwrite = TRUE)
}

local.daily.files <- list.files(folder, full.names = TRUE)

ddat <- map_df(local.daily.files, function(x) read_csv(x, col_types = "cccccc") %>%
                      mutate(file_name = basename(x))) 

ddat <- ddat %>% 
  mutate(sampleDate = as.Date(sampleDate),
         minDT = as.numeric(minDT),
         maxDT = as.numeric(maxDT),
         meanDT = case_when(is.na(meanDT) ~ (minDT + maxDT)/2,
                            TRUE ~ as.numeric(meanDT)))

summary(ddat)


ddat %>% 
  filter(grepl("UW", SiteID)) %>% 
  distinct(SiteID, year = year(sampleDate)) %>% 
  group_by(SiteID) %>% 
  summarize(toString(year))
```

Clean up daily file for analysis. Some USGS data is missing the mean daily temperatures or minimum daily temperatures. Tim Cline's data only have the mean daily temperatures -- they were downloaded as summary data from Zenodo.

```{r}
ddat %>% filter(is.na(meanDT)) %>% distinct(file_name)

ddat %>% filter(is.na(minDT)) %>% distinct(file_name)

ddat <- ddat %>% 
  filter(!is.na(meanDT))

#remove kuskokwim delta sites
ddat <- ddat %>% 
  filter(!SiteID %in% kusko)

write_csv(ddat, paste0("data_preparation/final_data/daily_data", Sys.Date()))
```


# Check SiteIDs and save metadata

Check that all sites with data have entries in md files.

All sites with data have entries in metadata, but two sites in data that are classified as lakes in metadata at Solf Lake. Include them in metadata for now because both are at streams. 

Note one stream site in md without data that can be dropped. kdk_olgcr01b. This was an entry in akoats for a duplicate logger
that never should have been there. Same lat/long and same site. Only provided one time series for this site
and so site name in data matches kdk_olgcr01a. 

These can be resolved by filtering to only select metadata entries for SiteIDs in the data. But, there are multiple entries for some sites in the metadata that should be fixed:

* Asked Dustin to fix duplicate entries in Little Susitna network, some are actual duplicates and others are from sites that we are giving the same SiteID but have slightly different locations between 2016/2020.
* Note that there are duplicates for NPS lake outlets because I combined level and temp loggers into one site id since they are essentially the same site. Filter to just select the temp sites: 



```{r compare md and ddat}

ddat %>% 
  distinct(SiteID) %>% 
  left_join(md %>% mutate(md = 1) %>% dplyr::select(SiteID, md, Waterbody_type)) %>% 
  filter(Waterbody_type == "L")

nrow(ddat %>% distinct(SiteID))

#extra entries in md mostly at lakes except one which we don't need.
md %>% left_join(ddat %>% distinct(SiteID) %>% mutate(dat = 1)) %>% 
  filter(is.na(dat)) %>% 
  arrange(SiteID) %>% 
  dplyr::select(SiteID, Waterbody_type)

#not working because we have duplicate entries for some siteids.
left_join(ddat %>% distinct(SiteID), md) %>% 
  count(SiteID) %>% 
  arrange(desc(n))

md %>% filter(SiteID == "nps_KATM_naknlo") # remove seq_id 1631, 1470
md %>% filter(SiteID == "nps_LACL_lclaro") # remove seq_id 1633, 1476
md %>% filter(SiteID == "nps_KATM_lbrooo") # remove seq_id 1441
md %>% filter(SiteID == "nps_LACL_kijilo") # remove seq_id 1635
md %>% filter(SiteID == "BM7") # remove seq_id 1841

remove_seqids <- c(1631, 1470, 1633, 1476, 1441, 1635, 1841)

#want 485 metadata entries to match unique SiteIDs in data.
md_final <- left_join(ddat %>% distinct(SiteID), md %>% filter(!seq_id %in% remove_seqids))  

```

Chunk above run and everything saved on 6/14/21. Note, resaved on 11/23/21 because found error in Little Susitna site names. Also fixed all the duplicate md entries so now just 1 SiteID in the data to match 1 SiteID in the metadata.

Add a column for deshka sites in case that helps Tim. We may want to randomly subsample them for 2017-2019 so they don't dominate the common trend in the DFA.

```{r}
md_final <- md_final %>% 
  mutate(deshka = case_when(grepl("Beverley", SiteID) ~ 0,
                            grepl("Moose|Deshka|Kroto|OCC|OMCT|OWL|PGC|PKC|PMC|PNC|PSM|PTC|PWM", SiteID) ~ 1,
                            TRUE ~ 0))

md_final %>% 
  filter(deshka == 1) %>% 
  select(SiteID, SourceName)

md_final %>% 
  filter(SourceName %in% c("CIK", "fwsAnchorage")) %>% 
  select(SiteID, deshka)
```


```{r save metadata}
saveRDS(md_final, file = "data_preparation/final_data/md.rds")
write_csv(md_final, path = "data_preparation/final_data/md.csv")
saveRDS(md_sf %>% right_join(md_final %>% dplyr::select(SiteID)), file = "data_preparation/final_data/md_sf.rds")

```

# Merge with air temperaures


Merge with batched air temperatures.

```{r merge with air temps}

daymet <- read_csv("data_preparation/daymet/site_daymet.csv")
daymet %>% distinct(measurement)

#correct conversion is date2 bc date example below has Jan 1 as day 0!
daymet %>% 
  filter(year == 2006, 
         yday == 152) %>% 
  mutate(date = as.Date(yday, origin = paste0(year, "-01-01")),
         date2 = as.Date(paste(year, yday), format = "%Y %j"))

air <- daymet %>%
  filter(measurement %in% c("tmax..deg.c.", "tmin..deg.c.")) %>% 
  mutate(sampleDate = as.Date(paste(year, yday), format = "%Y %j")) %>% 
  filter(month(sampleDate) %in% 6:9) %>% 
  rename(SiteID = site) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(airDT = mean(value))

rm(daymet)

# ddat <- read_csv("data_preparation/final_data/daily_data2021-06-15")

left_join(ddat %>% distinct(SiteID), air %>% distinct(SiteID) %>% mutate(air = 1)) %>% count(air)

#half the data are outside summer window.
ddat %>% 
  count(summer = month(sampleDate) %in% 6:9,
        SiteID) %>% 
  arrange(SiteID)

sumdat <- left_join(ddat %>% filter(month(sampleDate) %in% 6:9,
                                    year(sampleDate) %in% 1980:2020), 
                    air)

summary(sumdat)

```

Save summer data frame for report on github page -- summary_report.rmd

```{r}
saveRDS(sumdat, paste0("data_preparation/final_data/summer_data_wair", Sys.Date(), ".rds"))
write_csv(sumdat, path = paste0("data_preparation/final_data/summer_data_wair", Sys.Date(), ".csv"))
```


```{r daylength summary}
daymet <- read_csv("data_preparation/daymet/site_daymet.csv")
daymet %>% distinct(measurement)

#bring in md to check with latitude
md <- readRDS("data_preparation/final_data/md_2022-02-08.rds")
md %>% 
  summary()

md %>% 
  arrange(Latitude)

dayl <- daymet %>%
  filter(measurement == "dayl..s.") %>% 
  mutate(sampleDate = as.Date(paste(year, yday), format = "%Y %j"),
         daylen = value/3600,
         daylen_min = value/60) %>% 
  filter(month(sampleDate) %in% 6:9) %>% 
  rename(SiteID = site) 

saveRDS(dayl, "data_preparation/daymet/daily_daylength_only.rds")


#june 21 is yday 172, summer solstice
daylength_solstice <- dayl %>% 
  filter(yday == 172) %>% 
  left_join(md %>% select(SiteID = Site, Latitude)) %>% 
  group_by(SiteID, Latitude) %>% 
  summarize(mndl = mean(daylen))


#no variability by year
dayl %>% 
  filter(grepl("cik|CIK", SiteID), yday == 172) %>% 
  ggplot() +
  geom_line(aes(x = year, y = daylen, group = SiteID))
  
daylength_solstice %>% 
  ggplot() +
  geom_point(aes(x = Latitude, y = mndl*60)) 

daylength_solstice %>% 
  ggplot() +
  geom_histogram(aes(x = mndl)) +
  labs(title = "Daylength on June 21st across AKSSF sites", x = "daylength")
ggsave("output/daylength_histogram.jpeg")

dayl %>% 
  filter(SiteID %in% c("kdk_olgcr01a", "CIK_41"), year == 2000) %>% 
  ggplot() +
  geom_line(aes(x =yday, y = daylen, color = SiteID)) +
  labs(title = "Daylength from June 1 to Oct 1 for 57 and 63 degress Latitude")
ggsave("output/daylength_plot.jpeg")

dayl %>% filter(SiteID == "CIK_11", year == 2000) #%>% 
  ggplot(aes(x = yday, y = value/60)) +
  geom_point()
  
md %>% filter(Site == "CIK_11")


dayl %>% summary()

```


# read and check saved data

Note, Sue has a site in BB that doesn't have any summer data - Big Creek in Naknek. Seems like she started it in fall 2016, recovered a little bit of data and that was it - confirmed on her metadata sheet.

```{r}
md <- read_csv("data_preparation/final_data/md.csv")

dat <- read_csv("data_preparation/final_data/summer_data_wair2021-11-23.csv")

md
dat %>% distinct(SiteID)

#all there
left_join(dat %>% distinct(SiteID), md %>% select(SiteID, Region) %>% mutate(md = 1)) %>% filter(is.na(md))

sites485 <- md %>% distinct(SiteID) %>% arrange(SiteID) %>% pull(SiteID)

pdf("data_preparation/final_data/air-stream_temps.pdf")

for(i in sites485) {
  plot.dat <- left_join(dat, md %>% select(SiteID, Region)) %>%
    filter(SiteID %in% i) %>% 
    mutate(jd = format(sampleDate, "%j"), year = year(sampleDate)) %>% 
    filter(jd >151, jd < 244)   #june 1 to aug 31
  if(nrow(plot.dat) > 0) {
    p1 <- plot.dat %>% 
      ggplot() +
      geom_line(aes(x = sampleDate, y = meanDT)) +
      geom_line(aes(x = sampleDate, y = airDT), color = "blue") +
      facet_wrap(~year, scales = "free_x") +
      labs(title = i)
    plot(p1)
  } else {
    (print(paste("missing data for ", i)))
  }
}

dev.off()

```

# Updated stream-air data frame 3/1/22

Discovered that Newhalen 2015 was bad after running thermal metrics. Also one air temperature spike in maxes, but that might be due to using UW database dailies in thermal regime dataset we reviewed - everything looks fine in daily means from Tim. Dustin and I reviewed the max min means plots for all sites in dataset. Lots of suspected burials but hard to know for sure.

- Also removing 4 sites that have very low thermal sensitivities some were in lakes and one had one year of data and we did not calculate spatial predictors for.
- Adding in daylength as secondary covariate for DFA. Will add in site specific and also an average across all sites.

```{r}
final_data <- readRDS("data_preparation/final_data/summer_data_wair2021-11-23.rds")

# hardenburg bay looks like a lake site that was missed, two usgs sites are lake outlets, one is below a dam and other is outlet of kenai lake, very lake influenced, nerka pike creek we don't have spatial data for, only 1 year of stream temps anyways.

drop_sites <- c("NPS_Hardenburg_Bay", "usgs_15258000", "usgs_15260001", "UW_Nerka Pike Creek")

#3960
final_data %>% 
  filter((SiteID == "NPS_Newhalen_River" & year(sampleDate) == 2015)|(SiteID %in% drop_sites)) %>% 
  count(SiteID)

# 237898 - 3960 = 233938
final_data2 <- final_data %>% 
  filter(!(SiteID == "NPS_Newhalen_River" & year(sampleDate) == 2015),
         !(SiteID %in% drop_sites))
         
#add in site-specific daylength
final_data3 <- left_join(final_data2, dayl %>% dplyr::select(SiteID, sampleDate, daylen_min))

final_data4 <- left_join(final_data3, dayl %>% 
                           filter(SiteID == "CIK_11") %>% #has approx. mean lat and long across all sites
                           dplyr::select(sampleDate, global_dayl = daylen_min))

saveRDS(final_data4, paste0("data_preparation/final_data/summer_data_wair_dayl", Sys.Date(), ".rds"))
write_csv(final_data4, file = paste0("data_preparation/final_data/summer_data_wair_dayl", Sys.Date(), ".csv"))
```

# Updated data frame with all air temperatures from June 1 - August 31

Tim would like complete air temperature time series for each site and year for the DFA.

```{r all air temps and daylength}
final_data4 <- readRDS("data_preparation/final_data/summer_data_wair_dayl2022-03-01.rds")

daymet <- read_csv("data_preparation/daymet/site_daymet.csv")

air <- daymet %>%
  filter(measurement %in% c("tmax..deg.c.", "tmin..deg.c.")) %>% 
  mutate(sampleDate = as.Date(paste(year, yday), format = "%Y %j")) %>% 
  filter(month(sampleDate) %in% 6:9) %>% 
  rename(SiteID = site) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(airDT = mean(value))

dayl <- daymet %>%
  filter(measurement == "dayl..s.") %>% 
  mutate(sampleDate = as.Date(paste(year, yday), format = "%Y %j"),
         daylen = value/3600,
         daylen_min = value/60) %>% 
  filter(month(sampleDate) %in% 6:9) %>% 
  rename(SiteID = site) %>% 
  select(SiteID, sampleDate, daylen, daylen_min)

#add in global daylength for site with mean latitude
dayl <- left_join(dayl, dayl %>% 
  filter(SiteID == "CIK_11") %>% #has approx. mean lat and long across all sites
  dplyr::select(sampleDate, global_dayl = daylen_min))

rm(daymet)

#need to filter daymet data so that is for just the sites and years we have data for (but complete summers)
site_yr <- final_data4 %>% distinct(SiteID, year = year(sampleDate))
daymet_merge <- left_join(air, dayl) %>% 
  mutate(year = year(sampleDate)) %>% 
  right_join(site_yr)

final_data5 <- left_join(daymet_merge, final_data4) #now based on dates in the daymet file

# I left in September because I'm not sure how we cutoff the original water temperatures. The filter to August may have been later by Tim.
final_data5 %>% 
  ungroup() %>% 
  filter(is.na(meanDT)) %>% 
  count(month(sampleDate))
summary(final_data5)

saveRDS(final_data5, paste0("data_preparation/final_data/summer_data_wair_dayl", Sys.Date(), ".rds"))
write_csv(final_data5, file = paste0("data_preparation/final_data/summer_data_wair_dayl", Sys.Date(), ".csv"))

```


