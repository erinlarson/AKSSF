---
title: "data_combine_all"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
knitr::opts_knit$set(root.dir = normalizePath("..")) 

# load packages
library(googledrive)
library(lubridate)
library(readr)
library(hms)
library(ggmap)
library(sf)
library(stars)
# library(leaflet)
# library(osmdata)
library(broom)
library(caTools)
library(tidyverse)
library(daymetr)

# install.packages("devtools")
# devtools::install_github("yutannihilation/ggsflabel")
library(ggsflabel)

```


# Metadata

Bring in the metadata for each dataset. Export a shapefile so that we can check site locations in ArcGIS and also intersect to get catchment IDs. The site locations are used to extract DAYMET air temperatures in the air tempertures rmd.

```{r read in metadata}
gd.akssf.files <- drive_ls(path = "https://drive.google.com/drive/u/0/folders/1_qtmORSAow1fIxvh116ZP7oKd_PC36L0")

gd.akssf.files %>% 
  arrange(name)

gd.metadata.files <- gd.akssf.files %>% 
  filter(grepl("Metadata", name) & grepl(".csv", name))

gd.metadata.files %>% 
  arrange(name)

folder <- "data_preparation/final_data/Metadata/"

#note may need to redirect to root dir to get this to run. Doesn't always work after just running line in setup chunk.
for (i in seq_along(gd.metadata.files$name)) {
  drive_download(as_id(gd.metadata.files$id[i]),
                 path = str_c(folder, gd.metadata.files$name[i]),
                 overwrite = TRUE)
}

local.md.files <- list.files(folder, full.names = TRUE)

md <- map_df(local.md.files, function(x) read_csv(x, col_types = "ccccccccccccccc") %>%
                      mutate(file_name = basename(x))) %>% 
  mutate(Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude))

md <- md %>% 
  mutate(SiteID = case_when(is.na(SiteID) ~ Agency_ID,
                            TRUE ~ SiteID))

md %>% dplyr::select(SiteID, file_name)
```


Convert to sf object and save as a shapefile.
NOT RUN, this was exported previously so we could shift sites to correct locations in ArcGIS.

```{r eval = FALSE}
md_sf <- st_as_sf(md, coords = c("Longitude", "Latitude"), crs = "wgs84")

akssf_sa <- st_read(dsn = "W:/GIS/AKSSF Southcentral/AKSSF_Hydrography.gdb", layer = "AKSSF_studyarea_HUC8")
akssf_sa_wgs84 <- st_transform(akssf_sa, crs = "wgs84")
st_crs(akssf_sa_wgs84) == st_crs(md_sf)
md_sf <- st_join(md_sf, akssf_sa_wgs84 %>% select(HUC8, Name))

ggplot() +
  geom_sf(data = akssf_sa) +
  geom_sf(data = md_sf, aes(color = HUC8 > 1903)) 

#remove sites in bb and kusko
md_sf_nobb <- md_sf %>% filter(!grepl("1903", HUC8)) %>% filter(!is.na(HUC8)) 

#check
ggplot() +
  geom_sf(data = akssf_sa, aes(color = HUC8 > 1903)) +
  geom_sf(data = md_sf_nobb) 

st_write(md_sf_nobb, "output/sites_outside_bb.shp", append = FALSE)
```

# Get regions and huc8 on metadata file

```{r create metadata sf}
md_sf <- st_as_sf(md, coords = c("Longitude", "Latitude"), crs = "WGS84")
```

Read in HUC8s and reproject.

```{r add huc8 names to md_sf}
huc8 <- st_read(dsn = "S:/Leslie/GIS/NHD Harmonized/WBD_19_GDB.gdb", layer = "WBDHU8")
st_crs(huc8)
huc8_wgs84 <- st_transform(huc8, crs = "WGS84")

st_crs(md_sf) == st_crs(huc8_wgs84)

md_sf <- st_join(md_sf, huc8_wgs84)

ggplot() +
  geom_sf(data = md_sf, aes(color = Name))

```

Remove Kuskokwim Delta sites from md and md_sf.

```{r remove kuskokwim sites}
kusko <- md_sf %>% filter(Name == "Kuskokwim Delta") %>% pull(SiteID)

md <- md %>% 
  filter(!SiteID %in% kusko)

md_sf <- md_sf %>% 
  filter(!SiteID %in% kusko)

```

Add in a region - Bristol Bay, Cook Inlet, PWS, Kodiak, or Copper. 

* Copper 19020101-19020104
* PWS    19020201-19020203
* CI     19020301-19020602
* Kodiak 19020701 - just this one?
* BB     19030202-19030306


```{r add region to md}
md_sf %>% 
  st_drop_geometry() %>% 
  distinct(Name, HUC8) %>% 
  arrange(HUC8)

md_sf <- md_sf %>% 
  mutate(Region = case_when(HUC8 %in% 19020301:19020602 ~ "Cook Inlet",
                            HUC8 == 19020701 ~ "Kodiak",
                            HUC8 %in% 19020101:19020104 ~ "Copper",
                            HUC8 %in% 19020201:19020203 ~ "Prince William Sound",
                            TRUE ~ "Bristol Bay"))

md <- left_join(md, st_drop_geometry(md_sf) %>% distinct(SiteID, HUC8, Name, Region))
```



# Data

Bring in the data files and exclude files associated with lakes.

```{r create daily data frame}

gd.daily.files <- gd.akssf.files %>% 
  filter(grepl("Daily_Data", name) & grepl(".csv", name), !grepl("lakes", name))

gd.daily.files %>% 
  arrange(name)

folder <- "data_preparation/final_data/Daily_Data/"

for (i in seq_along(gd.daily.files$name)) {
  drive_download(as_id(gd.daily.files$id[i]),
                 path = str_c(folder, gd.daily.files$name[i]),
                 overwrite = TRUE)
}

local.daily.files <- list.files(folder, full.names = TRUE)

ddat <- map_df(local.daily.files, function(x) read_csv(x, col_types = "cccccc") %>%
                      mutate(file_name = basename(x))) 

ddat <- ddat %>% 
  mutate(sampleDate = as.Date(sampleDate),
         minDT = as.numeric(minDT),
         maxDT = as.numeric(maxDT),
         meanDT = case_when(is.na(meanDT) ~ (minDT + maxDT)/2,
                            TRUE ~ as.numeric(meanDT)))

summary(ddat)

```

Clean up daily file for analysis. Some USGS data is missing the mean daily temperatures or minimum daily temperatures. Tim Cline's data only have the mean daily temperatures -- they were downloaded as summary data from Zenodo.

```{r}
ddat %>% filter(is.na(meanDT)) %>% distinct(file_name)

ddat %>% filter(is.na(minDT)) %>% distinct(file_name)

ddat <- ddat %>% 
  filter(!is.na(meanDT))

#remove kuskokwim delta sites
ddat <- ddat %>% 
  filter(!SiteID %in% kusko)

write_csv(ddat, paste0("data_preparation/final_data/daily_data", Sys.Date()))
```


# Check SiteIDs and save metadata

Check that all sites with data have entries in md files.

All sites with data have entries in metadata, but two sites in data that are classified as lakes in metadata at Solf Lake. Include them in metadata for now because both are at streams. 

Note one stream site in md without data that can be dropped. kdk_olgcr01b. This was an entry in akoats for a duplicate logger
that never should have been there. Same lat/long and same site. Only provided one time series for this site
and so site name in data matches kdk_olgcr01a. 

These can be resolved by filtering to only select metadata entries for SiteIDs in the data. But, there are multiple entries for some sites in the metadata that should be fixed:

* Asked Dustin to fix duplicate entries in Little Susitna network, some are actual duplicates and others are from sites that we are giving the same SiteID but have slightly different locations between 2016/2020.
* Note that there are duplicates for NPS lake outlets because I combined level and temp loggers into one site id since they are essentially the same site. Filter to just select the temp sites: 



```{r compare md and ddat}

ddat %>% 
  distinct(SiteID) %>% 
  left_join(md %>% mutate(md = 1) %>% dplyr::select(SiteID, md, Waterbody_type)) %>% 
  filter(Waterbody_type == "L")

nrow(ddat %>% distinct(SiteID))

#extra entries in md mostly at lakes except one which we don't need.
md %>% left_join(ddat %>% distinct(SiteID) %>% mutate(dat = 1)) %>% 
  filter(is.na(dat)) %>% 
  arrange(SiteID) %>% 
  dplyr::select(SiteID, Waterbody_type)

#not working because we have duplicate entries for some siteids.
left_join(ddat %>% distinct(SiteID), md) %>% 
  count(SiteID) %>% 
  arrange(desc(n))

md %>% filter(SiteID == "nps_KATM_naknlo") # remove seq_id 1631, 1470
md %>% filter(SiteID == "nps_LACL_lclaro") # remove seq_id 1633, 1476
md %>% filter(SiteID == "nps_KATM_lbrooo") # remove seq_id 1441
md %>% filter(SiteID == "nps_LACL_kijilo") # remove seq_id 1635
md %>% filter(SiteID == "BM7") # remove seq_id 1841

remove_seqids <- c(1631, 1470, 1633, 1476, 1441, 1635, 1841)

#want 485 metadata entries to match unique SiteIDs in data.
md_final <- left_join(ddat %>% distinct(SiteID), md %>% filter(!seq_id %in% remove_seqids))  

```

Chunk above run and everything saved on 6/14/21. Note, resaved on 11/23/21 because found error in Little Susitna site names. Also fixed all the duplicate md entries so now just 1 SiteID in the data to match 1 SiteID in the metadata.

```{r save metadata}
saveRDS(md_final, file = "data_preparation/final_data/md.rds")
write_csv(md_final, path = "data_preparation/final_data/md.csv")
saveRDS(md_sf %>% right_join(md_final %>% dplyr::select(SiteID)), file = "data_preparation/final_data/md_sf.rds")

```

# Merge with air temperaures


Merge with batched air temperatures.

```{r merge with air temps}

daymet <- read_csv("data_preparation/daymet/site_daymet.csv")
daymet %>% distinct(measurement)

#correct conversion is date2 bc date example below has Jan 1 as day 0!
daymet %>% 
  filter(year == 2006, 
         yday == 152) %>% 
  mutate(date = as.Date(yday, origin = paste0(year, "-01-01")),
         date2 = as.Date(paste(year, yday), format = "%Y %j"))

air <- daymet %>%
  filter(measurement %in% c("tmax..deg.c.", "tmin..deg.c.")) %>% 
  mutate(sampleDate = as.Date(paste(year, yday), format = "%Y %j")) %>% 
  filter(month(sampleDate) %in% 6:9) %>% 
  rename(SiteID = site) %>% 
  group_by(SiteID, sampleDate) %>% 
  summarize(airDT = mean(value))

rm(daymet)

# ddat <- read_csv("data_preparation/final_data/daily_data2021-06-15")

left_join(ddat %>% distinct(SiteID), air %>% distinct(SiteID) %>% mutate(air = 1)) %>% count(air)

#half the data are outside summer window.
ddat %>% 
  count(summer = month(sampleDate) %in% 6:9,
        SiteID) %>% 
  arrange(SiteID)

sumdat <- left_join(ddat %>% filter(month(sampleDate) %in% 6:9,
                                    year(sampleDate) %in% 1980:2020), 
                    air)

summary(sumdat)

```

Save summer data frame for report on github page -- summary_report.rmd

```{r}
saveRDS(sumdat, paste0("data_preparation/final_data/summer_data_wair", Sys.Date(), ".rds"))
write_csv(sumdat, path = paste0("data_preparation/final_data/summer_data_wair", Sys.Date(), ".csv"))
```

# QA 2019 temperatures for Deshka
