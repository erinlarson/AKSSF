{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Network/Flow Accumulation Covariates\n",
    "Covariates to be calculated along across the flow accumulation grids:\n",
    "1. fac_glacier_per - % Glacier along flow accumulation = glacier weighted fac/fac\n",
    "2. fac_lake_per\t- % Lake along flow accumulation = lake weighted fac/fac\n",
    "3. fac_mn_slope - Mean slope contributing area = slope weighted fac/fac\n",
    "4. fac_north_per - % North along flow accumulation\tnorth weighted fac/fac\n",
    "5. fac_wetland_per - % Wetland along flow accumulation = wetland weighted fac/fac\n",
    "6. fac_mn_lcld_xxxx - Mean day of year for LCLD (by year) along flow accumulation = mdy(year) weighted fac/fac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports complete\n",
      "sys paths ['C:\\\\Users\\\\dwmerrigan\\\\Documents\\\\GitHub\\\\AKSSF\\\\data_preparation\\\\sensitivity_drivers\\\\geomorphology', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\Resources\\\\ArcPy', 'C:\\\\Users\\\\dwmerrigan\\\\Documents\\\\GitHub\\\\AKSSF', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\python37.zip', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\DLLs', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3', '', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\Resources\\\\ArcToolbox\\\\Scripts', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\Resources\\\\ArcToolBox\\\\Scripts', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\Resources\\\\ArcToolBox\\\\Scripts\\\\archydro', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\Resources\\\\ArcToolBox\\\\Scripts\\\\GRAIP', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\future-0.18.2-py3.7.egg', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\pytz-2020.1-py3.7.egg', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\pywin32_ctypes-0.2.0-py3.7.egg', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\pywin32security', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\sympy-1.5.1-py3.7.egg', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\dwmerrigan\\\\.ipython']\n",
      "2022-01-27 16:14:06.643594\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Import modules\n",
    "import datetime\n",
    "import arcpy\n",
    "import sys\n",
    "import os\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "\n",
    "print('imports complete')\n",
    "print(f'sys paths {sys.path}')\n",
    "print (datetime.datetime.now())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collect datasets\n",
    "Store the following in a data dictionary using region as the key\n",
    "1. Flow accumulation grids\n",
    "2. Slope - Slope in degrees - float\n",
    "3. Wetlands - Binary raster data\n",
    "4. Lakes/Ponds - Feature class convert to raster and\n",
    "5. Glaciers - Binary raster data\n",
    "6. North - Binary raster data\n",
    "7. LCLD rasters - integertype\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Function to add key, value pairs to dictionary\n",
    "def append_value(dict_obj, key, value):\n",
    "    # Check if key exist in dict or not\n",
    "    if key in dict_obj:\n",
    "        # Key exist in dict.\n",
    "        # Check if type of value of key is list or not\n",
    "        if not isinstance(dict_obj[key], list):\n",
    "            # If type is not list then make it list\n",
    "            dict_obj[key] = [dict_obj[key]]\n",
    "        # Append the value in list\n",
    "        dict_obj[key].append(value)\n",
    "    else:\n",
    "        # As key is not in dict,\n",
    "        # so, add key-value pair\n",
    "        dict_obj[key] = value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\GIS\\\\AKSSF\\\\Bristol_Bay', 'D:\\\\GIS\\\\AKSSF\\\\Cook_Inlet', 'D:\\\\GIS\\\\AKSSF\\\\Copper_River', 'D:\\\\GIS\\\\AKSSF\\\\Kodiak', 'D:\\\\GIS\\\\AKSSF\\\\Prince_William_Sound']\n",
      "Scratch folder U:\\\\ca_Outputs already exists\n",
      "----------\n",
      "Scratch folder U:\\\\ca_Outputs\\\\ca_scratch.gdb already exists\n",
      "----------\n",
      "Scratch folder for Bristol_Bay already created at U:\\\\ca_Outputs\\Bristol_Bay_ca\n",
      "----------\n",
      "Scratch folder for Cook_Inlet already created at U:\\\\ca_Outputs\\Cook_Inlet_ca\n",
      "----------\n",
      "Scratch folder for Copper_River already created at U:\\\\ca_Outputs\\Copper_River_ca\n",
      "----------\n",
      "Scratch folder for Kodiak already created at U:\\\\ca_Outputs\\Kodiak_ca\n",
      "----------\n",
      "Scratch folder for Prince_William_Sound already created at U:\\\\ca_Outputs\\Prince_William_Sound_ca\n",
      "----------\n",
      "All scratch workspaces set\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'Bristol_Bay': 'U:\\\\\\\\ca_Outputs\\\\Bristol_Bay_ca',\n 'Cook_Inlet': 'U:\\\\\\\\ca_Outputs\\\\Cook_Inlet_ca',\n 'Copper_River': 'U:\\\\\\\\ca_Outputs\\\\Copper_River_ca',\n 'Kodiak': 'U:\\\\\\\\ca_Outputs\\\\Kodiak_ca',\n 'Prince_William_Sound': 'U:\\\\\\\\ca_Outputs\\\\Prince_William_Sound_ca'}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "# Set data dir equal to directory containing the AKSSF regional sub-folders.\n",
    "data_dir = r\"D:\\\\GIS\\\\AKSSF\"\n",
    "arcpy.env.workspace = data_dir\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "# Create dictionary to store scratch workspaces\n",
    "scrDict = {}\n",
    "# Regions to be processed\n",
    "rList = ['Kodiak','Prince_William_Sound','Copper_River','Bristol_Bay','Cook_Inlet']\n",
    "regions = arcpy.ListWorkspaces(workspace_type=\"Folder\")\n",
    "regions = [os.path.normpath(r) for r in regions if os.path.basename(r) in rList]\n",
    "\n",
    "#working gdb to save intermediate outputs\n",
    "ca_scratchgdb = r\"U:\\\\ca_Outputs\\\\ca_scratch.gdb\"\n",
    "ca_scratchfol = r\"U:\\\\ca_Outputs\"\n",
    "print (regions)\n",
    "\n",
    "#Create rvbd scratch folders if they do not already exist\n",
    "import os, arcpy\n",
    "\n",
    "\n",
    "if not arcpy.Exists(ca_scratchfol):\n",
    "    os.mkdir(ca_scratchfol)\n",
    "    print(f'Creating scratch folder {ca_scratchfol}')\n",
    "else:\n",
    "    print(f'Scratch folder {ca_scratchfol} already exists')\n",
    "print('----------')\n",
    "\n",
    "if not arcpy.Exists(ca_scratchgdb):\n",
    "    arcpy.CreateFileGDB_management(ca_scratchfol,'ca_scratch.gdb')\n",
    "    print(f'Creating scratch gdb {ca_scratchgdb}')\n",
    "else:\n",
    "    print(f'Scratch folder {ca_scratchgdb} already exists')\n",
    "print('----------')\n",
    "\n",
    "#Create Regional output folders\n",
    "for region in regions:\n",
    "    rname = os.path.basename(region)\n",
    "    cascratchpath = os.path.join(ca_scratchfol, rname + '_ca')\n",
    "    append_value(scrDict,rname,cascratchpath)\n",
    "    if not arcpy.Exists(cascratchpath):\n",
    "        os.mkdir(cascratchpath)\n",
    "        print (f'{rname} does not have a scratch folder')\n",
    "        print (f'Creating contributing area scratch folder at  {cascratchpath}')\n",
    "    else:\n",
    "        print(f'Scratch folder for {rname} already created at {cascratchpath}')\n",
    "\n",
    "    print('----------')\n",
    "print(f'All scratch workspaces set')\n",
    "scrDict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Iterate through regions and pull out glacier polygons and nhd waterbodies and convert to raster.  Use elev raster as snap raster/mask for these processes\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set data dictionary and create/modify inputs as necessary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bristol_Bay\n",
      "Dissolved watershed dataset already created...\n",
      "Lake dataset D:\\GIS\\AKSSF\\Bristol_Bay\\Bristol_Bay.gdb\\NHDWaterbody_merge set for Bristol_Bay\n",
      "Lakes raster D:\\GIS\\AKSSF\\Bristol_Bay\\lakes.tif already exists...\n",
      "Glacier raster D:\\GIS\\AKSSF\\Bristol_Bay\\glaciers.tif already...\n",
      "INPUTS:\n",
      "Flow accumulation raster input = D:\\GIS\\AKSSF\\Bristol_Bay\\fac.tif and exists = True\n",
      "Flow direction raster input = D:\\GIS\\AKSSF\\Bristol_Bay\\fdr.tif and exists = True\n",
      "Slope input = D:\\GIS\\AKSSF\\Bristol_Bay\\slope.tif and exists = True\n",
      "North raster input = D:\\GIS\\AKSSF\\Bristol_Bay\\north.tif and exists = True\n",
      "Wetlands raster input = D:\\GIS\\AKSSF\\Bristol_Bay\\wetlands.tif and exists = True\n",
      "Lakes raster input = D:\\GIS\\AKSSF\\Bristol_Bay\\lakes.tif and exists = True\n",
      "Glaciers raster input = D:\\GIS\\AKSSF\\Bristol_Bay\\glaciers.tif and exists = True\n",
      "Catchments input = D:\\GIS\\AKSSF\\Bristol_Bay\\Bristol_Bay.gdb\\cats_intersect and exists = True\n",
      "Watersheds to process input = D:\\GIS\\AKSSF\\Bristol_Bay\\Bristol_Bay.gdb\\wtds_dis and exists = True\n",
      "Process time for Bristol_Bay complete - Elapsed time: (0:00:01)\n",
      "----------\n",
      "Cook_Inlet\n",
      "Dissolved watershed dataset already created...\n",
      "Lake dataset D:\\GIS\\AKSSF\\Cook_Inlet\\Cook_Inlet.gdb\\NHDWaterbody_merge set for Cook_Inlet\n",
      "Lakes raster D:\\GIS\\AKSSF\\Cook_Inlet\\lakes.tif already exists...\n",
      "Glacier raster D:\\GIS\\AKSSF\\Cook_Inlet\\glaciers.tif already...\n",
      "INPUTS:\n",
      "Flow accumulation raster input = D:\\GIS\\AKSSF\\Cook_Inlet\\fac.tif and exists = True\n",
      "Flow direction raster input = D:\\GIS\\AKSSF\\Cook_Inlet\\fdr.tif and exists = True\n",
      "Slope input = D:\\GIS\\AKSSF\\Cook_Inlet\\slope.tif and exists = True\n",
      "North raster input = D:\\GIS\\AKSSF\\Cook_Inlet\\north.tif and exists = True\n",
      "Wetlands raster input = D:\\GIS\\AKSSF\\Cook_Inlet\\wetlands.tif and exists = True\n",
      "Lakes raster input = D:\\GIS\\AKSSF\\Cook_Inlet\\lakes.tif and exists = True\n",
      "Glaciers raster input = D:\\GIS\\AKSSF\\Cook_Inlet\\glaciers.tif and exists = True\n",
      "Catchments input = D:\\GIS\\AKSSF\\Cook_Inlet\\Cook_Inlet.gdb\\cats_intersect and exists = True\n",
      "Watersheds to process input = D:\\GIS\\AKSSF\\Cook_Inlet\\Cook_Inlet.gdb\\wtds_dis and exists = True\n",
      "Process time for Cook_Inlet complete - Elapsed time: (0:00:00)\n",
      "----------\n",
      "Copper_River\n",
      "Dissolved watershed dataset already created...\n",
      "Lake dataset D:\\GIS\\AKSSF\\Copper_River\\Copper_River.gdb\\NHDWaterbody_merge set for Copper_River\n",
      "Lakes raster D:\\GIS\\AKSSF\\Copper_River\\lakes.tif already exists...\n",
      "Glacier raster D:\\GIS\\AKSSF\\Copper_River\\glaciers.tif already...\n",
      "INPUTS:\n",
      "Flow accumulation raster input = D:\\GIS\\AKSSF\\Copper_River\\fac.tif and exists = True\n",
      "Flow direction raster input = D:\\GIS\\AKSSF\\Copper_River\\fdr.tif and exists = True\n",
      "Slope input = D:\\GIS\\AKSSF\\Copper_River\\slope.tif and exists = True\n",
      "North raster input = D:\\GIS\\AKSSF\\Copper_River\\north.tif and exists = True\n",
      "Wetlands raster input = D:\\GIS\\AKSSF\\Copper_River\\wetlands.tif and exists = True\n",
      "Lakes raster input = D:\\GIS\\AKSSF\\Copper_River\\lakes.tif and exists = True\n",
      "Glaciers raster input = D:\\GIS\\AKSSF\\Copper_River\\glaciers.tif and exists = True\n",
      "Catchments input = D:\\GIS\\AKSSF\\Copper_River\\Copper_River.gdb\\cats_intersect and exists = True\n",
      "Watersheds to process input = D:\\GIS\\AKSSF\\Copper_River\\Copper_River.gdb\\wtds_dis and exists = True\n",
      "Process time for Copper_River complete - Elapsed time: (0:00:00)\n",
      "----------\n",
      "Kodiak\n",
      "Dissolved watershed dataset already created...\n",
      "Lake dataset D:\\GIS\\AKSSF\\Kodiak\\Kodiak.gdb\\NHDWaterbody_merge set for Kodiak\n",
      "Lakes raster D:\\GIS\\AKSSF\\Kodiak\\lakes.tif already exists...\n",
      "Glacier raster D:\\GIS\\AKSSF\\Kodiak\\glaciers.tif already...\n",
      "INPUTS:\n",
      "Flow accumulation raster input = D:\\GIS\\AKSSF\\Kodiak\\fac.tif and exists = True\n",
      "Flow direction raster input = D:\\GIS\\AKSSF\\Kodiak\\fdr.tif and exists = True\n",
      "Slope input = D:\\GIS\\AKSSF\\Kodiak\\slope.tif and exists = True\n",
      "North raster input = D:\\GIS\\AKSSF\\Kodiak\\north.tif and exists = True\n",
      "Wetlands raster input = D:\\GIS\\AKSSF\\Kodiak\\wetlands.tif and exists = True\n",
      "Lakes raster input = D:\\GIS\\AKSSF\\Kodiak\\lakes.tif and exists = True\n",
      "Glaciers raster input = D:\\GIS\\AKSSF\\Kodiak\\glaciers.tif and exists = True\n",
      "Catchments input = D:\\GIS\\AKSSF\\Kodiak\\Kodiak.gdb\\cats_intersect and exists = True\n",
      "Watersheds to process input = D:\\GIS\\AKSSF\\Kodiak\\Kodiak.gdb\\wtds_dis and exists = True\n",
      "Process time for Kodiak complete - Elapsed time: (0:00:00)\n",
      "----------\n",
      "Prince_William_Sound\n",
      "Dissolved watershed dataset already created...\n",
      "Lake dataset D:\\GIS\\AKSSF\\Prince_William_Sound\\Prince_William_Sound.gdb\\NHDWaterbody_merge set for Prince_William_Sound\n",
      "Lakes raster D:\\GIS\\AKSSF\\Prince_William_Sound\\lakes.tif already exists...\n",
      "Glacier raster D:\\GIS\\AKSSF\\Prince_William_Sound\\glaciers.tif already...\n",
      "INPUTS:\n",
      "Flow accumulation raster input = D:\\GIS\\AKSSF\\Prince_William_Sound\\fac.tif and exists = True\n",
      "Flow direction raster input = D:\\GIS\\AKSSF\\Prince_William_Sound\\fdr.tif and exists = True\n",
      "Slope input = D:\\GIS\\AKSSF\\Prince_William_Sound\\slope.tif and exists = True\n",
      "North raster input = D:\\GIS\\AKSSF\\Prince_William_Sound\\north.tif and exists = True\n",
      "Wetlands raster input = D:\\GIS\\AKSSF\\Prince_William_Sound\\wetlands.tif and exists = True\n",
      "Lakes raster input = D:\\GIS\\AKSSF\\Prince_William_Sound\\lakes.tif and exists = True\n",
      "Glaciers raster input = D:\\GIS\\AKSSF\\Prince_William_Sound\\glaciers.tif and exists = True\n",
      "Catchments input = D:\\GIS\\AKSSF\\Prince_William_Sound\\Prince_William_Sound.gdb\\cats_intersect and exists = True\n",
      "Watersheds to process input = D:\\GIS\\AKSSF\\Prince_William_Sound\\Prince_William_Sound.gdb\\wtds_dis and exists = True\n",
      "Process time for Prince_William_Sound complete - Elapsed time: (0:00:00)\n",
      "----------\n",
      "Process complete 0:00:03.655497\n",
      "----------\n",
      "Data Dictionary created\n"
     ]
    }
   ],
   "source": [
    "import arcpy, os, datetime, time\n",
    "from arcpy.sa import *\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "# set pp factor\n",
    "arcpy.env.parallelProcessingFactor = \"80%\"\n",
    "\n",
    "# Path to lakes data for regions without NHDPlus data\n",
    "sourcelakes = r\"D:\\\\Basedata\\\\NHD_H_Alaska_State_GDB.gdb\\\\Hydrography\\\\NHDWaterbody\"\n",
    "\n",
    "# List to store fac grids for data that need to be iterated on grid by grid basis (mdy LCLD)\n",
    "fdr_list = []\n",
    "fac_list = []\n",
    "\n",
    "caDict = {}\n",
    "\n",
    "# Walk through gdbs and set input data sets. Create necessary raster datasets if they do not already exist.\n",
    "akssf_gdb = r'D:\\GIS\\AKSSF_land_met\\AKSSF_land_met.gdb'\n",
    "start = datetime.datetime.now()\n",
    "for region in regions:\n",
    "    # Start iter timing function\n",
    "    iteration_start = time.time()\n",
    "    rname = os.path.basename(region)\n",
    "    print(rname)\n",
    "    arcpy.env.workspace = region\n",
    "    arcpy.env.snapRaster = os.path.join(region, 'fac.tif')\n",
    "    arcpy.env.cellSize = os.path.join(region, 'fac.tif')\n",
    "    arcpy.env.mask = os.path.join(region, 'fac.tif')\n",
    "    gdb = arcpy.ListWorkspaces(workspace_type='FileGDB')\n",
    "    outgdb = gdb[0]\n",
    "    walk = arcpy.da.Walk(region, topdown = True, datatype=['RasterDataset','FeatureClass'])\n",
    "    for dirpath, dirnames, filenames in walk:\n",
    "        for filename in filenames:\n",
    "            # Set glacier polygon fc\n",
    "            if 'glaciers' == filename:\n",
    "                glacfc_path = os.path.join(dirpath, filename)\n",
    "                append_value(caDict,rname,glacfc_path)\n",
    "\n",
    "            # Extract region buffer\n",
    "            elif 'region_buf' == filename:\n",
    "                rbuff_path = os.path.join(dirpath, filename)\n",
    "                append_value(caDict,rname,rbuff_path)\n",
    "\n",
    "            # Set watersheds - Dissolve watershed merge if not already created\n",
    "            elif 'wtds_merge' == filename:\n",
    "                wtdsource = os.path.join(dirpath, filename)\n",
    "                # Dissolve Watersheds - keep as single part\n",
    "                wtd_dis = os.path.join(outgdb,'wtds_dis')\n",
    "                append_value(caDict, rname, wtd_dis)\n",
    "                if not arcpy.Exists(os.path.join(outgdb,'wtds_dis')):\n",
    "                    print('Dissolving watersheds')\n",
    "                    arcpy.Dissolve_management(wtdsource,wtd_dis,None, None, \"SINGLE_PART\", \"DISSOLVE_LINES\")\n",
    "                    #Delete dissolve artifacts\n",
    "                    c1=0\n",
    "                    c2=0\n",
    "                    c1 = arcpy.GetCount_management(wtd_dis)\n",
    "                    print(f'{c1} watersheds in dissolved fc')\n",
    "                    expression = arcpy.AddFieldDelimiters(wtd_dis, \"Shape_Area\") + \" <= 2200\"\n",
    "                    print(expression)\n",
    "                    artifacts = arcpy.SelectLayerByAttribute_management(wtd_dis,\"NEW_SELECTION\",expression)\n",
    "                    c2 = arcpy.GetCount_management(artifacts)\n",
    "                    print(f'{c2} artifacts identified in dissolved fc')\n",
    "                    arcpy.DeleteRows_management(artifacts)\n",
    "                    print('----------')\n",
    "                    arcpy.SelectLayerByAttribute_management(artifacts,\"CLEAR_SELECTION\",expression)\n",
    "                else:\n",
    "                    print(f'Dissolved watershed dataset already created...')\n",
    "\n",
    "            # Extract cats intersect\n",
    "            elif 'cats_intersect' == filename:\n",
    "                cats_path = os.path.join(dirpath, filename)\n",
    "                append_value(caDict,rname,cats_path)\n",
    "\n",
    "            # Set stream polygon fc\n",
    "            elif 'streams_merge' == filename or 'NHDFlowline_merge' == filename:\n",
    "                strsource = os.path.join(dirpath, filename)\n",
    "                append_value(caDict,rname,strsource)\n",
    "\n",
    "            # Set merged watersheds dataset\n",
    "            elif 'wtds_merge' == filename:\n",
    "                wtdsource = os.path.join(dirpath, filename)\n",
    "                # Append watersheds to dictionary\n",
    "                append_value(caDict,rname,wtdsource)\n",
    "\n",
    "            # Set elev raster\n",
    "            elif 'elev.tif' == filename:\n",
    "                elev_path = os.path.join(dirpath, filename)\n",
    "                append_value(caDict,rname,elev_path)\n",
    "\n",
    "            # Set ca raster\n",
    "            elif 'fac.tif' == filename:\n",
    "                fac_path = os.path.join(dirpath, filename)\n",
    "                append_value(caDict,rname,fac_path)\n",
    "                fac_list.append(fac_path)\n",
    "\n",
    "            # Set flow direction raster\n",
    "            elif 'fdr.tif' == filename:\n",
    "                fdr_path = os.path.join(dirpath, filename)\n",
    "                append_value(caDict,rname,fdr_path)\n",
    "                fdr_list.append(fdr_path)\n",
    "\n",
    "            # Set slope raster\n",
    "            elif 'slope.tif' == filename:\n",
    "                slope_path = os.path.join(dirpath, filename)\n",
    "                append_value(caDict,rname,slope_path)\n",
    "\n",
    "            # Set North raster\n",
    "            elif 'north.tif' == filename:\n",
    "                nor_path = os.path.join(dirpath, filename)\n",
    "                append_value(caDict,rname,nor_path)\n",
    "\n",
    "            # Set wetland raster\n",
    "            elif 'wetlands.tif' == filename:\n",
    "                wras_path = os.path.join(dirpath, filename)\n",
    "                append_value(caDict,rname,wras_path)\n",
    "\n",
    "    # Check that waterbody feature class has been copied over to gdb, Originally never created for synthetic regions\n",
    "    lakefc_path = os.path.join(outgdb,'NHDWaterbody_merge')\n",
    "    if not arcpy.Exists(lakefc_path):\n",
    "        lakesubset = arcpy.SelectLayerByLocation_management(sourcelakes,'INTERSECT',rbuff_path,'','SUBSET_SELECTION')\n",
    "        print(f'{arcpy.GetCount_management(lakesubset)} Lake/ponds in {rname}...')\n",
    "        lakefc_path = arcpy.FeatureClassToFeatureClass_conversion(lakesubset, outgdb, 'NHDWaterbody_merge')\n",
    "    else:\n",
    "        print(f'Lake dataset {lakefc_path} set for {rname}')\n",
    "\n",
    "    # Check for lakes raster\n",
    "    lake_path = os.path.join(region, 'lakes.tif')\n",
    "    lakerast = os.path.join(region,'lakesras.tif')\n",
    "    if not arcpy.Exists(lake_path):\n",
    "        print('Lake raster not yet created')\n",
    "        lakeselect = arcpy.SelectLayerByAttribute_management(lakefc_path, 'NEW_SELECTION','FTYPE = 390')\n",
    "        print(f'{arcpy.GetCount_management(lakeselect)} Lake/ponds in source dataset...')\n",
    "        arcpy.PolygonToRaster_conversion(lakeselect,'FType',lakerast)\n",
    "        arcpy.ddd.Reclassify(lakerast, \"Value\", \"390 1;NODATA 0\", lake_path, \"DATA\")\n",
    "        append_value(caDict,rname,lake_path)\n",
    "    else:\n",
    "        print(f'Lakes raster {lake_path} already exists...')\n",
    "        append_value(caDict,rname,lake_path)\n",
    "\n",
    "    # Check for glacier raster\n",
    "    glac_path = os.path.join(region, 'glaciers.tif')\n",
    "    glacrast = os.path.join(region,'glacrast.tif')\n",
    "    if not arcpy.Exists(glac_path):\n",
    "        print('Glacier raster not yet created')\n",
    "        arcpy.PolygonToRaster_conversion(glacfc_path,'O1Region',glacrast)\n",
    "        arcpy.ddd.Reclassify(glacrast, \"Value\", \"1 1;NODATA 0\", glac_path, \"DATA\")\n",
    "        append_value(caDict,rname,glac_path)\n",
    "    else:\n",
    "        print(f'Glacier raster {glac_path} already...')\n",
    "        append_value(caDict,rname,glac_path)\n",
    "    print('INPUTS:')\n",
    "    print(f'Flow accumulation raster input = {fac_path} and exists = {arcpy.Exists(fac_path)}')\n",
    "    print(f'Flow direction raster input = {fdr_path} and exists = {arcpy.Exists(fdr_path)}')\n",
    "    print(f'Slope input = {slope_path} and exists = {arcpy.Exists(slope_path)}')\n",
    "    print(f'North raster input = {nor_path} and exists = {arcpy.Exists(nor_path)}')\n",
    "    print(f'Wetlands raster input = {wras_path} and exists = {arcpy.Exists(wras_path)}')\n",
    "    print(f'Lakes raster input = {lake_path} and exists = {arcpy.Exists(lake_path)}')\n",
    "    print(f'Glaciers raster input = {glac_path} and exists = {arcpy.Exists(glac_path)}')\n",
    "    print(f'Catchments input = {cats_path} and exists = {arcpy.Exists(cats_path)}')\n",
    "    print(f'Watersheds to process input = {wtd_dis} and exists = {arcpy.Exists(wtd_dis)}')\n",
    "\n",
    "\n",
    "    # Stop iteration timer\n",
    "    iteration_stop = time.time()\n",
    "    iter_time = int (iteration_stop - iteration_start)\n",
    "    print(f'Process time for {rname} complete - Elapsed time: ({datetime.timedelta(seconds=iter_time)})')\n",
    "    print('----------')\n",
    "# End timer\n",
    "stop = datetime.datetime.now()\n",
    "print(f'Process complete {stop-start}')\n",
    "print('----------')\n",
    "print('Data Dictionary created')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Begin Creating weighted fac grids\n",
    "\n",
    "Can these be run in parallel?\n",
    "\n",
    "Example from <a href = 'https://www.e-education.psu.edu/geog489/node/2282'>PSU sequential to multiprocessing</a> and <a href='https://www.esri.com/arcgis-blog/products/analytics/analytics/multiprocessing-with-arcgis-raster-analysis/#processing-a-workflow'>Esri talk</a>\n",
    "\n",
    "### Setup _very_ simple timing.\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import arcpy\n",
    "from arcpy.sa import *\n",
    "\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.workspace = r'C:\\489\\PSU_LiDAR'\n",
    "\n",
    "## If our rasters aren't in our filter list then drop them from our list.\n",
    "def filter_list(fileList,filterList):\n",
    "    return[i for i in fileList if any(j in i for j in filterList)]\n",
    "\n",
    "def worker(raster):\n",
    "    ## Note also for performance we're not saving any of the intermediate rasters - they will exist only in memory\n",
    "    ## Fill the DEM to remove any sinks\n",
    "    # This is the initial workspace but to prevent any file locking we'll create a scratch workspace\n",
    "    # just for this raster\n",
    "    arcpy.env.scratchWorkspace = os.path.join(arcpy.env.workspace,'f'+raster.replace(\".img\",\"\"))  # r'C:\\Users\\yourname\\PSU_LiDAR\\f'+raster.replace(\".img\",\"\")\n",
    "    ## and we'll check if that scratch folder exists and if not we'll create it.\n",
    "    if not os.path.exists(arcpy.env.scratchWorkspace):\n",
    "        os.makedirs(arcpy.env.scratchWorkspace)\n",
    "    try:\n",
    "        FilledRaster = Fill(raster)\n",
    "        ## Calculate the Flow Direction (how water runs across the surface)\n",
    "        FlowDirRaster = FlowDirection(FilledRaster)\n",
    "        ## Calculate the Flow Accumulation (where the water accumulates in the surface)\n",
    "        FlowAccRaster = FlowAccumulation(FlowDirRaster)\n",
    "        ## Convert the Flow Accumulation to a Stream Network\n",
    "        ## We're setting an arbitrary threshold of 100 cells flowing into another cell to set it as part of our stream\n",
    "        ## http://pro.arcgis.com/en/pro-app/tool-reference/spatial-analyst/identifying-stream-networks.htm\n",
    "        Streams = Con(FlowAccRaster,1,\"\",\"Value > 100\")\n",
    "        ## Convert the Raster Stream network to a feature class\n",
    "        output_Polyline = raster.replace(\".img\",\".shp\")\n",
    "        arcpy.CheckOutExtension(\"Spatial\")\n",
    "        arcpy.sa.StreamToFeature(Streams,FlowDirRaster,output_Polyline)\n",
    "        arcpy.CheckInExtension(\"Spatial\")\n",
    "    except:\n",
    "        print (\"Errors occured\")\n",
    "        print (arcpy.GetMessages())\n",
    "        arcpy.AddMessage (\"Errors occurred\")\n",
    "        arcpy.AddMessage(arcpy.GetMessages())\n",
    "\n",
    "def mp_handler():\n",
    "\n",
    "    # Ordinarily we would want all of the rasters I'm filtering by a small set for testing & efficiency\n",
    "    # I did this by manually looking up the tile index for the LiDAR and determining an area of interest\n",
    "    # tiles ending in 227, 228, 230, 231, 232, 233, 235, 236\n",
    "    wildCardList = set(['227','228','230','231','232','233','235','236'])\n",
    "\n",
    "    # Get a list of rasters in my folder\n",
    "    rasters = arcpy.ListRasters(\"*\")\n",
    "    new_rasters = filter_list(rasters,wildCardList)\n",
    "\n",
    "    # setup the multiprocessing pool with the number of cores in the current PC\n",
    "    with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n",
    "        # use the map function of pool to call the function worker() and pass it a raster\n",
    "        pool.map(worker,new_rasters)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp_handler()\n",
    "    # Output how long the process took.\n",
    "    arcpy.AddMessage(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    print (\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "### Extract input raster by mask and store in memory to reduce processing time - setting a mask will still process entire extent, just output those cells as No Data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GIS\\AKSSF\\Bristol_Bay\\elev.tif. 0\n",
      "D:\\GIS\\AKSSF\\Bristol_Bay\\fac.tif. 1\n",
      "D:\\GIS\\AKSSF\\Bristol_Bay\\fdr.tif. 2\n",
      "D:\\GIS\\AKSSF\\Bristol_Bay\\north.tif. 3\n",
      "D:\\GIS\\AKSSF\\Bristol_Bay\\slope.tif. 4\n",
      "D:\\GIS\\AKSSF\\Bristol_Bay\\wetlands.tif. 5\n",
      "D:\\GIS\\AKSSF\\Bristol_Bay\\Bristol_Bay.gdb\\streams_merge. 6\n",
      "D:\\GIS\\AKSSF\\Bristol_Bay\\Bristol_Bay.gdb\\region_buf. 7\n",
      "D:\\GIS\\AKSSF\\Bristol_Bay\\Bristol_Bay.gdb\\glaciers. 8\n",
      "D:\\GIS\\AKSSF\\Bristol_Bay\\Bristol_Bay.gdb\\cats_intersect. 9\n",
      "D:\\GIS\\AKSSF\\Bristol_Bay\\Bristol_Bay.gdb\\wtds_dis. 10\n",
      "D:\\GIS\\AKSSF\\Bristol_Bay\\lakes.tif. 11\n",
      "D:\\GIS\\AKSSF\\Bristol_Bay\\glaciers.tif. 12\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for v in caDict['Bristol_Bay']:\n",
    "    print(f'{v}. {c}')\n",
    "    c+=1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cook_Inlet\n",
      "Extent for watershed2 = -13717.395999999717 1021742.3960999995 -9391.659499999136 1027495.2323000003\n",
      "\n",
      "**************************************************************************************************************\n",
      "                 Clipping Flow Direction Grid to Watershed OID = 2 with an area of 10,841.55 sqKm               \n",
      "**************************************************************************************************************\n",
      "Clipping flow direction grid to watershed oid 2\n",
      "Flow Direction for Cook_Inlet_wtd_oid_2 clipped - Elapsed time: (0:00:00)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Clipping flow acc grid to watershed oid 2\n",
      "Flow Accumulation for Cook_Inlet_wtd_oid_2 complete - Elapsed time: (0:00:00)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Begin Process: Glacial weighting (01-27-2022 16:14:34)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Finish glacier weighted flow accumulation at 01-27-2022 16:14:34\n",
      "Finish percent glacier raster 01-27-2022 16:14:35\n",
      "Percent Glacier cover along FAC for Cook_Inlet_wtd_oid_2 complete - Elapsed time: (0:00:00)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Begin Process: Lake Weighting (01-27-2022 16:14:35)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Finish lake weighted flow accumulation at 01-27-2022 16:14:35\n",
      "Finish percent lake raster 01-27-2022 16:14:35\n",
      "Percent lake cover along FAC for Cook_Inlet_wtd_oid_2 complete - Elapsed time: (0:00:00)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Begin Process: Slope Weighting (01-27-2022 16:14:35)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Finish slope weighted flow accumulation at 01-27-2022 16:14:35\n",
      "Finish mean slope raster 01-27-2022 16:14:35\n",
      "Mean slope cover along FAC for Cook_Inlet_wtd_oid_2 complete - Elapsed time: (0:00:00)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Begin Process: North Weighting (01-27-2022 16:14:35)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Finish north weighted flow accumulation at 01-27-2022 16:14:36\n",
      "Finish percent north raster 01-27-2022 16:14:36\n",
      "Percent north cover along FAC for Cook_Inlet_wtd_oid_2 complete - Elapsed time: (0:00:00)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Begin Process: Wetland Weighting (01-27-2022 16:14:36)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Finish wetlands weighted flow accumulation at 01-27-2022 16:14:36\n",
      "Finish percent wetlands raster 01-27-2022 16:14:36\n",
      "Percent wetlands cover along FAC for Cook_Inlet_wtd_oid_2 complete - Elapsed time: (0:00:00)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Begin Process: 2013_lcld_32.tif Weighting (01-27-2022 16:14:36)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "2013_lcld_32\n",
      "Finish lcld weighted flow accumulation at 01-27-2022 16:14:37\n",
      "Finish mean lcld raster 01-27-2022 16:14:37\n",
      "2013 Mean lcld along FAC for Cook_Inlet_wtd_oid_2 complete - Elapsed time: (0:00:00)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Begin Process: 2019_lcld_32.tif Weighting (01-27-2022 16:14:37)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "2019_lcld_32\n",
      "Finish lcld weighted flow accumulation at 01-27-2022 16:14:37\n",
      "Finish mean lcld raster 01-27-2022 16:14:37\n",
      "2019 Mean lcld along FAC for Cook_Inlet_wtd_oid_2 complete - Elapsed time: (0:00:00)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Cook_Inlet fac weights complete - Elapsed time: (0:00:03)\n",
      "----------\n",
      "Extent for watershed7 = 66912.02309999987 1110465.3798999991 74583.01369999908 1116606.4286000002\n",
      "Watershed 7 out of index and not yet processed...\n",
      "Extent for watershed3 = 117397.51610000059 1028737.1897 130416.38409999944 1041312.2329999991\n",
      "Watershed 3 out of index and not yet processed...\n",
      "Extent for watershed4 = 130173.51319999993 1035635.2071000002 142168.2633999996 1046727.6688000001\n",
      "Watershed 4 out of index and not yet processed...\n",
      "Extent for watershed6 = 169179.8139999993 1078191.4207000006 197195.30790000036 1097628.8365000002\n",
      "Watershed 6 out of index and not yet processed...\n",
      "Extent for watershed1 = -40762.621099999174 989592.0218000002 -15571.782199999318 1014854.5878999997\n",
      "Watershed 1 out of index and not yet processed...\n",
      "Extent for watershed13 = 116986.92200000025 1237073.2030999996 152287.87680000067 1258181.0604999997\n",
      "Watershed 13 out of index and not yet processed...\n",
      "Extent for watershed12 = 270266.18090000004 1215402.8198000006 292617.27859999985 1242975.3292999994\n",
      "Watershed 12 out of index and not yet processed...\n",
      "Extent for watershed14 = 217130.0 1236169.9999000002 259105.00009999983 1263760.0000999998\n",
      "Watershed 14 out of index and not yet processed...\n",
      "Extent for watershed8 = 120657.20979999937 1077497.6355000008 176477.15799999982 1151418.0030000005\n",
      "Watershed 8 out of index and not yet processed...\n",
      "Extent for watershed11 = 146537.72609999962 1121982.1122999992 281028.4498999994 1223539.2547999993\n",
      "Watershed 11 out of index and not yet processed...\n",
      "Extent for watershed15 = 38600.0 1257483.1338 396700.0 1536505.0\n",
      "Watershed 15 out of index and not yet processed...\n",
      "Processed 1 of 12 watersheds for Cook_Inlet\n",
      "Cook_Inlet complete - Elapsed time: (0:00:05)\n",
      "All regions complete - Elapsed time: (0:00:05)\n"
     ]
    }
   ],
   "source": [
    "### Begin fac calculations\n",
    "import arcpy, os, datetime, time\n",
    "from time import strftime\n",
    "from arcpy.sa import *\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "# set pp factor\n",
    "arcpy.env.parallelProcessingFactor = \"100%\"\n",
    "\n",
    "# path to parent folder containing all lcld rasters - May run seperately\n",
    "lcldfol = r\"D:\\\\Basedata\\\\LCLD_rasters\"\n",
    "\n",
    "# Dictionary to fac covariate rasters to convert to table/merge\n",
    "facDict = {}\n",
    "\n",
    "# Walk through gdbs and set input data sets. Create necessary raster datasets if they do not already exist.\n",
    "akssf_gdb = r'D:\\GIS\\AKSSF_land_met\\AKSSF_land_met.gdb'\n",
    "process_start = time.time()\n",
    "\n",
    "# Limit to cook inlet for nhd test\n",
    "regions = [\"D:\\\\GIS\\\\AKSSF\\\\Cook_Inlet\"]\n",
    "\n",
    "for region in regions:\n",
    "    rname = os.path.basename(region)\n",
    "    print(rname)\n",
    "    region_time_start = time.time()\n",
    "    # Set inputs from dictionaries\n",
    "    fac_source = caDict[rname][1]\n",
    "    fdr_source = caDict[rname][2]\n",
    "    north_in =caDict[rname][3]\n",
    "    slope_in = caDict[rname][4]\n",
    "    wetlands_in = caDict[rname][5]\n",
    "    cats_in = caDict[rname][9]\n",
    "    wtds_in = caDict[rname][10]\n",
    "    lake_in = caDict[rname][11]\n",
    "    glac_in = caDict[rname][12]\n",
    "    scratch = scrDict[rname]\n",
    "\n",
    "    # Set regional specific environments\n",
    "    arcpy.env.snapRaster = fdr_source\n",
    "    arcpy.env.workspace = region\n",
    "\n",
    "    pcount = 0\n",
    "    # get number of watersheds for region\n",
    "    xr = arcpy.management.GetCount(wtds_in)\n",
    "    # x = int(xr.getOutput(0)) + 1 #index begins at 0\n",
    "    # Change number of watersheds to process if necessary - index starts at 0 so set to 0 if only processing smallest watershed\n",
    "    x = 0\n",
    "    # Iterate over watersheds in dissolved watershed fc and sort by area ascending to run smallest watersheds first\n",
    "    with arcpy.da.SearchCursor(wtds_in,['OID@','SHAPE@','Shape_Area'], sql_clause=(None,'ORDER BY Shape_Area ASC')) as cur:\n",
    "\n",
    "        # Start iter timing function\n",
    "        iteration_start = time.time()\n",
    "        for index, row in enumerate(cur):\n",
    "            geom = row[1]\n",
    "            oid = str(row[0])\n",
    "            wtd_bnd = arcpy.CopyFeatures_management(geom,r'in_memory/wtd_bnd')\n",
    "            desc = arcpy.Describe(wtd_bnd)\n",
    "            extent = (\"{} {} {} {}\".format(desc.extent.XMin, desc.extent.YMin, desc.extent.XMax, desc.extent.YMax)) #pull extent from buffered study area to use as input\n",
    "            print(f'Extent for watershed{oid} = {extent}')\n",
    "            if index <= x:\n",
    "                pcount += 1\n",
    "                start = time.time()\n",
    "                text = f\"Clipping Flow Direction Grid to Watershed OID = {row[0]} with an area of {(row[2]*.001):,.2f} sqKm\"\n",
    "                print(\"\\n{0}\\n  {1:^110}\\n{0}\".format((\"*\" * 110), text))\n",
    "\n",
    "                # Set names and clip rasters\n",
    "                wtdname = rname + \"_wtd_oid_\" + oid\n",
    "                fdr_clipname = wtdname+'_fdr_clip.tif'\n",
    "                fdr_in = os.path.join(scratch,fdr_clipname)\n",
    "                append_value(facDict,wtdname,fdr_in)\n",
    "                if not arcpy.Exists(fdr_in):\n",
    "                    #Clip Raster by watershed extent to reduce footprint/speed up processing and decrease storage space\n",
    "                    print(f'Clipping flow direction grid to watershed oid {oid}')\n",
    "                    arcpy.management.Clip(fdr_source, extent, fdr_in, geom, '', \"NONE\", \"NO_MAINTAIN_EXTENT\")\n",
    "                else:\n",
    "                    print(f'Flow dir already exists as {fdr_in}')\n",
    "                stop = time.time()\n",
    "                elapsed = int(stop - start)\n",
    "                print(f'Flow Direction for {wtdname} clipped - Elapsed time: ({datetime.timedelta(seconds=elapsed)})')\n",
    "                print(\"\\n{0}\\n\".format((\"-\" * 110)))\n",
    "\n",
    "                # Clip flow accumulation grid or create new from clipped fdr?\n",
    "                start = time.time()\n",
    "                fac_clipname = wtdname+'_fac_clip.tif'\n",
    "                fac_in = os.path.join(scratch,fac_clipname)\n",
    "                append_value(facDict,wtdname,fac_in)\n",
    "                if not arcpy.Exists(fac_in):\n",
    "                    # Clip Raster by watershed extent to reduce footprint/speed up processing and decrease storage space\n",
    "                    print(f'Clipping flow acc grid to watershed oid {oid}')\n",
    "                    arcpy.management.Clip(fac_source, extent, fac_in, geom, '', \"NONE\", \"NO_MAINTAIN_EXTENT\")\n",
    "                else:\n",
    "                    print(f'Flow acc clip already exists as {fac_path}')\n",
    "\n",
    "                # # Create fac from clipped fdr\n",
    "                # print(f'Creating flow accumulation from clipped flow direction grid')\n",
    "                # fac_in = FlowAccumulation(fdr_in, '', 'INTEGER', 'D8')\n",
    "                # fac_in.save(fac_path)\n",
    "                stop = time.time()\n",
    "                elapsed = int(stop - start)\n",
    "                print(f'Flow Accumulation for {wtdname} complete - Elapsed time: ({datetime.timedelta(seconds=elapsed)})')\n",
    "                print(\"\\n{0}\\n\".format((\"-\" * 110)))\n",
    "\n",
    "                # fac_glacier_per - % Glacier along flow accumulation = glacier weighted fac/fac\n",
    "                start = time.time()\n",
    "                print (f'Begin Process: Glacial weighting ({strftime(\"%m-%d-%Y %H:%M:%S\")})')\n",
    "                print(\"\\n{0}\\n\".format((\"-\" * 110)))\n",
    "                glacw8name = wtdname +'_glacw8fac.tif'\n",
    "                glacpername = wtdname +'_perglacfac.tif'\n",
    "                glacPer_path = os.path.join(scratch, glacpername)\n",
    "                append_value(facDict,wtdname,glacPer_path)\n",
    "                if not arcpy.Exists(glacPer_path):\n",
    "                    glacw8flow = FlowAccumulation(fdr_in, glac_in, 'FLOAT', 'D8') # Create glacier weighted flow accumulation raster\n",
    "                    glacw8flow.save(os.path.join(scratch,glacw8name)) # not sure if we need to save intermediates?\n",
    "                    print(f'Finish glacier weighted flow accumulation at {strftime(\"%m-%d-%Y %H:%M:%S\")}')\n",
    "                    glacPer = (glacw8flow/fac_in)\n",
    "                    glacPer.save(os.path.join(scratch,glacpername))\n",
    "                    print(f'Finish percent glacier raster {strftime(\"%m-%d-%Y %H:%M:%S\")}')\n",
    "                else:\n",
    "                    print(f'Percent glacier along flow accumulation already created {glacPer_path}')\n",
    "                stop = time.time()\n",
    "                elapsed = int(stop - start)\n",
    "                print(f'Percent Glacier cover along FAC for {wtdname} complete - Elapsed time: ({datetime.timedelta(seconds=elapsed)})')\n",
    "                print(\"\\n{0}\\n\".format((\"-\" * 110)))\n",
    "\n",
    "                # fac_lake_per - % Lake along flow accumulation = lake weighted fac/fac\n",
    "                start = time.time()\n",
    "                print (f'Begin Process: Lake Weighting ({strftime(\"%m-%d-%Y %H:%M:%S\")})')\n",
    "                print(\"\\n{0}\\n\".format((\"-\" * 110)))\n",
    "\n",
    "                lakew8name = wtdname +'_lakew8fac.tif'\n",
    "                lakepername = wtdname +'_perlakefac.tif'\n",
    "                lakePer_path = os.path.join(scratch, lakepername)\n",
    "                append_value(facDict,wtdname,lakePer_path)\n",
    "                if not arcpy.Exists(lakePer_path):\n",
    "                    lakew8flow = FlowAccumulation(fdr_in, lake_in, 'FLOAT', 'D8') # Create lake weighted flow accumulation raster\n",
    "                    lakew8flow.save(os.path.join(scratch,lakew8name)) #not sure if we need to save intermediates?\n",
    "                    print(f'Finish lake weighted flow accumulation at {strftime(\"%m-%d-%Y %H:%M:%S\")}')\n",
    "                    lakePer = (lakew8flow/fac_in)\n",
    "                    lakePer.save(os.path.join(scratch,lakepername))\n",
    "                    print(f'Finish percent lake raster {strftime(\"%m-%d-%Y %H:%M:%S\")}')\n",
    "                else:\n",
    "                    print(f'Percent Lake cover along flow accumulation already created {lakePer_path}')\n",
    "                stop = time.time()\n",
    "                elapsed = int(stop - start)\n",
    "                print(f'Percent lake cover along FAC for {wtdname} complete - Elapsed time: ({datetime.timedelta(seconds=elapsed)})')\n",
    "                print(\"\\n{0}\\n\".format((\"-\" * 110)))\n",
    "\n",
    "                # fac_mn_slope - Mean slope contributing area = slope weighted fac/fac\n",
    "                start = time.time()\n",
    "                print (f'Begin Process: Slope Weighting ({strftime(\"%m-%d-%Y %H:%M:%S\")})')\n",
    "                print(\"\\n{0}\\n\".format((\"-\" * 110)))\n",
    "\n",
    "                slopew8name = wtdname +'_slopew8fac.tif'\n",
    "                slopepername = wtdname +'_mnslopefac.tif'\n",
    "                slopePer_path = os.path.join(scratch, slopepername)\n",
    "                append_value(facDict,wtdname,slopePer_path)\n",
    "                if not arcpy.Exists(slopePer_path):\n",
    "                    slopew8flow = FlowAccumulation(fdr_in, slope_in, 'FLOAT', 'D8') # Create slope weighted flow accumulation raster\n",
    "                    slopew8flow.save(os.path.join(scratch,slopew8name)) #not sure if we need to save intermediates?\n",
    "                    print(f'Finish slope weighted flow accumulation at {strftime(\"%m-%d-%Y %H:%M:%S\")}')\n",
    "                    slopePer = (slopew8flow/fac_in)\n",
    "                    slopePer.save(os.path.join(scratch,slopepername))\n",
    "                    print(f'Finish mean slope raster {strftime(\"%m-%d-%Y %H:%M:%S\")}')\n",
    "                else:\n",
    "                    print(f'Mean slope along flow accumulation already created {slopePer_path}')\n",
    "                stop = time.time()\n",
    "                elapsed = int(stop - start)\n",
    "                print(f'Mean slope cover along FAC for {wtdname} complete - Elapsed time: ({datetime.timedelta(seconds=elapsed)})')\n",
    "                print(\"\\n{0}\\n\".format((\"-\" * 110)))\n",
    "\n",
    "                # fac_north_per - % North along flow accumulation north weighted fac/fac\n",
    "                start = time.time()\n",
    "                print (f'Begin Process: North Weighting ({strftime(\"%m-%d-%Y %H:%M:%S\")})')\n",
    "                print(\"\\n{0}\\n\".format((\"-\" * 110)))\n",
    "\n",
    "                northw8name = wtdname +'_northw8fac.tif'\n",
    "                northpername = wtdname +'_pernorthfac.tif'\n",
    "                northPer_path = os.path.join(scratch, northpername)\n",
    "                append_value(facDict,wtdname,northPer_path)\n",
    "                if not arcpy.Exists(northPer_path):\n",
    "                    northw8flow = FlowAccumulation(fdr_in, north_in, 'FLOAT', 'D8') # Create north weighted flow accumulation raster\n",
    "                    northw8flow.save(os.path.join(scratch,northw8name)) #not sure if we need to save intermediates?\n",
    "                    print(f'Finish north weighted flow accumulation at {strftime(\"%m-%d-%Y %H:%M:%S\")}')\n",
    "                    northPer = (northw8flow/fac_in)\n",
    "                    northPer.save(os.path.join(scratch,northpername))\n",
    "                    print(f'Finish percent north raster {strftime(\"%m-%d-%Y %H:%M:%S\")}')\n",
    "                else:\n",
    "                    print(f'Percent north cover along flow accumulation already created {northPer_path}')\n",
    "                stop = time.time()\n",
    "                elapsed = int(stop - start)\n",
    "                print(f'Percent north cover along FAC for {wtdname} complete - Elapsed time: ({datetime.timedelta(seconds=elapsed)})')\n",
    "                print(\"\\n{0}\\n\".format((\"-\" * 110)))\n",
    "\n",
    "                # fac_wetland_per - % Wetland along flow accumulation = wetland weighted fac/fac\n",
    "                start = time.time()\n",
    "                print (f'Begin Process: Wetland Weighting ({strftime(\"%m-%d-%Y %H:%M:%S\")})')\n",
    "                print(\"\\n{0}\\n\".format((\"-\" * 110)))\n",
    "\n",
    "                wetlandsw8name = wtdname +'_wetlandsw8fac.tif'\n",
    "                wetlandspername = wtdname +'_perwetlandsfac.tif'\n",
    "                wetlandsPer_path = os.path.join(scratch, wetlandspername)\n",
    "                append_value(facDict,wtdname,wetlandsPer_path)\n",
    "                if not arcpy.Exists(wetlandsPer_path):\n",
    "                    wetlandsw8flow = FlowAccumulation(fdr_in, wetlands_in, 'FLOAT', 'D8') # Create wetlands weighted flow accumulation raster\n",
    "                    wetlandsw8flow.save(os.path.join(scratch,wetlandsw8name)) #not sure if we need to save intermediates?\n",
    "                    print(f'Finish wetlands weighted flow accumulation at {strftime(\"%m-%d-%Y %H:%M:%S\")}')\n",
    "                    wetlandsPer = (wetlandsw8flow/fac_in)\n",
    "                    wetlandsPer.save(os.path.join(scratch,wetlandspername))\n",
    "                    print(f'Finish percent wetlands raster {strftime(\"%m-%d-%Y %H:%M:%S\")}')\n",
    "                else:\n",
    "                    print(f'Percent wetlands cover along flow accumulation already created {wetlandsPer_path}')\n",
    "                stop = time.time()\n",
    "                elapsed = int(stop - start)\n",
    "                print(f'Percent wetlands cover along FAC for {wtdname} complete - Elapsed time: ({datetime.timedelta(seconds=elapsed)})')\n",
    "                print(\"\\n{0}\\n\".format((\"-\" * 110)))\n",
    "\n",
    "                # fac_mn_lcld_xxxx - Mean day of year for LCLD (by year) along flow accumulation = mdy(year) weighted fac/fac\n",
    "                walk = arcpy.da.Walk(lcldfol, datatype = ['RasterDataset'])\n",
    "                for dirpath, dirnames, filenames in walk:\n",
    "                    for filename in filenames:\n",
    "                        start = time.time()\n",
    "                        print (f'Begin Process: {filename} Weighting ({strftime(\"%m-%d-%Y %H:%M:%S\")})')\n",
    "                        print(\"\\n{0}\\n\".format((\"-\" * 110)))\n",
    "                        print(filename[:-4])\n",
    "                        rasname = filename[:-4]\n",
    "                        lcldw8name = wtdname +'_' + rasname +\"_w8fac.tif\"\n",
    "                        lcldpername = wtdname +'_' + rasname + '_mnlcldfac.tif'\n",
    "                        lcldPer_path = os.path.join(scratch, lcldpername)\n",
    "                        lcld_in = os.path.join(dirpath,filename)\n",
    "                        append_value(facDict,wtdname,lcldPer_path)\n",
    "                        if not arcpy.Exists(lcldPer_path):\n",
    "                            lcldw8flow = FlowAccumulation(fdr_in, lcld_in, 'FLOAT', 'D8') # Create lcld weighted flow accumulation raster\n",
    "                            lcldw8flow.save(os.path.join(scratch,lcldw8name)) # not sure if we need to save intermediates?\n",
    "                            print(f'Finish lcld weighted flow accumulation at {strftime(\"%m-%d-%Y %H:%M:%S\")}')\n",
    "                            lcldPer = (lcldw8flow/fac_in)\n",
    "                            lcldPer.save(os.path.join(scratch,lcldpername))\n",
    "                            print(f'Finish mean lcld raster {strftime(\"%m-%d-%Y %H:%M:%S\")}')\n",
    "                        else:\n",
    "                            print(f'Mean lcld cover along flow accumulation already created {lcldPer_path}')\n",
    "                        stop = time.time()\n",
    "                        elapsed = int(stop - start)\n",
    "                        print(f'{filename[:4]} Mean lcld along FAC for {wtdname} complete - Elapsed time: ({datetime.timedelta(seconds=elapsed)})')\n",
    "                        print(\"\\n{0}\\n\".format((\"-\" * 110)))\n",
    "                # End Timing\n",
    "                iter_time_stop = time.time()\n",
    "                iter_time = int(iter_time_stop-iteration_start)\n",
    "                print(f'{rname} fac weights complete - Elapsed time: ({datetime.timedelta(seconds=iter_time)})')\n",
    "                print('----------')\n",
    "            else:\n",
    "                print(f'Watershed {oid} out of index and not yet processed...')\n",
    "                print(\"\\n{0}\\n\".format((\"-\" * 110)))\n",
    "        print(f'Processed {pcount} of {xr} watersheds for {rname}')\n",
    "    region_time_stop = time.time()\n",
    "    region_process_time = int(region_time_stop - region_time_start)\n",
    "    print(f'{rname} complete - Elapsed time: ({datetime.timedelta(seconds=region_process_time)})')\n",
    "process_stop = time.time()\n",
    "process_time = int(process_stop - process_start)\n",
    "print(f'All regions complete - Elapsed time: ({datetime.timedelta(seconds=process_time)})')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract to outlets of catchments outlets for catchments with a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}