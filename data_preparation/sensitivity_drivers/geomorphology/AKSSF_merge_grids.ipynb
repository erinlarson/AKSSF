{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Can follow some code in timm's dem merging function.\n",
    "https://github.com/accs-uaa/beringian-vegetation/blob/master/package_GeospatialProcessing/mergeElevationTiles.py\n",
    "\n",
    "Recommended workflow for geoprocessing:\n",
    "- import arcpy\n",
    "- set environments\n",
    "- create lists of gis objects\n",
    "- iterate through list elements\n",
    "- execute geoprocessing tools\n",
    "\n",
    "Geoprocessing steps in this script\n",
    "1. create a set of folders and gdb for each region under a master AKSSF folder\n",
    "2. loop through regions and grids or feature classes (e.g. catchments or flowlines)\n",
    "3. merge grids or feature classes\n",
    "4. save in folder/gdb with consistent name (e.g. fac_merge)\n",
    "\n",
    "These seamed grids will enable us to use zonal statistics with the selected catchments\n",
    "and watersheds to get our environmental variables for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AKSSF exists. Please provide a new project name.\n"
     ]
    }
   ],
   "source": [
    "# step 1\n",
    "# Create folders for storing outputs\n",
    "\n",
    "#import modules\n",
    "import os, arcpy, sys\n",
    "#user inputs\n",
    "baseFolder = \"D:\\\\GIS\\\\\"\n",
    "projectName = \"AKSSF\"\n",
    "akssfol = os.path.join(baseFolder, projectName)\n",
    "\n",
    "#define folder paths\n",
    "ciFolder = baseFolder + \"\\\\\" + projectName + \"\\\\\"+ \"Cook_Inlet\"\n",
    "pwsFolder = baseFolder + \"\\\\\" + projectName + \"\\\\\"+ \"Prince_William_Sound\"\n",
    "bbFolder = baseFolder + \"\\\\\" + projectName + \"\\\\\"+ \"Bristol_Bay\"\n",
    "crFolder = baseFolder + \"\\\\\" + projectName + \"\\\\\"+ \"Copper_River\"\n",
    "kodFolder = baseFolder + \"\\\\\" + projectName + \"\\\\\"+ \"Kodiak\"\n",
    "\n",
    "#create list of folder paths\n",
    "folders = [ciFolder, pwsFolder, bbFolder, crFolder, kodFolder]\n",
    "\n",
    "#check if project folder exists\n",
    "if not os.path.exists(baseFolder + projectName):\n",
    "    #Create folder if it does not exist\n",
    "    for folder in folders:\n",
    "        os.makedirs(folder)\n",
    "        print(\"Createds: \" + folder)\n",
    "else:\n",
    "    #if project folder exists, print error message\n",
    "    print (projectName + \" exists. Please provide a new project name.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\Basedata\\\\NHDPlus08112021\\\\Cook_Inlet\\\\NHDPLUS_H_19020202_HU8_GDB.gdb', 'D:\\\\Basedata\\\\NHDPlus08112021\\\\Cook_Inlet\\\\NHDPLUS_H_19020301_HU8_GDB.gdb', 'D:\\\\Basedata\\\\NHDPlus08112021\\\\Cook_Inlet\\\\NHDPLUS_H_19020302_HU8_GDB.gdb', 'D:\\\\Basedata\\\\NHDPlus08112021\\\\Cook_Inlet\\\\NHDPLUS_H_19020401_HU8_GDB.gdb', 'D:\\\\Basedata\\\\NHDPlus08112021\\\\Cook_Inlet\\\\NHDPLUS_H_19020402_HU8_GDB.gdb', 'D:\\\\Basedata\\\\NHDPlus08112021\\\\Cook_Inlet\\\\NHDPLUS_H_19020501_HU8_GDB.gdb', 'D:\\\\Basedata\\\\NHDPlus08112021\\\\Cook_Inlet\\\\NHDPLUS_H_19020502_HU8_GDB.gdb', 'D:\\\\Basedata\\\\NHDPlus08112021\\\\Cook_Inlet\\\\NHDPLUS_H_19020503_HU8_GDB.gdb', 'D:\\\\Basedata\\\\NHDPlus08112021\\\\Cook_Inlet\\\\NHDPLUS_H_19020504_HU8_GDB.gdb', 'D:\\\\Basedata\\\\NHDPlus08112021\\\\Cook_Inlet\\\\NHDPLUS_H_19020505_HU8_GDB.gdb', 'D:\\\\Basedata\\\\NHDPlus08112021\\\\Cook_Inlet\\\\NHDPLUS_H_19020601_HU8_GDB.gdb', 'D:\\\\Basedata\\\\NHDPlus08112021\\\\Cook_Inlet\\\\NHDPLUS_H_19020602_HU8_GDB.gdb', 'D:\\\\Basedata\\\\NHDPlus08112021\\\\Cook_Inlet\\\\NHDPLUS_H_19020800_HU8_GDB.gdb']\n",
      "D:\\GIS\\AKSSF\\Cook_Inlet\\Cook_Inlet.gdb already exists\n",
      "Copying flowline_19020202...\n",
      "Copying waterbody_19020202...\n",
      "Copying flowline_19020301...\n",
      "Copying waterbody_19020301...\n",
      "Copying flowline_19020302...\n",
      "Copying waterbody_19020302...\n",
      "Copying flowline_19020401...\n",
      "Copying waterbody_19020401...\n",
      "Copying flowline_19020402...\n",
      "Copying waterbody_19020402...\n",
      "Copying flowline_19020501...\n",
      "Copying waterbody_19020501...\n",
      "Copying flowline_19020502...\n",
      "Copying waterbody_19020502...\n",
      "Copying flowline_19020503...\n",
      "Copying waterbody_19020503...\n",
      "Copying flowline_19020504...\n",
      "Copying waterbody_19020504...\n",
      "Copying flowline_19020505...\n",
      "Copying waterbody_19020505...\n",
      "Copying flowline_19020601...\n",
      "Copying waterbody_19020601...\n",
      "Copying flowline_19020602...\n",
      "Copying waterbody_19020602...\n",
      "Copying flowline_19020800...\n",
      "Copying waterbody_19020800...\n",
      "Merging catchments for Cook_Inlet...\n",
      "----------\n",
      "Merging vaas for Cook_Inlet...\n",
      "----------\n",
      "Merging flowlines for Cook_Inlet...\n",
      "----------\n",
      "Merging waterbodies for Cook_Inlet...\n",
      "----------\n",
      "['D:\\\\Basedata\\\\NHDPlus08112021\\\\Copper_River\\\\NHDPLUS_H_19020101_HU8_GDB.gdb', 'D:\\\\Basedata\\\\NHDPlus08112021\\\\Copper_River\\\\NHDPLUS_H_19020102_HU8_GDB.gdb', 'D:\\\\Basedata\\\\NHDPlus08112021\\\\Copper_River\\\\NHDPLUS_H_19020103_HU8_GDB.gdb', 'D:\\\\Basedata\\\\NHDPlus08112021\\\\Copper_River\\\\NHDPLUS_H_19020104_HU8_GDB.gdb']\n",
      "D:\\GIS\\AKSSF\\Copper_River\\Copper_River.gdb already exists\n",
      "Copying flowline_19020101...\n",
      "Copying waterbody_19020101...\n",
      "Copying flowline_19020102...\n",
      "Copying waterbody_19020102...\n",
      "Copying flowline_19020103...\n",
      "Copying waterbody_19020103...\n",
      "Copying flowline_19020104...\n",
      "Copying waterbody_19020104...\n",
      "Merging catchments for Copper_River...\n",
      "----------\n",
      "Merging vaas for Copper_River...\n",
      "----------\n",
      "Merging flowlines for Copper_River...\n",
      "----------\n",
      "Merging waterbodies for Copper_River...\n",
      "----------\n",
      "NHDPlus gdb data merge complete\n"
     ]
    }
   ],
   "source": [
    "# step 2\n",
    "# merge catchments, waterbodies, flowlines,  and vaa tables\n",
    "# must copy all flowlines and waterbodies to scratch gdb before merging or the operation will fail.\n",
    "#This code chunk is for NHDPlus regions -- Cook Inlet and Copper River\n",
    "\n",
    "import arcpy, sys, os\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "\n",
    "#scratch gdb to store copies of nhdplus data that will not merge\n",
    "scratch = r'D:\\GIS\\scratch.gdb'\n",
    "#path to folder with most Current NHdplus data split by region\n",
    "nhdPlusfol = r'D:\\Basedata\\NHDPlus08112021'\n",
    "rois = ['Cook_Inlet','Copper_River']\n",
    "\n",
    "#NHD folders - Update to new data downloaded Nov 8, 2021\n",
    "for roi in rois:\n",
    "    arcpy.env.workspace = os.path.join(nhdPlusfol,roi)\n",
    "    gdbs = arcpy.ListWorkspaces('NHDPLUS*','FileGDB')\n",
    "    print(gdbs)\n",
    "    Cats = []\n",
    "    VAA = []\n",
    "    waterbodies = []\n",
    "    flowlines = []\n",
    "\n",
    "    outpath =os.path.join(akssfol, roi)\n",
    "    gdbname = roi + '.gdb'\n",
    "    outgdb = os.path.join(outpath, gdbname)\n",
    "    if not arcpy.Exists(outgdb):\n",
    "        print(f'Creating {outgdb}')\n",
    "        arcpy.CreateFileGDB_management(outpath, roi+'.gdb ')\n",
    "    else:\n",
    "        print(f'{outgdb} already exists')\n",
    "        outgdb = outgdb\n",
    "\n",
    "    for gdb in gdbs:\n",
    "        arcpy.env.workspace = gdb\n",
    "        huc = gdb[-20:-12]\n",
    "        datasets = arcpy.ListDatasets(feature_type='feature')\n",
    "        for ds in datasets:\n",
    "            for fc in arcpy.ListFeatureClasses(feature_dataset=ds):\n",
    "                if fc == \"NHDFlowline\":\n",
    "                    flowpath = os.path.join(arcpy.env.workspace, ds, fc)\n",
    "                    flowname = 'flowline_' + str(huc)\n",
    "                    print(f'Copying {flowname}...')\n",
    "                    flowcopypath = os.path.join(scratch, flowname)\n",
    "                    arcpy.management.CopyFeatures(flowpath, flowcopypath)\n",
    "                    flowlines.append(flowcopypath)\n",
    "                elif fc == \"NHDWaterbody\":\n",
    "                    waterbodyname = 'waterbody_' + str(huc)\n",
    "                    waterbodycopypath = os.path.join(scratch, waterbodyname)\n",
    "                    waterpath = os.path.join(arcpy.env.workspace, ds, fc)\n",
    "                    print(f'Copying {waterbodyname}...')\n",
    "                    arcpy.management.CopyFeatures(waterpath, waterbodycopypath)\n",
    "                    waterbodies.append(waterbodycopypath)\n",
    "                elif fc == \"NHDPlusCatchment\":\n",
    "                    catchpath = os.path.join(arcpy.env.workspace, ds, fc)\n",
    "                    Cats.append(catchpath)\n",
    "        vaapath = os.path.join(arcpy.env.workspace, \"NHDPlusFlowlineVAA\")\n",
    "        VAA.append(vaapath)\n",
    "    arcpy.env.workspace =  outgdb\n",
    "    arcpy.env.outputCoordinateSystem = sr\n",
    "    arcpy.env.overwriteOutput = True\n",
    "    try:\n",
    "        print(f'Merging catchments for {roi}...')\n",
    "        print('----------')\n",
    "        arcpy.management.Merge(Cats, os.path.join(outgdb,'cats_merge'))\n",
    "        print(f'Merging vaas for {roi}...')\n",
    "        print('----------')\n",
    "        arcpy.management.Merge(VAA, os.path.join(outgdb,'vaa_merge'))\n",
    "        print(f'Merging flowlines for {roi}...')\n",
    "        print('----------')\n",
    "        arcpy.management.Merge(flowlines, os.path.join(outgdb,'NHDFlowline_merge'))\n",
    "        print(f'Merging waterbodies for {roi}...')\n",
    "        print('----------')\n",
    "        arcpy.management.Merge(waterbodies, os.path.join(outgdb,'NHDWaterbody_merge'))\n",
    "    except:\n",
    "        e = sys.exc_info()[1]\n",
    "        print(e.args[0])\n",
    "        arcpy.AddError(e.args[0])\n",
    "\n",
    "print('NHDPlus gdb data merge complete')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merge catchments and streams for Bristol Bay\n",
    "\n",
    "This code chunk is for Bristol Bay, which had several processing units created\n",
    "with synthetic networks from TauDEM. The gridcodes all start from 1 so are not unique\n",
    "when combining the different VPUS. To create unique IDs across the VPUs,\n",
    "need to make a new catID with a prefix for each VPU: 10, 20, 30, 40, 50.\n",
    "Note that gridcode = LINKNO in the stream reach feature class.\n",
    "Also for the streams fc that we are using to create watersheds, the upstream\n",
    "links must be fixed so they are unique as well.\n",
    "\n",
    "For Kodiak and Prince William Sound, can just copy over catchments and streams since\n",
    "only one feature class for each."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "baseFolder = \"D:\\\\GIS\\\\\"\n",
    "ds = baseFolder + \"AKSSF\\\\AKSSF_Hydrography.gdb\\\\TauDEM_Outputs\"\n",
    "region = \"Bristol_Bay\"\n",
    "arcpy.env.workspace = ds\n",
    "cats = arcpy.ListFeatureClasses('Catchments*')\n",
    "streams = arcpy.ListFeatureClasses('Stream*')\n",
    "\n",
    "#For Bristol Bay - remove feature classes with Pws\n",
    "cats = [x for x in cats if 'Pws' not in x]\n",
    "streams = [x for x in streams if 'Pws' not in x]\n",
    "\n",
    "#get full path names onto lists\n",
    "cats = [ds + \"\\\\\" + x for x in cats]\n",
    "streams = [ds + \"\\\\\" + x for x in streams]\n",
    "\n",
    "#add a new field with the VPU prefix to the gridcode.\n",
    "# for cat in cats:\n",
    "#     if len(arcpy.ListFields(cat, \"catID\")) > 0:\n",
    "#         arcpy.DeleteField_management(cat, \"catID\")\n",
    "#         arcpy.AddField_management(cat, \"catID\", \"LONG\")\n",
    "#     vpu = cat[61:63]\n",
    "#     print(vpu)\n",
    "#     options = {'Eg': 1000000, 'La': 2000000, 'Na': 3000000, 'Nu': 4000000, 'To': 5000000}\n",
    "#     if vpu in options:\n",
    "#         prefix = options[vpu]\n",
    "#     else:\n",
    "#         print(\"vpu not found\")\n",
    "#     arcpy.management.CalculateField(cat, \"catID\", 'prefix + !gridcode!')\n",
    "\n",
    "# for stream in streams:\n",
    "#     print(stream)\n",
    "#     if len(arcpy.ListFields(stream, \"catID\")) > 0:\n",
    "#         arcpy.DeleteField_management(stream, [\"catID\", \"upCatID1\", \"upCatID2\"])\n",
    "#         arcpy.management.AddFields(stream,[[\"catID\", \"LONG\"], [\"upCatID1\", \"LONG\"], [\"upCatID2\", \"LONG\"]])\n",
    "#     vpu = stream[65:67]\n",
    "#     print(vpu)\n",
    "#     # options = {'Eg': 1000000, 'La': 2000000, 'Na': 3000000, 'Nu': 4000000, 'To': 5000000}\n",
    "#     if vpu in options:\n",
    "#         prefix = options[vpu]\n",
    "#     else:\n",
    "#         print(\"vpu not found\")\n",
    "#     arcpy.management.CalculateField(stream, \"catID\", 'prefix + !LINKNO!')\n",
    "#     arcpy.management.CalculateField(stream, \"upCatID1\", 'prefix + !USLINKNO1!')\n",
    "#     arcpy.management.CalculateField(stream, \"upCatID2\", 'prefix + !USLINKNO2!')\n",
    "\n",
    "\n",
    "print(\"Catchment feature classes to be merged: \")\n",
    "print(cats)\n",
    "print(\"Stream feature classes to be merged: \")\n",
    "print(streams)\n",
    "\n",
    "arcpy.env.outputCoordinateSystem = arcpy.Describe(cats[1]).spatialReference\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.CreateFileGDB_management(baseFolder + \"AKSSF\\\\\" + region, region + \".gdb\")\n",
    "# arcpy.env.workspace =  r\"W:/GIS/AKSSF/\" + region + \".gdb\"\n",
    "\n",
    "arcpy.management.Merge(cats, r\"W:/GIS/AKSSF/\" + region + \"\\\\\" + region + \".gdb/cats_merge\")\n",
    "arcpy.management.Merge(streams, r\"W:/GIS/AKSSF/\" + region + \"\\\\\" + region + \".gdb/streams_merge\")\n",
    "\n",
    "print(\"Merged feature classes saved to: \" + arcpy.env.workspace)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Copy feature classes for PWS and Kodiak (one VPU each)\n",
    "\n",
    "Copy feature classes from AKSSF Hydrography geodb to new region specific geodb\n",
    "for Kodiak and PWS. These will be used to create watersheds and run zonal\n",
    "stats for catchments and watersheds."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "\n",
    "# Create new geodb in each regional folder\n",
    "baseFolder = \"D:\\\\GIS\\\\\"\n",
    "region = \"Prince_William_Sound\"\n",
    "# region = \"Kodiak\"\n",
    "\n",
    "# TauDEM outputs\n",
    "pwsNet = baseFolder + \"AKSSF\\\\AKSSF_Hydrography.gdb\\\\TauDEM_Outputs\"\n",
    "kodNet = baseFolder + \"AKSSF\\\\AKSSF_Hydrography.gdb\\\\TauDEM_StreamBurn\"\n",
    "\n",
    "arcpy.env.workspace = pwsNet\n",
    "fcs = arcpy.ListFeatureClasses()\n",
    "fcs = [os.path.join(arcpy.env.workspace, x) for x in fcs]\n",
    "fcs = [x for x in fcs if \"Pws\" in x]\n",
    "print(fcs)\n",
    "\n",
    "# arcpy.CreateFileGDB_management(r\"W:/GIS/AKSSF/\" + region, region + \".gdb\")\n",
    "arcpy.env.workspace =  r\"W:/GIS/AKSSF/\" + region + \"//\" + region + \".gdb\"\n",
    "arcpy.env.outputCoordinateSystem = arcpy.Describe(fcs[0]).spatialReference\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "# Copy feature classes over from AKSSF_Hydrography geodb\n",
    "for fc in fcs:\n",
    "    if \"Cat\" in fc:\n",
    "        out_fc = \"cats_merge\"\n",
    "    else:\n",
    "        out_fc = \"streams_merge\"\n",
    "    arcpy.CopyFeatures_management(fc, out_fc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merge raster datasets by region\n",
    "1. Create a regional boundary (maybe buffer by 10 km) for clipping statewide datasets and putting all grids into regional geodb\n",
    "2. Elevation grids: clip 10m DEM for synthetic flow networks (ask Dustin for this) and merge elevation rasters for each NHDPlus VPU\n",
    "3. Flow accumulation grids: copy and/or merge as needed by region\n",
    "4. Use elevation grids to create slope grids for each region\n",
    "5. Use elevation grids to create the aspect grid\n",
    "6. Use elevation grids to create a grid of north facing cells (Dustin has this code ready)\n",
    "7. Lake feature class: merge from NHPPlus VPUs or clip from regular NHD to region geodb\n",
    "8. Copy glims glacier inventory shp to local AKSSF folder and clip to create glacier polys for each region\n",
    "9. Copy NLCD to local AKSSF folder and clip create a 1/0 grid of wetland cover (Dustin should have this code in deshka repo)\n",
    "10. (MODIS lcld in separate script)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GIS\\AKSSF\\Cook_Inlet\\Cook_Inlet.gdb\\cats_merge\n",
      "Buffering D:\\GIS\\AKSSF\\Cook_Inlet\\Cook_Inlet.gdb\\cats_merge....\n",
      "D:\\GIS\\AKSSF\\Copper_River\\Copper_River.gdb\\cats_merge\n",
      "Buffering D:\\GIS\\AKSSF\\Copper_River\\Copper_River.gdb\\cats_merge....\n",
      "buffer complete\n"
     ]
    }
   ],
   "source": [
    "# step 1, create regional boundaries with buffer for clipping grids\n",
    "# dissolve on cats_merge and buffer by 10 km\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.workspace = akssfol\n",
    "regions = arcpy.ListWorkspaces(workspace_type=\"Folder\")\n",
    "nhdplus_dat = ['Cook_Inlet', 'Copper_River']\n",
    "tauDem_dat = ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound']\n",
    "# Loop through all processing areas\n",
    "#rois = nhdplus_dat + tauDem_dat\n",
    "rois = nhdplus_dat\n",
    "for roi in rois:\n",
    "    # Loop through regional folders\n",
    "    for region in regions:\n",
    "            if roi in str(region):\n",
    "                walk = arcpy.da.Walk(region, datatype=['FeatureClass', 'Table'])\n",
    "                for dirpath, dirnames, filenames in walk:\n",
    "                   for filename in filenames:\n",
    "                       if filename == 'cats_merge':\n",
    "                           catspath = os.path.join(dirpath,filename)\n",
    "                           print (catspath)\n",
    "                           print (f'Buffering {catspath}....')\n",
    "                           arcpy.Buffer_analysis(catspath, os.path.join(dirpath, 'region_buf'), \"10 Kilometers\", \"FULL\", \"ROUND\", \"ALL\")\n",
    "print('buffer complete')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging elev_cm...\n",
      "Converting elevation in cm to meters, resampling to 10 meters cellsize and converting to Integer...\n",
      "Processing Copper_River using rasters in D:\\Basedata\\NHDPlus08112021\\Copper_River...\n",
      "Merging Flow acc...\n",
      "Merging elev_cm...\n",
      "Converting elevation in cm to meters, resampling to 10 meters cellsize and converting to Integer...\n",
      "Merge complete for ['Cook_Inlet', 'Copper_River']\n",
      "----------\n",
      "--- 9057.211299657822 seconds ---\n",
      "----------\n",
      "['D:\\\\GIS\\\\AKSSF\\\\Bristol_Bay', 'D:\\\\GIS\\\\AKSSF\\\\Cook_Inlet', 'D:\\\\GIS\\\\AKSSF\\\\Copper_River', 'D:\\\\GIS\\\\AKSSF\\\\Kodiak', 'D:\\\\GIS\\\\AKSSF\\\\Prince_William_Sound']\n",
      "Processing Cook_Inlet using rasters in D:\\Basedata\\NHDPlus08112021\\Cook_Inlet...\n",
      "Merging Flow acc...\n",
      "Merging elev_cm...\n",
      "Converting elevation in cm to meters, resampling to 10 meters cellsize and converting to Integer...\n",
      "Processing Copper_River using rasters in D:\\Basedata\\NHDPlus08112021\\Copper_River...\n",
      "Merging Flow acc...\n",
      "Merging elev_cm...\n",
      "Converting elevation in cm to meters, resampling to 10 meters cellsize and converting to Integer...\n",
      "Merge complete for ['Cook_Inlet', 'Copper_River']\n",
      "----------\n",
      "--- 8857.002339839935 seconds ---\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# steps 2 and 3: merge elevation and flow accumulation rasters for regions with NHDPlus\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "from arcpy.sa import *\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "arcpy.env.workspace = akssfol\n",
    "# Let tools decide parallel processing factor\n",
    "arcpy.env.parallelProcessingFactor = \"\"\n",
    "regions = arcpy.ListWorkspaces()\n",
    "print (regions)\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "#Path to folder with NHDPlus data separated by region\n",
    "nhdPlusfol = r'D:\\Basedata\\NHDPlus08112021'\n",
    "rois = ['Cook_Inlet','Copper_River']\n",
    "for roi in rois:\n",
    "    for region in regions:\n",
    "        if roi in str(region):\n",
    "            nhdfol = os.path.join(nhdPlusfol,roi)\n",
    "            print(f'Processing {roi} using rasters in {nhdfol}...')\n",
    "            flowacc_rasters = []\n",
    "            elev_rasters = []\n",
    "            walk = arcpy.da.Walk(nhdfol, datatype = \"RasterDataset\")\n",
    "            for dirpath, dirnames, filenames in walk:\n",
    "                for filename in filenames:\n",
    "                    if filename == 'fac.tif':\n",
    "                        flowacc_raspath = os.path.normpath(os.path.join(dirpath, filename))\n",
    "                        flowacc_rasters.append(flowacc_raspath)\n",
    "                    elif filename == 'elev_cm.tif':\n",
    "                        elev_raspath = os.path.normpath(os.path.join(dirpath, filename))\n",
    "                        elev_rasters.append(elev_raspath)\n",
    "            print(f'Merging Flow acc...')\n",
    "            fac = arcpy.MosaicToNewRaster_management(flowacc_rasters, region, \"fac.tif\", sr, \"32_BIT_SIGNED\", \"5\", \"1\", \"LAST\",\"FIRST\")\n",
    "            print('Merging elev_cm...')\n",
    "            elev_cm = arcpy.MosaicToNewRaster_management(elev_rasters, region, \"elev_cm.tif\", sr, \"32_BIT_SIGNED\", \"5\", \"1\", \"LAST\",\"FIRST\")\n",
    "            print('Converting elevation in cm to meters, resampling to 10 meters cellsize and converting to Integer...')\n",
    "            elev = arcpy.Resample_management(Int(Times(elev_cm, .01)), os.path.join(region, 'elev.tif'), \"10\", \"BILINEAR\")\n",
    "\n",
    "print(f'Merge complete for {rois}')\n",
    "print('----------')\n",
    "arcpy.AddMessage(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print (\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('----------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# steps 2 and 3: merge elevation and flow accumulation rasters for regions with NHDPlus\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "from arcpy.sa import *\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "arcpy.env.workspace = akssfol\n",
    "# Let tools decide parallel processing factor\n",
    "arcpy.env.parallelProcessingFactor = \"\"\n",
    "regions = arcpy.ListWorkspaces()\n",
    "print (regions)\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "#Path to folder with NHDPlus data separated by region\n",
    "nhdPlusfol = r'D:\\Basedata\\NHDPlus08112021'\n",
    "rois = ['Cook_Inlet','Copper_River']\n",
    "for roi in rois:\n",
    "    for region in regions:\n",
    "        if roi in str(region):\n",
    "            nhdfol = os.path.join(nhdPlusfol,roi)\n",
    "            print(f'Processing {roi} using rasters in {nhdfol}...')\n",
    "            flowacc_rasters = []\n",
    "            elev_rasters = []\n",
    "            walk = arcpy.da.Walk(nhdfol, datatype = \"RasterDataset\")\n",
    "            for dirpath, dirnames, filenames in walk:\n",
    "                for filename in filenames:\n",
    "                    if filename == 'fac.tif':\n",
    "                        flowacc_raspath = os.path.normpath(os.path.join(dirpath, filename))\n",
    "                        flowacc_rasters.append(flowacc_raspath)\n",
    "                    elif filename == 'elev_cm.tif':\n",
    "                        elev_raspath = os.path.normpath(os.path.join(dirpath, filename))\n",
    "                        elev_rasters.append(elev_raspath)\n",
    "            print(f'Merging Flow acc...')\n",
    "            fac = arcpy.MosaicToNewRaster_management(flowacc_rasters, region, \"fac.tif\", sr, \"32_BIT_SIGNED\", \"5\", \"1\", \"LAST\",\"FIRST\")\n",
    "            print('Merging elev_cm...')\n",
    "            elev_cm = arcpy.MosaicToNewRaster_management(elev_rasters, region, \"elev_cm.tif\", sr, \"32_BIT_SIGNED\", \"5\", \"1\", \"LAST\",\"FIRST\")\n",
    "            print('Converting elevation in cm to meters, resampling to 10 meters cellsize and converting to Integer...')\n",
    "            elev = arcpy.Resample_management(Int(Times(elev_cm, .01)), os.path.join(region, 'elev.tif'), \"10\", \"BILINEAR\")\n",
    "\n",
    "print(f'Merge complete for {rois}')\n",
    "print('----------')\n",
    "arcpy.AddMessage(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print (\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('----------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# steps 2 and 3: clip elevation grid and copy flow accumulation raster for regions with synthetic networks\n",
    "# T:\\Aquatic\\AKSSF\\topography for 10 m DEM used on these networks\n",
    "# This code chunk clips the 10m dem for each region\n",
    "import arcpy\n",
    "from arcpy.sa import *\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "dem_sa = \"T:\\\\Aquatic\\\\AKSSF\\\\topography\\\\AKSSF_Elevation_Composite_10m.tif\"\n",
    "dem_bb = \"T:\\\\Aquatic\\\\AKSSF\\\\topography\\\\AKSSF_Bristol_Bay_10m_comp_clip.tif\"\n",
    "arcpy.env.outputCoordinateSystem = dem_sa\n",
    "arcpy.env.snapRaster = dem_sa\n",
    "\n",
    "regions = ['Bristol_Bay', 'Prince_William_Sound', 'Kodiak']\n",
    "base_folder = \"W:\\\\GIS\\\\AKSSF\\\\\"\n",
    "\n",
    "for region in regions:\n",
    "    folder = base_folder + region\n",
    "    buffer = folder + \"\\\\\" + region + \".gdb\\\\region_buf\"\n",
    "    print(buffer)\n",
    "    extract_raster = arcpy.sa.ExtractByMask(dem_sa, buffer)\n",
    "    extract_raster.save(folder + \"\\\\elev.tif\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Copy FAC grids for synthetic networks - ad8 grid from TauDEM\n",
    "For BB, will need to seam them together, similar to NHDPlus regions\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import arcpy\n",
    "from arcpy.sa import *\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "baseFolder = \"W:\\\\GIS\\\\AKSSF\\\\\"\n",
    "v1_folder = r\"W:\\GIS\\AKSSF\\TauDEM\\V1\"\n",
    "v2_folder = r\"W:\\GIS\\AKSSF\\TauDEM\\V2_CartoStreamBurn\\KodiakAfognakIslands\"\n",
    "\n",
    "#Bristol Bay first\n",
    "arcpy.env.workspace = v1_folder\n",
    "flowacc_rasters = []\n",
    "raster_folders = arcpy.ListWorkspaces()\n",
    "condition = [\"Kodiak\", \"Pws\", \"ResRiver\"]\n",
    "# raster_folders = [x for x in raster_folders if any(c not in raster_folders for c in condition)]\n",
    "raster_folders = [x for x in raster_folders if not (\"Pws\" in x or \"Kodiak\" in x or \"ResRiver\" in x)]\n",
    "print(raster_folders)\n",
    "for folder in raster_folders:\n",
    "    arcpy.env.workspace = folder\n",
    "    fac_grids = arcpy.ListRasters(\"*ad8.tif\")\n",
    "    for grid in fac_grids:\n",
    "        fac_path = os.path.join(folder, grid)\n",
    "        print(fac_path)\n",
    "        flowacc_rasters.append(fac_path)\n",
    "dataset = flowacc_rasters[1]\n",
    "spatial_ref = arcpy.Describe(dataset).spatialReference\n",
    "outFolder = baseFolder + \"Bristol_Bay\"\n",
    "print(outFolder)\n",
    "arcpy.MosaicToNewRaster_management(flowacc_rasters, outFolder, \"fac.tif\", spatial_ref, \"32_BIT_SIGNED\", \"10\", \"1\", \"LAST\",\"FIRST\")\n",
    "\n",
    "#Prince William Sound\n",
    "fac_path = v1_folder + \"\\\\Pws\\\\Pws_ad8.tif\"\n",
    "print(fac_path)\n",
    "out_fac = baseFolder + \"Prince_William_Sound\\\\fac.tif\"\n",
    "arcpy.CopyRaster_management(fac_path, out_fac)\n",
    "\n",
    "# Kodiak\n",
    "fac_path = v2_folder + \"\\\\KodiakAfognakIslands_ad8.tif\"\n",
    "print(fac_path)\n",
    "out_fac = baseFolder + \"Kodiak\\\\fac.tif\"\n",
    "arcpy.CopyRaster_management(fac_path, out_fac)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Slope, Aspect, and North grids\n",
    "Calculate slope and aspect with the ESRI recommended surface parameter tool using a 5x5 window.  Produce North grid using\n",
    "new Aspect raster."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\GIS\\\\AKSSF\\\\Bristol_Bay', 'D:\\\\GIS\\\\AKSSF\\\\Cook_Inlet', 'D:\\\\GIS\\\\AKSSF\\\\Copper_River', 'D:\\\\GIS\\\\AKSSF\\\\Kodiak', 'D:\\\\GIS\\\\AKSSF\\\\Prince_William_Sound']\n",
      "D:\\GIS\\AKSSF\\Bristol_Bay\\elev.tif\n",
      "D:\\GIS\\AKSSF\\Cook_Inlet\\elev.tif\n",
      "D:\\GIS\\AKSSF\\Copper_River\\elev.tif\n",
      "D:\\GIS\\AKSSF\\Kodiak\\elev.tif\n",
      "D:\\GIS\\AKSSF\\Prince_William_Sound\\elev.tif\n"
     ]
    }
   ],
   "source": [
    "# steps 4-6, use elevation grids for each region to create slope, aspect, and north-facing grids.\n",
    "\n",
    "import arcpy\n",
    "from arcpy.sa import *\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "arcpy.env.workspace = akssfol\n",
    "region_folders = arcpy.ListWorkspaces()\n",
    "#region_folders_filter = [x for x in region_folders if \"Bristol_Bay\" in x or \"Kodiak\" in x or \"Prince_William_Sound\" in x]\n",
    "#region_folders_filter = [x for x in region_folders if \"Cook\" in x or \"Copper\" in x]\n",
    "# Re-run for all regions using new surface parameters tool and overwrite existing datasets\n",
    "region_folders_filter = region_folders\n",
    "print(region_folders_filter)\n",
    "\n",
    "for folder in region_folders_filter:\n",
    "    arcpy.env.workspace = folder\n",
    "    elev_raster = folder + \"\\\\elev.tif\"\n",
    "    print(elev_raster)\n",
    "    arcpy.env.snapRaster = elev_raster\n",
    "    aspect2 = arcpy.sa.SurfaceParameters(elev_raster, \"ASPECT\", \"\", 5, \"\", \"METER\")\n",
    "    aspect2.save(folder + \"\\\\aspect.tif\")\n",
    "    slope2 = arcpy.sa.SurfaceParameters(elev_raster, \"SLOPE\", \"\", 5, \"\", \"METER\", \"DEGREE\")\n",
    "    slope2.save(folder + \"\\\\slope.tif\")\n",
    "    aspect_raster = folder + \"\\\\aspect.tif\"\n",
    "    aspect_in = Raster(aspect_raster)\n",
    "    north_raster = Con((aspect_in>=0)&(aspect_in<=45),1,Con((aspect_in<=360)&(aspect_in>=315),1,0))\n",
    "    north_raster.save(\"north.tif\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Glaciers\n",
    "Downloaded RGI version 6.0 from 2017 locally as that would be the most current\n",
    "and best source of glacier outlines.\n",
    " - No need to re-run this as of Nov 18 2021"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "\n",
    "baseFolder = baseFolder\n",
    "rgi = baseFolder + \"Randolph_Glacier_Inventory\\\\01_rgi60_Alaska.shp\"\n",
    "regions = [\"Cook_Inlet\", \"Copper_River\", \"Bristol_Bay\", \"Prince_William_Sound\", \"Kodiak\"]\n",
    "regions = [\"Cook_Inlet\"]\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "#Note, the glims layer has overlapping polygons, just select the glacier lines for Alaska.\n",
    "arcpy.env.workspace = baseFolder\n",
    "\n",
    "for region in regions:\n",
    "    gdb = baseFolder + region + \"\\\\\" + region + \".gdb\"\n",
    "    arcpy.env.workspace = gdb\n",
    "    reg_buf = os.path.join(gdb, \"region_buf\")\n",
    "    print(reg_buf)\n",
    "    glacier_clip = arcpy.Clip_analysis(rgi, reg_buf, \"glaciers\")\n",
    "    print(\"glaciers created for \" + region)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Wetlands\n",
    "\n",
    "For wetlands, first convert NLCD to wetlands 1/0 grid, then mask using the study area for each region.\n",
    "- No need to re-run this as of Nov 18 2021"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "from arcpy.sa import *\n",
    "\n",
    "nlcd = \"K:\\\\GIS_data\\\\biota\\\\Terrestrial\\\\Flora\\\\ak_nlcd_2011_landcover_1_15_15\\\\ak_nlcd_2011_landcover_1_15_15.img\"\n",
    "baseFolder = \"W:\\\\GIS\\\\AKSSF\\\\\"\n",
    "regions = [\"Cook_Inlet\", \"Copper_River\", \"Bristol_Bay\", \"Prince_William_Sound\", \"Kodiak\"]\n",
    "# regions = [x for x in regions if \"Kodiak\" in x]\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "nlcd_ras = Raster(nlcd)\n",
    "wetlands = Con(nlcd_ras == 90, 1, Con(nlcd_ras == 95, 1, 0))\n",
    "\n",
    "for region in regions:\n",
    "    gdb = baseFolder + region + \"\\\\\" + region + \".gdb\"\n",
    "    reg_buf = os.path.join(gdb, \"region_buf\")\n",
    "    print(reg_buf)\n",
    "    arcpy.env.workspace = baseFolder + region\n",
    "    wet_mask = arcpy.sa.ExtractByMask(wetlands, reg_buf)\n",
    "    wet_mask.save(baseFolder + region + \"\\\\wetlands.tif\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}