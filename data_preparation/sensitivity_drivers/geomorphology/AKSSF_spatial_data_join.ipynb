{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This script will be for linking the siteIDs to the catchment IDs - which are either NHDPlusIDs,\n",
    "gridcodes, or new catIDs made from the gridcodes for BB. This will give us a way to link the\n",
    "thermal sensitivity responses by year to the covariates. This script also includes code to\n",
    "check the watersheds. The watersheds are individual feature classes to avoid overlapping\n",
    "polygons so the best way to check they are correct is to 1) check that every siteID/catID\n",
    "has a watershed, and 2) check that the watershed area approximately matches the watershed\n",
    "area from the flow accumulation grid. They won't be exact for sites that are not close to\n",
    "the catchment outlet and could be quite different for very small watersheds in some cases.\n",
    "\n",
    "# Watershed Summaries\n",
    "\n",
    "1. read in all watersheds feature classes\n",
    "2. create a table with the NHDPlusID/catID of the watershed name, region, and watershed area\n",
    "3. merge original point feature classes (inside and outside bb)\n",
    "4. do a spatial join with merged catchments by region to get catID and region on the points dataset\n",
    "5. clip to the region and save in each regional gdb\n",
    "6. merge all point feature classes with siteIDs and catIDs into one table\n",
    "7. join the watershed area from the watershed fcs to the points table using the catID\n",
    "NOT DONE and not sure we need it: 8. join the watershed area from the fac grid to the points table using the catID\n",
    "9. compare the results to ensure that watersheds have been created correctly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kodiak: 28 watersheds\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-21-771fb2423dba>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     16\u001B[0m     \u001B[0mwtdMerge\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[0mwtdMerge\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwtds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m     \u001B[0mwtdMerge\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mlocal_gdb\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\"\\\\\"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0ms\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mwtdMerge\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m     \u001B[0mwtd_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"wtds_merge\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m     \u001B[0marcpy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mMerge_management\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwtdMerge\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwtd_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-21-771fb2423dba>\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     16\u001B[0m     \u001B[0mwtdMerge\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[0mwtdMerge\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwtds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m     \u001B[0mwtdMerge\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mlocal_gdb\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\"\\\\\"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0ms\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mwtdMerge\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m     \u001B[0mwtd_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"wtds_merge\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m     \u001B[0marcpy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mMerge_management\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwtdMerge\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwtd_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "# steps 1 and 2\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "regions = [\"Kodiak\", \"Copper_River\", \"Prince_William_Sound\", \"Cook_Inlet\", \"Bristol_Bay\"]\n",
    "wtdList = []\n",
    "\n",
    "for region in regions:\n",
    "    local_gdb = \"W:\\\\GIS\\\\AKSSF\\\\\" + region + \"\\\\\" + region + \".gdb\\\\Watersheds\"\n",
    "    arcpy.env.workspace = local_gdb\n",
    "    wtds = arcpy.ListFeatureClasses()\n",
    "    print(region + \": \" + str(len(wtds)) + \" watersheds\")\n",
    "\n",
    "    for wtd in wtds:\n",
    "        wtdName = wtd[4:20]\n",
    "        print(\"Starting wtd: \" + wtdName)\n",
    "        wtdPath = os.path.join(arcpy.env.workspace, wtd)\n",
    "        # field_names = [f.name for f in arcpy.ListFields(wtdPath)]\n",
    "        # print(field_names)\n",
    "        # if \"Area_km2\" in field_names:\n",
    "        #     print(\"Area already calculated\")\n",
    "        # else:\n",
    "        #     arcpy.AddField_management(wtdPath, \"Area_km2\", \"DOUBLE\")\n",
    "        #     expression1 = \"{0}\".format(\"!SHAPE.area@SQUAREKILOMETERS!\")\n",
    "        #     arcpy.CalculateField_management(wtdPath, \"Area_km2\", expression1, \"PYTHON\", )\n",
    "        wtdArea = [row[0] for row in arcpy.da.SearchCursor(wtdPath, ['Area_km2'])]\n",
    "        # print(\"wtdName: \" + str(wtdArea))\n",
    "        wtdList.append({'Region': region, 'cat_ID': wtdName, 'Area_km2': wtdArea})\n",
    "\n",
    "wtdDf = pd.DataFrame(wtdList)\n",
    "print(wtdDf)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kodiak: 28 watersheds\n",
      "Copper_River: 28 watersheds\n",
      "Prince_William_Sound: 19 watersheds\n",
      "Cook_Inlet: 241 watersheds\n",
      "Bristol_Bay: 114 watersheds\n"
     ]
    }
   ],
   "source": [
    "#merge watersheds into one feature class for each region\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "import pandas as pd\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "regions = [\"Kodiak\", \"Copper_River\", \"Prince_William_Sound\", \"Cook_Inlet\", \"Bristol_Bay\"]\n",
    "\n",
    "for region in regions:\n",
    "    local_gdb = \"W:\\\\GIS\\\\AKSSF\\\\\" + region + \"\\\\\" + region + \".gdb\"\n",
    "    arcpy.env.workspace = local_gdb + \"\\\\Watersheds\"\n",
    "    wtds = arcpy.ListFeatureClasses()\n",
    "    print(region + \": \" + str(len(wtds)) + \" watersheds\")\n",
    "\n",
    "    #add cat_ID to each watershed before merging\n",
    "    for wtd in wtds:\n",
    "        wtdName = wtd[4:20]\n",
    "        wtdPath = os.path.join(arcpy.env.workspace, wtd)\n",
    "        arcpy.AddField_management(wtdPath, \"cat_ID\", \"DOUBLE\")\n",
    "        # expression1 = \"{0}\".format(\"!SHAPE.area@SQUAREKILOMETERS!\")\n",
    "        arcpy.CalculateField_management(wtdPath, \"cat_ID\", wtdName, \"PYTHON\")\n",
    "\n",
    "    arcpy.env.workspace = local_gdb\n",
    "    wtdMerge = [local_gdb + \"\\\\Watersheds\\\\\" + s for s in wtds]\n",
    "    wtd_output = \"wtds_merge\"\n",
    "    arcpy.Merge_management(wtdMerge, wtd_output)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(wtdDf))\n",
    "print(len(wtdList))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# step 3 - done\n",
    "\n",
    "import arcpy\n",
    "\n",
    "gdb = \"W:\\\\GIS\\\\AKSSF\\\\AKSSF_Hydrography.gdb\"\n",
    "arcpy.env.workspace = gdb\n",
    "bb_pts = gdb + \"\\\\bb_md_verified_DM\"\n",
    "other_pts = gdb + \"\\\\sites_outside_bb_verified_DM\"\n",
    "\n",
    "output = \"akssf_pts_verified\"\n",
    "arcpy.Merge_management([bb_pts, other_pts], output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# steps 4-6\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "regions_dict = {\"Kodiak\": '!gridcode!', \"Copper_River\": '!NHDPlusID!', \"Prince_William_Sound\": '!gridcode!', \"Cook_Inlet\": '!NHDPlusID!', \"Bristol_Bay\": '!catID!'}\n",
    "# regions_dict = {\"Bristol_Bay\": '!catID!'}\n",
    "\n",
    "points = \"W:\\\\GIS\\\\AKSSF\\\\AKSSF_Hydrography.gdb\\\\akssf_pts_verified\"\n",
    "# sites_lst = []\n",
    "cats_lst = []\n",
    "\n",
    "for key, value in regions_dict.items():\n",
    "    arcpy.env.workspace = \"W:\\\\GIS\\\\AKSSF\\\\\" + key + \"\\\\\" + key + \".gdb\"\n",
    "    cats = os.path.join(arcpy.env.workspace, \"cats_merge\")\n",
    "    # arcpy.Clip_analysis(points, cats, r\"memory\\sites_clip\")\n",
    "    # arcpy.SpatialJoin_analysis(r\"memory\\sites_clip\", cats, \"sites_sj\")\n",
    "    # # #note that catID is in the BB cats_merge, but only a LONG, need a DOUBLE to account for NHDPlusIDs\n",
    "    # arcpy.AddField_management(\"sites_sj\", \"cat_ID\", \"DOUBLE\")\n",
    "    # arcpy.CalculateField_management(\"sites_sj\", \"cat_ID\", value)\n",
    "    # sites_fc = os.path.join(arcpy.env.workspace, \"sites_sj\")\n",
    "    # count = arcpy.GetCount_management(sites_fc)\n",
    "    # print('{} has {} records'.format(sites_fc, count[0]))\n",
    "    # sites_lst.append(sites_fc)\n",
    "    cats_lst.append(cats)\n",
    "\n",
    "print(sites_lst)\n",
    "cats_outfile = \"W:\\\\GIS\\\\AKSSF\\\\AKSSF_Hydrography.gdb\\\\all_cats\"\n",
    "arcpy.Merge_management(cats_lst, cats_outfile)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also want to intersect sites with maximum flow accumulation in small buffer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "ExecuteError",
     "evalue": "Failed to execute. Parameters are not valid.\nLocal raster dataset is not supported in this parameter. Please specify a portal item or image service url.\nFailed to execute (ZonalStatisticsAsTable).\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mExecuteError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-20-c8b7df06baf9>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m     \u001B[0mfac\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marcpy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mworkspace\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"fac.tif\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m     \u001B[0mouttable\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mregion\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\"_maxfac.dbf\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m     \u001B[0marcpy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mZonalStatisticsAsTable_ra\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclip_outfile\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfac\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mouttable\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"SiteID\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"DATA\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m\"MAXIMUM\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m     \u001B[0mtbl_lst\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mouttable\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\ra.py\u001B[0m in \u001B[0;36mZonalStatisticsAsTable\u001B[1;34m(inputZoneRasterOrFeatures, inputValueRaster, outputTableName, zoneField, ignoreNodata, statisticType, percentileValues, processAsMultidimensional, percentileInterpolationType)\u001B[0m\n\u001B[0;32m   1190\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mretval\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1191\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1192\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1193\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1194\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\ra.py\u001B[0m in \u001B[0;36mZonalStatisticsAsTable\u001B[1;34m(inputZoneRasterOrFeatures, inputValueRaster, outputTableName, zoneField, ignoreNodata, statisticType, percentileValues, processAsMultidimensional, percentileInterpolationType)\u001B[0m\n\u001B[0;32m   1187\u001B[0m     \u001B[1;32mfrom\u001B[0m \u001B[0marcpy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marcobjects\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marcobjectconversion\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mconvertArcObjectToPythonObject\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1188\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1189\u001B[1;33m         \u001B[0mretval\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconvertArcObjectToPythonObject\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mZonalStatisticsAsTable_ra\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mgp_fixargs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputZoneRasterOrFeatures\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputValueRaster\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutputTableName\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mzoneField\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mignoreNodata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstatisticType\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpercentileValues\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprocessAsMultidimensional\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpercentileInterpolationType\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1190\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mretval\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1191\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\geoprocessing\\_base.py\u001B[0m in \u001B[0;36m<lambda>\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m    510\u001B[0m         \u001B[0mval\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_gp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    511\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcallable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mval\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 512\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[1;32mlambda\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mval\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mgp_fixargs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    513\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    514\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mconvertArcObjectToPythonObject\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mval\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mExecuteError\u001B[0m: Failed to execute. Parameters are not valid.\nLocal raster dataset is not supported in this parameter. Please specify a portal item or image service url.\nFailed to execute (ZonalStatisticsAsTable).\n"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "import pandas as pd\n",
    "cats_outfile = \"W:\\\\GIS\\\\AKSSF\\\\AKSSF_Hydrography.gdb\\\\all_cats\"\n",
    "\n",
    "#merge all sites_sj into one point file with all the catIDs.\n",
    "outfile = \"W:\\\\GIS\\\\AKSSF\\\\AKSSF_Hydrography.gdb\\\\all_sites_catid\"\n",
    "arcpy.Merge_management(sites_lst, outfile, \"\", \"ADD_SOURCE_INFO\")\n",
    "\n",
    "#buffer sites\n",
    "buffer = r\"memory\\sites_buffer\"\n",
    "arcpy.Buffer_analysis(outfile, buffer, \"30 meters\")\n",
    "\n",
    "#this makes sure the buffer does not extend outside of the catchment\n",
    "clip_outfile = r\"memory\\sites_clip\"\n",
    "arcpy.Clip_analysis(buffer, cats_outfile, clip_outfile)\n",
    "\n",
    "#zonal stats on buffer intersection with flow accumulation grids (need to loop through regions for this)\n",
    "# to get maximum flow accumulation bc some points may not exactly fall on stream grid\n",
    "tbl_lst = []\n",
    "\n",
    "for region in regions:\n",
    "    arcpy.env.workspace = \"W:\\\\GIS\\\\AKSSF\\\\\" + region + \"\\\\\"\n",
    "    fac = os.path.join(arcpy.env.workspace, \"fac.tif\")\n",
    "    outtable = region + \"_maxfac.dbf\"\n",
    "    arcpy.ZonalStatisticsAsTable_ra(clip_outfile, fac, outtable, \"SiteID\", \"DATA\",\"MAXIMUM\")\n",
    "    tbl_lst.append(outtable)\n",
    "\n",
    "arr_lst = []\n",
    "for tbl in tbl_lst:\n",
    "    arr = arcpy.da.TableToNumPyArray(outtable, (\"siteID\", \"cat_ID\"))\n",
    "    arr_lst.append(arr)\n",
    "sitesDF = pd.concat(arr_lst)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# steps 7-9\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "\n",
    "arcpy.env.workspace = \"W:\\\\GIS\\\\AKSSF\"\n",
    "# print(wtdDf)\n",
    "# print(sitesDF)\n",
    "\n",
    "print(wtdDf.dtypes)\n",
    "print(sitesDF.dtypes)\n",
    "\n",
    "#fix mismatched data types for merge field.\n",
    "# wtdDf.cat_ID.astype('int64')\n",
    "wtdDf['cat_ID'] = wtdDf['cat_ID'].astype('int64')\n",
    "#merge sitesDf with wtdDf to get watershed area linked to siteID\n",
    "join = sitesDF.merge(wtdDf, on = 'cat_ID', how = 'left', indicator = True)\n",
    "print(join)\n",
    "\n",
    "join.to_csv(\"W:\\\\GIS\\\\AKSSF\\\\site_wtd_join.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}