{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This script will be for linking the siteIDs to the catchment IDs - which are either NHDPlusIDs,\n",
    "gridcodes, or new catIDs made from the gridcodes for BB. This will give us a way to link the\n",
    "thermal sensitivity responses by year to the covariates. This script also includes code to\n",
    "check the watersheds. The watersheds are individual feature classes to avoid overlapping\n",
    "polygons so the best way to check they are correct is to 1) check that every siteID/catID\n",
    "has a watershed, and 2) check that the watershed area approximately matches the watershed\n",
    "area from the flow accumulation grid. They won't be exact for sites that are not close to\n",
    "the catchment outlet and could be quite different for very small watersheds in some cases.\n",
    "\n",
    "# Watershed Summaries\n",
    "\n",
    "1. read in all watersheds feature classes\n",
    "2. create a table with the NHDPlusID/catID of the watershed name, region, and watershed area\n",
    "3. merge original point feature classes (inside and outside bb)\n",
    "4. do a spatial join with merged catchments by region to get catID and region on the points dataset\n",
    "5. clip to the region and save in each regional gdb\n",
    "6. merge all point feature classes with siteIDs and catIDs into one table\n",
    "7. join the watershed area from the watershed fcs to the points table using the catID\n",
    "NOT DONE and not sure we need it: 8. join the watershed area from the fac grid to the points table using the catID\n",
    "9. compare the results to ensure that watersheds have been created correctly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kodiak: 28 watersheds\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-21-771fb2423dba>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     16\u001B[0m     \u001B[0mwtdMerge\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[0mwtdMerge\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwtds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m     \u001B[0mwtdMerge\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mlocal_gdb\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\"\\\\\"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0ms\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mwtdMerge\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m     \u001B[0mwtd_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"wtds_merge\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m     \u001B[0marcpy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mMerge_management\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwtdMerge\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwtd_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-21-771fb2423dba>\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     16\u001B[0m     \u001B[0mwtdMerge\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[0mwtdMerge\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwtds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m     \u001B[0mwtdMerge\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mlocal_gdb\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\"\\\\\"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0ms\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mwtdMerge\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m     \u001B[0mwtd_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"wtds_merge\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m     \u001B[0marcpy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mMerge_management\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwtdMerge\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwtd_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "# steps 1 and 2\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "regions = [\"Kodiak\", \"Copper_River\", \"Prince_William_Sound\", \"Cook_Inlet\", \"Bristol_Bay\"]\n",
    "wtdList = []\n",
    "\n",
    "for region in regions:\n",
    "    local_gdb = \"W:\\\\GIS\\\\AKSSF\\\\\" + region + \"\\\\\" + region + \".gdb\\\\Watersheds\"\n",
    "    arcpy.env.workspace = local_gdb\n",
    "    wtds = arcpy.ListFeatureClasses()\n",
    "    print(region + \": \" + str(len(wtds)) + \" watersheds\")\n",
    "\n",
    "    for wtd in wtds:\n",
    "        wtdName = wtd[4:20]\n",
    "        print(\"Starting wtd: \" + wtdName)\n",
    "        wtdPath = os.path.join(arcpy.env.workspace, wtd)\n",
    "        # field_names = [f.name for f in arcpy.ListFields(wtdPath)]\n",
    "        # print(field_names)\n",
    "        # if \"Area_km2\" in field_names:\n",
    "        #     print(\"Area already calculated\")\n",
    "        # else:\n",
    "        #     arcpy.AddField_management(wtdPath, \"Area_km2\", \"DOUBLE\")\n",
    "        #     expression1 = \"{0}\".format(\"!SHAPE.area@SQUAREKILOMETERS!\")\n",
    "        #     arcpy.CalculateField_management(wtdPath, \"Area_km2\", expression1, \"PYTHON\", )\n",
    "        wtdArea = [row[0] for row in arcpy.da.SearchCursor(wtdPath, ['Area_km2'])]\n",
    "        # print(\"wtdName: \" + str(wtdArea))\n",
    "        wtdList.append({'Region': region, 'cat_ID': wtdName, 'Area_km2': wtdArea})\n",
    "\n",
    "wtdDf = pd.DataFrame(wtdList)\n",
    "print(wtdDf)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kodiak: 28 watersheds\n",
      "Copper_River: 28 watersheds\n",
      "Prince_William_Sound: 19 watersheds\n",
      "Cook_Inlet: 241 watersheds\n",
      "Bristol_Bay: 114 watersheds\n"
     ]
    }
   ],
   "source": [
    "#merge watersheds into one feature class for each region\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "import pandas as pd\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "regions = [\"Kodiak\", \"Copper_River\", \"Prince_William_Sound\", \"Cook_Inlet\", \"Bristol_Bay\"]\n",
    "\n",
    "for region in regions:\n",
    "    local_gdb = \"W:\\\\GIS\\\\AKSSF\\\\\" + region + \"\\\\\" + region + \".gdb\"\n",
    "    arcpy.env.workspace = local_gdb + \"\\\\Watersheds\"\n",
    "    wtds = arcpy.ListFeatureClasses()\n",
    "    print(region + \": \" + str(len(wtds)) + \" watersheds\")\n",
    "\n",
    "    #add cat_ID to each watershed before merging\n",
    "    for wtd in wtds:\n",
    "        wtdName = wtd[4:20]\n",
    "        wtdPath = os.path.join(arcpy.env.workspace, wtd)\n",
    "        arcpy.AddField_management(wtdPath, \"cat_ID\", \"DOUBLE\")\n",
    "        # expression1 = \"{0}\".format(\"!SHAPE.area@SQUAREKILOMETERS!\")\n",
    "        arcpy.CalculateField_management(wtdPath, \"cat_ID\", wtdName, \"PYTHON\")\n",
    "\n",
    "    arcpy.env.workspace = local_gdb\n",
    "    wtdMerge = [local_gdb + \"\\\\Watersheds\\\\\" + s for s in wtds]\n",
    "    wtd_output = \"wtds_merge\"\n",
    "    arcpy.Merge_management(wtdMerge, wtd_output)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(wtdDf))\n",
    "print(len(wtdList))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# step 3 - done\n",
    "\n",
    "import arcpy\n",
    "\n",
    "gdb = \"W:\\\\GIS\\\\AKSSF\\\\AKSSF_Hydrography.gdb\"\n",
    "arcpy.env.workspace = gdb\n",
    "bb_pts = gdb + \"\\\\bb_md_verified_DM\"\n",
    "other_pts = gdb + \"\\\\sites_outside_bb_verified_DM\"\n",
    "\n",
    "output = \"akssf_pts_verified\"\n",
    "arcpy.Merge_management([bb_pts, other_pts], output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# steps 4-6\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "arcpy.env.overwriteOutput = True\n",
    "regions_dict = {\"Kodiak\": '!gridcode!', \"Copper_River\": '!NHDPlusID!', \"Prince_William_Sound\": '!gridcode!', \"Cook_Inlet\": '!NHDPlusID!', \"Bristol_Bay\": '!catID!'}\n",
    "# regions_dict = {\"Bristol_Bay\": '!catID!'}\n",
    "\n",
    "points = \"W:\\\\GIS\\\\AKSSF\\\\AKSSF_Hydrography.gdb\\\\akssf_pts_verified\"\n",
    "# sites_lst = []\n",
    "cats_lst = []\n",
    "\n",
    "for key, value in regions_dict.items():\n",
    "    arcpy.env.workspace = \"W:\\\\GIS\\\\AKSSF\\\\\" + key + \"\\\\\" + key + \".gdb\"\n",
    "    cats = os.path.join(arcpy.env.workspace, \"cats_merge\")\n",
    "    # arcpy.Clip_analysis(points, cats, r\"memory\\sites_clip\")\n",
    "    # arcpy.SpatialJoin_analysis(r\"memory\\sites_clip\", cats, \"sites_sj\")\n",
    "    # # #note that catID is in the BB cats_merge, but only a LONG, need a DOUBLE to account for NHDPlusIDs\n",
    "    # arcpy.AddField_management(\"sites_sj\", \"cat_ID\", \"DOUBLE\")\n",
    "    # arcpy.CalculateField_management(\"sites_sj\", \"cat_ID\", value)\n",
    "    # sites_fc = os.path.join(arcpy.env.workspace, \"sites_sj\")\n",
    "    # count = arcpy.GetCount_management(sites_fc)\n",
    "    # print('{} has {} records'.format(sites_fc, count[0]))\n",
    "    # sites_lst.append(sites_fc)\n",
    "    cats_lst.append(cats)\n",
    "\n",
    "print(sites_lst)\n",
    "cats_outfile = \"W:\\\\GIS\\\\AKSSF\\\\AKSSF_Hydrography.gdb\\\\all_cats\"\n",
    "arcpy.Merge_management(cats_lst, cats_outfile)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## DM - Picked up here and modified to match local data\n",
    "### Also want to intersect sites with maximum flow accumulation in small (30 meters) buffer.\n",
    "\n",
    "\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "import os\n",
    "# Setting outgdb to location created in Covariate notebook but any temp gdb will work\n",
    "outgdb = r\"D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\"\n",
    "# Set data dir = to folder with regional subfolders and gdbs\n",
    "data_dir = r\"D:\\GIS_Temp\\AKSSF\"\n",
    "arcpy.env.workspace = data_dir\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338) #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "regions  = arcpy.ListWorkspaces(workspace_type=\"Folder\")\n",
    "\n",
    "# Create lists to store paths to data copies to be merged\n",
    "site_lst = []\n",
    "cats_lst = []\n",
    "\n",
    "# Walk through gdbs and select sites_sj and merge into single fc\n",
    "# Seperate data by\n",
    "nhdplus_dat = ['Cook_Inlet','Copper_River']\n",
    "tauDem_dat = ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound']\n",
    "# Loop through all processing areas\n",
    "rois = nhdplus_dat + tauDem_dat\n",
    "for roi in rois:\n",
    "    # Loop through regional folders\n",
    "    for region in regions:\n",
    "        if roi in str(region):\n",
    "            if roi in nhdplus_dat:\n",
    "                cat_cur_fields = ['cat_ID_txt', 'NHDPlusID',\"cat_ID_con\"]\n",
    "                sj_cur_fields = ['cat_ID_txt', 'cat_ID',\"cat_ID_con\"]\n",
    "                wtd_cur_fields = ['cat_ID_txt', 'cat_ID',\"cat_ID_con\"]\n",
    "                print (f'{roi} in {nhdplus_dat} AKSSF list, using cat_fields {cat_cur_fields} and watershed fields {sj_cur_fields}')\n",
    "                print('----------')\n",
    "            # Set data and variables unique to regions with TauDEM Data\n",
    "            elif roi in tauDem_dat:\n",
    "                # Fields for update cursor\n",
    "                cat_cur_fields = ['cat_ID_txt', 'gridcode',\"cat_ID_con\"]\n",
    "                sj_cur_fields = ['cat_ID_txt', 'cat_ID',\"cat_ID_con\"]\n",
    "                wtd_cur_fields = ['cat_ID_txt', 'cat_ID',\"cat_ID_con\"]\n",
    "                print (f'{roi} in {tauDem_dat} TauDEM list, using cat_fields {cat_cur_fields} and watershed fields {sj_cur_fields}')\n",
    "                print('----------')\n",
    "            print(f'{roi} using data from {region} folder')\n",
    "            arcpy.env.workspace = region\n",
    "            gdb = arcpy.ListWorkspaces(workspace_type='FileGDB')\n",
    "            walk = arcpy.da.Walk(region, datatype = 'FeatureClass')\n",
    "            for dirpath, dirnames, filenames in walk:\n",
    "                for filename in filenames:\n",
    "                    # Create list of watersheds\n",
    "                    if filename == 'sites_sj':\n",
    "                        print(f'{dirpath},{filename}')\n",
    "                        sitesource = os.path.join(dirpath, filename)\n",
    "                        sitename = roi +'_' +filename\n",
    "                        sitepath = os.path.join(outgdb,sitename)\n",
    "                        # Make local copy projected in AKAlbers\n",
    "                        if not arcpy.Exists(sitepath):\n",
    "                            print(f'Copying {sitename}...')\n",
    "                            print('----------')\n",
    "                            sitecopy = arcpy.FeatureClassToFeatureClass_conversion(sitesource,outgdb,sitename)\n",
    "                        else:\n",
    "                            print(f'{sitepath} already created')\n",
    "                            sitecopy = sitepath\n",
    "                            print('----------')\n",
    "                        site_lst.append(sitecopy)\n",
    "                        sitefieldnames = []\n",
    "                        sitelstFields = arcpy.ListFields(sitecopy)\n",
    "                        for field in sitelstFields:\n",
    "                            sitefieldnames.append(field.name)\n",
    "                        if str(sj_cur_fields[0]) in sitefieldnames:\n",
    "                            print (f'{sj_cur_fields[0]} field already in dataset')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print (f'Adding {sj_cur_fields[0]} field to watershed dataset {sitecopy}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_txt field and concat cat_ID + region\n",
    "                            arcpy.AddField_management(sitecopy, str(sj_cur_fields[0]),field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(sitecopy, sj_cur_fields[0:2]) as cur:\n",
    "                                for row in cur:\n",
    "                                    strval = str(row[1])\n",
    "                                    row[0] = strval.replace('.0','')\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del(row)\n",
    "                            del(cur)\n",
    "                        if str(sj_cur_fields[2]) in sitefieldnames:\n",
    "                            print (f'{sj_cur_fields[2]} field already in dataset {sitecopy}')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print (f'Adding {sj_cur_fields[2]} field to watershed dataset {sitecopy}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_con field and concat cat_ID + region\n",
    "                            arcpy.AddField_management(sitecopy, str(sj_cur_fields[2]),field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(sitecopy, sj_cur_fields) as cur:\n",
    "                                for row in cur:\n",
    "                                    strval = str(row[1])\n",
    "                                    row[2] = str(roi) +'_'+ strval.replace('.0','')\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del(row)\n",
    "                            del(cur)\n",
    "                                        # Set merged watersheds dataset\n",
    "                    elif 'wtds_merge' == filename:\n",
    "                        wtdsource = os.path.join(dirpath, filename)\n",
    "                        wtdname = roi +'_'+filename\n",
    "                        wtdpath = os.path.join(outgdb,wtdname)\n",
    "\n",
    "                        if not arcpy.Exists(wtdpath):\n",
    "                            print(f'Copying {wtdname}...')\n",
    "                            print('----------')\n",
    "                           # Make local copy projected in AKAlbers\n",
    "                            wtd_merge = arcpy.FeatureClassToFeatureClass_conversion(wtdsource, outgdb,wtdname)\n",
    "                        else:\n",
    "                            print(f'Merged watershed dataset {wtdpath} already created...')\n",
    "                            print('----------')\n",
    "                            wtd_merge = wtdpath\n",
    "                        wtdfieldnames = []\n",
    "                        wtdlstFields = arcpy.ListFields(wtd_merge)\n",
    "                        for field in wtdlstFields:\n",
    "                            wtdfieldnames.append(field.name)\n",
    "                        if str(wtd_cur_fields[0]) in wtdfieldnames:\n",
    "                            print (f'{wtd_cur_fields[0]} field already in dataset')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print (f'Adding {wtd_cur_fields[0]} field to watershed dataset {wtd_merge}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_txt field and concat cat_ID + region\n",
    "                            arcpy.AddField_management(wtd_merge, str(wtd_cur_fields[0]),field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(wtd_merge, wtd_cur_fields[0:2]) as cur:\n",
    "                                for row in cur:\n",
    "                                    strval = str(row[1])\n",
    "                                    row[2] = strval.replace('.0','')\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del(row)\n",
    "                            del(cur)\n",
    "                        if str(wtd_cur_fields[2]) in wtdfieldnames:\n",
    "                            print (f'{wtd_cur_fields[2]} field already in dataset {wtd_merge}')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print (f'Adding {wtd_cur_fields[2]} field to watershed dataset {wtd_merge}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_con field and concat cat_ID + region\n",
    "                            arcpy.AddField_management(wtd_merge, str(wtd_cur_fields[2]),field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(wtd_merge, wtd_cur_fields) as cur:\n",
    "                                for row in cur:\n",
    "                                    strval = str(row[1])\n",
    "                                    row[2] = str(roi) +'_'+ strval.replace(\".0\",\"\")\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del(row)\n",
    "                            del(cur)\n",
    "\n",
    "\n",
    "                    # Select catch_int fc (catchments of interest for region) and make a copy\n",
    "                    elif 'cats_intersect' == filename:\n",
    "                        # Make local copy projected in AKAlbers\n",
    "                        catssource = os.path.join(dirpath,filename)\n",
    "                        catsname = roi+\"_\"+filename\n",
    "                        catspath = os.path.join(outgdb,catsname)\n",
    "                        if not arcpy.Exists(catspath):\n",
    "                            print(f'Copying {catsname}...')\n",
    "                            print('----------')\n",
    "                            cats = arcpy.FeatureClassToFeatureClass_conversion(catssource, outgdb,catsname)\n",
    "                        else:\n",
    "                            print (f'{catspath} already created')\n",
    "                            print('----------')\n",
    "                            cats = catspath\n",
    "                        cats_lst.append(cats)\n",
    "                        catlstfields = arcpy.ListFields(cats)\n",
    "                        catfieldnames = []\n",
    "                        for field in catlstfields:\n",
    "                            catfieldnames.append(field.name)\n",
    "                        if str(cat_cur_fields[0]) in catfieldnames:\n",
    "                            print (f'{cat_cur_fields[0]} field already in dataset {cats}')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print (f'Adding {cat_cur_fields[0]} field to catchment dataset {cats}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_txt field\n",
    "                            arcpy.AddField_management(cats, str(cat_cur_fields[0]), field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(cats, cat_cur_fields[0:2]) as cur:\n",
    "                                for row in cur:\n",
    "                                    strval = str(row[1])\n",
    "                                    row[0] = strval.replace('.0','')\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del(row)\n",
    "                            del(cur)\n",
    "                        if str(cat_cur_fields[2]) in catfieldnames:\n",
    "                            print (f'{cat_cur_fields[2]} field already in dataset {cats}')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print (f'Adding {cat_cur_fields[2]} field to catchment dataset {cats}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_txt field & cat_ID + region concat field\n",
    "                            arcpy.AddField_management(cats,str(cat_cur_fields[2]),field_type='TEXT')\n",
    "                            # populate cat_ID_con\n",
    "                            with arcpy.da.UpdateCursor(cats, cat_cur_fields) as cur:\n",
    "                                for row in cur:\n",
    "                                    strval = str(row[1])\n",
    "                                    row[2] = str(roi) +'_'+ strval.replace(\".0\",\"\")\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del(row)\n",
    "                            del(cur)\n",
    "print(cats_lst)\n",
    "print(site_lst)\n",
    "outcats = os.path.join(outgdb, \"all_cats_int_merge\")\n",
    "outsites = os.path.join(outgdb,\"all_sites_sj_merge\")\n",
    "sitesmerge = arcpy.Merge_management(site_lst, outsites, \"\", \"ADD_SOURCE_INFO\")\n",
    "catsmerge = arcpy.Merge_management(cats_lst, outcats, \"\", \"ADD_SOURCE_INFO\")\n",
    "print(f\"Merge complete: {sitesmerge}\"\n",
    "      f\"{catsmerge} created...\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cook_Inlet in ['Cook_Inlet', 'Copper_River'] AKSSF list, using cat_fields ['cat_ID_txt', 'NHDPlusID', 'cat_ID_con'] and watershed fields ['cat_ID_txt', 'cat_ID', 'cat_ID_con']\n",
      "----------\n",
      "Cook_Inlet using data from D:\\GIS_temp\\AKSSF\\Cook_Inlet folder\n",
      "D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Cook_Inlet_cats_intersect already created\n",
      "----------\n",
      "cat_ID_txt field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Cook_Inlet_cats_intersect\n",
      "----------\n",
      "cat_ID_con field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Cook_Inlet_cats_intersect\n",
      "----------\n",
      "D:\\GIS_temp\\AKSSF\\Cook_Inlet\\Cook_Inlet.gdb,sites_sj\n",
      "D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Cook_Inlet_sites_sj already created\n",
      "----------\n",
      "cat_ID_txt field already in dataset\n",
      "----------\n",
      "cat_ID_con field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Cook_Inlet_sites_sj\n",
      "----------\n",
      "Merged watershed dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Cook_Inlet_wtds_merge already created...\n",
      "----------\n",
      "cat_ID_txt field already in dataset\n",
      "----------\n",
      "cat_ID_con field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Cook_Inlet_wtds_merge\n",
      "----------\n",
      "Copper_River in ['Cook_Inlet', 'Copper_River'] AKSSF list, using cat_fields ['cat_ID_txt', 'NHDPlusID', 'cat_ID_con'] and watershed fields ['cat_ID_txt', 'cat_ID', 'cat_ID_con']\n",
      "----------\n",
      "Copper_River using data from D:\\GIS_temp\\AKSSF\\Copper_River folder\n",
      "D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Copper_River_cats_intersect already created\n",
      "----------\n",
      "cat_ID_txt field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Copper_River_cats_intersect\n",
      "----------\n",
      "cat_ID_con field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Copper_River_cats_intersect\n",
      "----------\n",
      "D:\\GIS_temp\\AKSSF\\Copper_River\\Copper_River.gdb,sites_sj\n",
      "D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Copper_River_sites_sj already created\n",
      "----------\n",
      "cat_ID_txt field already in dataset\n",
      "----------\n",
      "cat_ID_con field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Copper_River_sites_sj\n",
      "----------\n",
      "Merged watershed dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Copper_River_wtds_merge already created...\n",
      "----------\n",
      "cat_ID_txt field already in dataset\n",
      "----------\n",
      "cat_ID_con field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Copper_River_wtds_merge\n",
      "----------\n",
      "Bristol_Bay in ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound'] TauDEM list, using cat_fields ['cat_ID_txt', 'gridcode', 'cat_ID_con'] and watershed fields ['cat_ID_txt', 'cat_ID', 'cat_ID_con']\n",
      "----------\n",
      "Bristol_Bay using data from D:\\GIS_temp\\AKSSF\\Bristol_Bay folder\n",
      "D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Bristol_Bay_cats_intersect already created\n",
      "----------\n",
      "cat_ID_txt field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Bristol_Bay_cats_intersect\n",
      "----------\n",
      "cat_ID_con field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Bristol_Bay_cats_intersect\n",
      "----------\n",
      "D:\\GIS_temp\\AKSSF\\Bristol_Bay\\Bristol_Bay.gdb,sites_sj\n",
      "D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Bristol_Bay_sites_sj already created\n",
      "----------\n",
      "cat_ID_txt field already in dataset\n",
      "----------\n",
      "cat_ID_con field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Bristol_Bay_sites_sj\n",
      "----------\n",
      "Merged watershed dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Bristol_Bay_wtds_merge already created...\n",
      "----------\n",
      "cat_ID_txt field already in dataset\n",
      "----------\n",
      "cat_ID_con field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Bristol_Bay_wtds_merge\n",
      "----------\n",
      "Kodiak in ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound'] TauDEM list, using cat_fields ['cat_ID_txt', 'gridcode', 'cat_ID_con'] and watershed fields ['cat_ID_txt', 'cat_ID', 'cat_ID_con']\n",
      "----------\n",
      "Kodiak using data from D:\\GIS_temp\\AKSSF\\Kodiak folder\n",
      "D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Kodiak_cats_intersect already created\n",
      "----------\n",
      "cat_ID_txt field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Kodiak_cats_intersect\n",
      "----------\n",
      "cat_ID_con field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Kodiak_cats_intersect\n",
      "----------\n",
      "D:\\GIS_temp\\AKSSF\\Kodiak\\Kodiak.gdb,sites_sj\n",
      "D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Kodiak_sites_sj already created\n",
      "----------\n",
      "cat_ID_txt field already in dataset\n",
      "----------\n",
      "cat_ID_con field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Kodiak_sites_sj\n",
      "----------\n",
      "Merged watershed dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Kodiak_wtds_merge already created...\n",
      "----------\n",
      "cat_ID_txt field already in dataset\n",
      "----------\n",
      "cat_ID_con field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Kodiak_wtds_merge\n",
      "----------\n",
      "Prince_William_Sound in ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound'] TauDEM list, using cat_fields ['cat_ID_txt', 'gridcode', 'cat_ID_con'] and watershed fields ['cat_ID_txt', 'cat_ID', 'cat_ID_con']\n",
      "----------\n",
      "Prince_William_Sound using data from D:\\GIS_temp\\AKSSF\\Prince_William_Sound folder\n",
      "D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Prince_William_Sound_cats_intersect already created\n",
      "----------\n",
      "cat_ID_txt field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Prince_William_Sound_cats_intersect\n",
      "----------\n",
      "cat_ID_con field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Prince_William_Sound_cats_intersect\n",
      "----------\n",
      "D:\\GIS_temp\\AKSSF\\Prince_William_Sound\\Prince_William_Sound.gdb,sites_sj\n",
      "D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Prince_William_Sound_sites_sj already created\n",
      "----------\n",
      "cat_ID_txt field already in dataset\n",
      "----------\n",
      "cat_ID_con field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Prince_William_Sound_sites_sj\n",
      "----------\n",
      "Merged watershed dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Prince_William_Sound_wtds_merge already created...\n",
      "----------\n",
      "cat_ID_txt field already in dataset\n",
      "----------\n",
      "cat_ID_con field already in dataset D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\Prince_William_Sound_wtds_merge\n",
      "----------\n",
      "['D:\\\\\\\\GIS_temp\\\\\\\\AKSSF_land_met\\\\\\\\AKSSF_land_met.gdb\\\\Cook_Inlet_cats_intersect', 'D:\\\\\\\\GIS_temp\\\\\\\\AKSSF_land_met\\\\\\\\AKSSF_land_met.gdb\\\\Copper_River_cats_intersect', 'D:\\\\\\\\GIS_temp\\\\\\\\AKSSF_land_met\\\\\\\\AKSSF_land_met.gdb\\\\Bristol_Bay_cats_intersect', 'D:\\\\\\\\GIS_temp\\\\\\\\AKSSF_land_met\\\\\\\\AKSSF_land_met.gdb\\\\Kodiak_cats_intersect', 'D:\\\\\\\\GIS_temp\\\\\\\\AKSSF_land_met\\\\\\\\AKSSF_land_met.gdb\\\\Prince_William_Sound_cats_intersect']\n",
      "['D:\\\\\\\\GIS_temp\\\\\\\\AKSSF_land_met\\\\\\\\AKSSF_land_met.gdb\\\\Cook_Inlet_sites_sj', 'D:\\\\\\\\GIS_temp\\\\\\\\AKSSF_land_met\\\\\\\\AKSSF_land_met.gdb\\\\Copper_River_sites_sj', 'D:\\\\\\\\GIS_temp\\\\\\\\AKSSF_land_met\\\\\\\\AKSSF_land_met.gdb\\\\Bristol_Bay_sites_sj', 'D:\\\\\\\\GIS_temp\\\\\\\\AKSSF_land_met\\\\\\\\AKSSF_land_met.gdb\\\\Kodiak_sites_sj', 'D:\\\\\\\\GIS_temp\\\\\\\\AKSSF_land_met\\\\\\\\AKSSF_land_met.gdb\\\\Prince_William_Sound_sites_sj']\n",
      "Merge complete: D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\all_sites_sj_mergeD:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\\all_cats_int_merge created...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cook_Inlet in ['Cook_Inlet', 'Copper_River'] AKSSF list, using cat_fields ['cat_ID_txt', 'NHDPlusID', 'cat_ID_con'] and spatial join fields ['cat_ID_txt', 'cat_ID', 'cat_ID_con']\n",
      "----------\n",
      "Cook_Inlet using data from D:\\GIS_temp\\AKSSF\\Cook_Inlet folder\n",
      "--------\n",
      "Flow accumulation grid D:\\GIS_temp\\AKSSF\\Cook_Inlet\\fac.tif selected for Cook_Inlet\n",
      "Calculating Cook_Inlet catchment Max flow accumulation stats...\n",
      "Max Flow zonal stats for Cook_Inlet Elapsed time: (0:22:41)\n",
      "----------\n",
      "Copper_River in ['Cook_Inlet', 'Copper_River'] AKSSF list, using cat_fields ['cat_ID_txt', 'NHDPlusID', 'cat_ID_con'] and spatial join fields ['cat_ID_txt', 'cat_ID', 'cat_ID_con']\n",
      "----------\n",
      "Copper_River using data from D:\\GIS_temp\\AKSSF\\Copper_River folder\n",
      "--------\n",
      "Flow accumulation grid D:\\GIS_temp\\AKSSF\\Copper_River\\fac.tif selected for Copper_River\n",
      "Calculating Copper_River catchment Max flow accumulation stats...\n",
      "Max Flow zonal stats for Copper_River Elapsed time: (0:00:53)\n",
      "----------\n",
      "Bristol_Bay in ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound'] TauDEM list, using cat_fields ['cat_ID_txt', 'gridcode', 'cat_ID_con'] and spatial join fields ['cat_ID_txt', 'cat_ID', 'cat_ID_con']\n",
      "----------\n",
      "Bristol_Bay using data from D:\\GIS_temp\\AKSSF\\Bristol_Bay folder\n",
      "--------\n",
      "Flow accumulation grid D:\\GIS_temp\\AKSSF\\Bristol_Bay\\fac.tif selected for Bristol_Bay\n",
      "Calculating Bristol_Bay catchment Max flow accumulation stats...\n",
      "Max Flow zonal stats for Bristol_Bay Elapsed time: (0:00:28)\n",
      "----------\n",
      "Kodiak in ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound'] TauDEM list, using cat_fields ['cat_ID_txt', 'gridcode', 'cat_ID_con'] and spatial join fields ['cat_ID_txt', 'cat_ID', 'cat_ID_con']\n",
      "----------\n",
      "Kodiak using data from D:\\GIS_temp\\AKSSF\\Kodiak folder\n",
      "--------\n",
      "Flow accumulation grid D:\\GIS_temp\\AKSSF\\Kodiak\\fac.tif selected for Kodiak\n",
      "Calculating Kodiak catchment Max flow accumulation stats...\n",
      "Max Flow zonal stats for Kodiak Elapsed time: (0:00:08)\n",
      "----------\n",
      "Prince_William_Sound in ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound'] TauDEM list, using cat_fields ['cat_ID_txt', 'gridcode', 'cat_ID_con'] and spatial join fields ['cat_ID_txt', 'cat_ID', 'cat_ID_con']\n",
      "----------\n",
      "Prince_William_Sound using data from D:\\GIS_temp\\AKSSF\\Prince_William_Sound folder\n",
      "--------\n",
      "Flow accumulation grid D:\\GIS_temp\\AKSSF\\Prince_William_Sound\\fac.tif selected for Prince_William_Sound\n",
      "Calculating Prince_William_Sound catchment Max flow accumulation stats...\n",
      "Max Flow zonal stats for Prince_William_Sound Elapsed time: (0:00:09)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "import os\n",
    "from arcpy.sa import *\n",
    "# Setting outgdb to location created in Covariate notebook but any temp gdb will work\n",
    "outgdb = r\"D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\"\n",
    "# Set data dir = to folder with regional subfolders and gdbs\n",
    "data_dir = r\"D:\\GIS_Temp\\AKSSF\"\n",
    "arcpy.env.workspace = data_dir\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338) #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "regions  = arcpy.ListWorkspaces(workspace_type=\"Folder\")\n",
    "\n",
    "#buffer sites\n",
    "bufpath = os.path.join(outgdb,\"sites_buffer\")\n",
    "buffer = arcpy.Buffer_analysis(sitesmerge, bufpath, \"30 meters\")\n",
    "\n",
    "# this makes sure the buffer does not extend outside of the catchment - cannot use memory/ object as input for zonal stats\n",
    "# must write to gdb \n",
    "clip_path = os.path.join(outgdb,'all_buffsites_clip')\n",
    "clip_outfile = arcpy.Clip_analysis(buffer, catsmerge, clip_path)\n",
    "\n",
    "\n",
    "#zonal stats on buffer intersection with flow accumulation grids (need to loop through regions for this)\n",
    "# to get maximum flow accumulation bc some points may not exactly fall on stream grid\n",
    "tbl_lst = []\n",
    "# Seperate data by\n",
    "nhdplus_dat = ['Cook_Inlet','Copper_River']\n",
    "tauDem_dat = ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound']\n",
    "\n",
    "# Loop through all processing areas\n",
    "rois = nhdplus_dat + tauDem_dat\n",
    "\n",
    "for roi in rois:\n",
    "    # Loop through regional folders\n",
    "    for region in regions:\n",
    "        arcpy.env.workspace = region\n",
    "        raslist = arcpy.ListRasters()\n",
    "        if roi in str(region):\n",
    "            if roi in nhdplus_dat:\n",
    "                cat_cur_fields = ['cat_ID_txt', 'NHDPlusID',\"cat_ID_con\"]\n",
    "                cf = 25\n",
    "                print (f'{roi} in {nhdplus_dat} AKSSF list, using cat_fields {cat_cur_fields} and spatial join fields {sj_cur_fields}')\n",
    "                print('----------')\n",
    "            # Set data and variables unique to regions with TauDEM Data\n",
    "            elif roi in tauDem_dat:\n",
    "                # Fields for update cursor\n",
    "                cat_cur_fields = ['cat_ID_txt', 'gridcode',\"cat_ID_con\"]\n",
    "                cf = 100\n",
    "                print (f'{roi} in {tauDem_dat} TauDEM list, using cat_fields {cat_cur_fields} and spatial join fields {sj_cur_fields}')\n",
    "                print('----------')\n",
    "            print(f'{roi} using data from {region} folder')\n",
    "            print('--------')\n",
    "            fac = os.path.join(region, 'fac.tif')\n",
    "            if arcpy.Exists(fac):\n",
    "                print (f'Flow accumulation grid {fac} selected for {roi}')\n",
    "            else:\n",
    "                print (f'No flow accumulation raster for {region}')\n",
    "\n",
    "\n",
    "            outtable = os.path.join(outgdb,(roi + \"_maxfac\"))\n",
    "            print(f'Calculating {roi} catchment Max flow accumulation stats...')\n",
    "            arcpy.env.snapRaster = fac\n",
    "            arcpy.env.cellSize = fac\n",
    "            try:\n",
    "                field = 'cat_ID_con'\n",
    "                operator = 'LIKE'\n",
    "                value = roi\n",
    "                sba = \"\"\"\"{}\" {} '{}%'\"\"\".format(field, operator, value)\n",
    "                clip_select = arcpy.SelectLayerByAttribute_management(clip_outfile,'NEW_SELECTION',sba)\n",
    "                zon_start = time.time()\n",
    "                cat_facMax_table = ZonalStatisticsAsTable(in_zone_data = clip_select ,\n",
    "                                                            zone_field = cat_cur_fields[2],\n",
    "                                                            in_value_raster = fac,\n",
    "                                                            out_table = outtable,\n",
    "                                                            statistics_type='MAXIMUM'\n",
    "                                                            )\n",
    "                # Add region identifier field for catchment table\n",
    "                arcpy.AddField_management(cat_facMax_table,'region',field_type='TEXT')\n",
    "                # Add cat_ID_Con field\n",
    "                arcpy.AddField_management(cat_facMax_table,'cat_ID_txt',field_type='TEXT')\n",
    "                # Update fields\n",
    "                # Add upstream area as calculated from Max flow acc zonal stats for catchment table\n",
    "                arcpy.AddField_management(cat_facMax_table,'site_acc_sqKm',field_type='DOUBLE')\n",
    "\n",
    "                with arcpy.da.UpdateCursor(cat_facMax_table,['region','cat_ID_txt','cat_ID_con', 'MAX', 'site_acc_sqKm']) as cur:\n",
    "                    for row in cur:\n",
    "                        strval = str(row[1])\n",
    "                        row[0] = roi\n",
    "                        row[1] = strval.replace('.0','')\n",
    "                        row[4] = (cf * row[3])/1000000\n",
    "                        # Update\n",
    "                        cur.updateRow(row)\n",
    "                    del(row)\n",
    "                del(cur)\n",
    "                # rename MAX field\n",
    "                arcpy.AlterField_management(cat_facMax_table,\"MAX\",'site_max_acc','site_max_acc')\n",
    "                zon_stop = time.time()\n",
    "                zon_time = int (zon_stop - zon_start)\n",
    "                print(f'Max Flow zonal stats for {roi} Elapsed time: ({datetime.timedelta(seconds=zon_time)})')\n",
    "                print('----------')\n",
    "\n",
    "                tbl_lst.append(outtable)\n",
    "            except:\n",
    "                e = sys.exc_info()[1]\n",
    "                print(e.args[0])\n",
    "                arcpy.AddError(e.args[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "['D:\\\\\\\\GIS_temp\\\\\\\\AKSSF_land_met\\\\\\\\AKSSF_land_met.gdb\\\\Cook_Inlet_maxfac',\n 'D:\\\\\\\\GIS_temp\\\\\\\\AKSSF_land_met\\\\\\\\AKSSF_land_met.gdb\\\\Copper_River_maxfac',\n 'D:\\\\\\\\GIS_temp\\\\\\\\AKSSF_land_met\\\\\\\\AKSSF_land_met.gdb\\\\Bristol_Bay_maxfac',\n 'D:\\\\\\\\GIS_temp\\\\\\\\AKSSF_land_met\\\\\\\\AKSSF_land_met.gdb\\\\Kodiak_maxfac',\n 'D:\\\\\\\\GIS_temp\\\\\\\\AKSSF_land_met\\\\\\\\AKSSF_land_met.gdb\\\\Prince_William_Sound_maxfac']"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_lst"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge all maxfac tables completed\n",
      "Cook_Inlet_wtds_merge\n",
      "Copper_River_wtds_merge\n",
      "Bristol_Bay_wtds_merge\n",
      "Kodiak_wtds_merge\n",
      "Prince_William_Sound_wtds_merge\n"
     ]
    }
   ],
   "source": [
    "# Merge flow accumulation outtables\n",
    "out_table_merge = os.path.join(outgdb,\"AKSSF_site_sj_maxfac\")\n",
    "flowmerge = arcpy.Merge_management(tbl_lst,out_table_merge,\"\",\"ADD_SOURCE_INFO\")\n",
    "print(\"Merge all maxfac tables completed\")\n",
    "# Merge watersheds and recalculate area_km2 to join back to sites sj - Not all sites had these fields joined\n",
    "wtds_merge_list = []\n",
    "arcpy.env.workspace = outgdb\n",
    "for fc in arcpy.ListFeatureClasses('*wtds_merge'):\n",
    "    print(fc)\n",
    "    arcpy.CalculateField_management(fc,\"Area_km2\",expression=\"!SHAPE.area@SQUAREKILOMETERS!\")\n",
    "    fieldlist = arcpy.ListFields(fc)\n",
    "    wtds_merge_list.append(fc)\n",
    "\n",
    "wtds_merge_path = os.path.join(outgdb,\"all_akssf_wtds\")\n",
    "wtds_merge = arcpy.Merge_management(wtds_merge_list,wtds_merge_path,\"\",\"ADD_SOURCE_INFO\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "#Join fields from flow acc zonal stats and wtds merge to compare watershed area\n",
    "flow_join_fields = ['site_max_acc','site_acc_sqKm']\n",
    "wtds_join_fields = ['Area_km2']\n",
    "arcpy.JoinField_management(sitesmerge,sj_cur_fields[2], out_table_merge,sj_cur_fields[2],flow_join_fields)\n",
    "arcpy.JoinField_management(sitesmerge,sj_cur_fields[2], wtds_merge,sj_cur_fields[2],wtds_join_fields)\n",
    "arcpy.AddField_management(sitesmerge,\"area_diff\",field_type=\"DOUBLE\")\n",
    "with arcpy.da.UpdateCursor(sitesmerge,[\"area_diff\",'Area_km2','site_acc_sqKm']) as cur:\n",
    "    for row in cur:\n",
    "        row[0] = row[1] - row[2]\n",
    "        cur.updateRow(row)\n",
    "    del(row)\n",
    "del(cur)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJECTID\n",
      "Shape\n",
      "Join_Count\n",
      "TARGET_FID\n",
      "SiteID\n",
      "Wtrbdy_n\n",
      "Agnc_ID\n",
      "SourcNm\n",
      "Cntct_p\n",
      "Cntct_m\n",
      "Cntct_t\n",
      "Snsr_cc\n",
      "file_nm\n",
      "seq_id\n",
      "Snsr_Pl\n",
      "Wtrbdy_t\n",
      "Verified\n",
      "Comments\n",
      "DM_Notes\n",
      "Sn_QAQC\n",
      "HUC8\n",
      "Name\n",
      "NHDPlusID\n",
      "SourceFC\n",
      "GridCode\n",
      "AreaSqKm\n",
      "VPUID\n",
      "Catch_ID\n",
      "cat_ID\n",
      "cat_ID_txt\n",
      "cat_ID_con\n",
      "HUC8_ID\n",
      "catID\n",
      "proc_reg\n",
      "MERGE_SRC\n",
      "site_max_acc\n",
      "site_acc_sqKm\n",
      "Area_km2\n",
      "area_diff\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                 SiteID  site_max_acc  \\\ncat_ID_con                                                              \nCook_Inlet_75005400001203                        CIK_48      11297657   \nCook_Inlet_75005400013272                        CIK_47        427430   \nCook_Inlet_75004300006239                        CIK_18       2224162   \nCook_Inlet_75004300007331                        CIK_19       2760921   \nCook_Inlet_75004300002289                          CIK3        309939   \n...                                                 ...           ...   \nPrince_William_Sound_36645               USFS_ERB Creek         35947   \nPrince_William_Sound_37815  USFS_Solf Lake Outlet Creek         52568   \nPrince_William_Sound_42563                     15219000        122628   \nPrince_William_Sound_43185                     15236900        237645   \nPrince_William_Sound_40285                     15237030        123168   \n\n                            site_acc_sqKm  Area_km2  area_diff  \ncat_ID_con                                                      \nCook_Inlet_75005400001203          282.44    282.47       0.02  \nCook_Inlet_75005400013272           10.69     10.78       0.10  \nCook_Inlet_75004300006239           55.60     56.02       0.42  \nCook_Inlet_75004300007331           69.02     69.45       0.43  \nCook_Inlet_75004300002289            7.75      8.15       0.41  \n...                                   ...       ...        ...  \nPrince_William_Sound_36645           3.59      4.13       0.53  \nPrince_William_Sound_37815           5.26      5.80       0.54  \nPrince_William_Sound_42563          12.26     12.80       0.53  \nPrince_William_Sound_43185          23.76     24.49       0.72  \nPrince_William_Sound_40285          12.32     12.91       0.59  \n\n[490 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SiteID</th>\n      <th>site_max_acc</th>\n      <th>site_acc_sqKm</th>\n      <th>Area_km2</th>\n      <th>area_diff</th>\n    </tr>\n    <tr>\n      <th>cat_ID_con</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Cook_Inlet_75005400001203</th>\n      <td>CIK_48</td>\n      <td>11297657</td>\n      <td>282.44</td>\n      <td>282.47</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>Cook_Inlet_75005400013272</th>\n      <td>CIK_47</td>\n      <td>427430</td>\n      <td>10.69</td>\n      <td>10.78</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>Cook_Inlet_75004300006239</th>\n      <td>CIK_18</td>\n      <td>2224162</td>\n      <td>55.60</td>\n      <td>56.02</td>\n      <td>0.42</td>\n    </tr>\n    <tr>\n      <th>Cook_Inlet_75004300007331</th>\n      <td>CIK_19</td>\n      <td>2760921</td>\n      <td>69.02</td>\n      <td>69.45</td>\n      <td>0.43</td>\n    </tr>\n    <tr>\n      <th>Cook_Inlet_75004300002289</th>\n      <td>CIK3</td>\n      <td>309939</td>\n      <td>7.75</td>\n      <td>8.15</td>\n      <td>0.41</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>Prince_William_Sound_36645</th>\n      <td>USFS_ERB Creek</td>\n      <td>35947</td>\n      <td>3.59</td>\n      <td>4.13</td>\n      <td>0.53</td>\n    </tr>\n    <tr>\n      <th>Prince_William_Sound_37815</th>\n      <td>USFS_Solf Lake Outlet Creek</td>\n      <td>52568</td>\n      <td>5.26</td>\n      <td>5.80</td>\n      <td>0.54</td>\n    </tr>\n    <tr>\n      <th>Prince_William_Sound_42563</th>\n      <td>15219000</td>\n      <td>122628</td>\n      <td>12.26</td>\n      <td>12.80</td>\n      <td>0.53</td>\n    </tr>\n    <tr>\n      <th>Prince_William_Sound_43185</th>\n      <td>15236900</td>\n      <td>237645</td>\n      <td>23.76</td>\n      <td>24.49</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>Prince_William_Sound_40285</th>\n      <td>15237030</td>\n      <td>123168</td>\n      <td>12.32</td>\n      <td>12.91</td>\n      <td>0.59</td>\n    </tr>\n  </tbody>\n</table>\n<p>490 rows  5 columns</p>\n</div>"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make site merge df\n",
    "sitemerge_df = pd.DataFrame()\n",
    "sitemerge_field_list = []\n",
    "keepfields = ['SiteID','cat_ID_con','site_max_acc','site_acc_sqKm','Area_km2','area_diff' ]\n",
    "for field in arcpy.ListFields(sitesmerge):\n",
    "    sitemerge_field_list.append(field.name)\n",
    "    print(field.name)\n",
    "sitesmerge_arr = arcpy.da.TableToNumPyArray(sitesmerge,keepfields)\n",
    "sitemerge_df = pd.DataFrame(sitesmerge_arr)\n",
    "sitemerge_df = sitemerge_df.set_index('cat_ID_con')\n",
    "dfs.append(sitemerge_df)\n",
    "sitemerge_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8caab41e",
   "language": "python",
   "display_name": "PyCharm (AKSSF)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}