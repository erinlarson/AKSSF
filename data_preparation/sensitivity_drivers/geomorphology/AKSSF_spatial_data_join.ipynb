{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This script will be for linking the siteIDs to the catchment IDs - which are either NHDPlusIDs,\n",
    "gridcodes, or new catIDs made from the gridcodes for BB. This will give us a way to link the\n",
    "thermal sensitivity responses by year to the covariates. This script also includes code to\n",
    "check the watersheds. The watersheds are individual feature classes to avoid overlapping\n",
    "polygons so the best way to check they are correct is to 1) check that every siteID/catID\n",
    "has a watershed, and 2) check that the watershed area approximately matches the watershed\n",
    "area from the flow accumulation grid. They won't be exact for sites that are not close to\n",
    "the catchment outlet and could be quite different for very small watersheds in some cases.\n",
    "\n",
    "# Watershed Summaries\n",
    "\n",
    "1. read in all watersheds feature classes\n",
    "2. create a table with the NHDPlusID/catID of the watershed name, region, and watershed area\n",
    "3. merge original point feature classes (inside and outside bb)\n",
    "4. do a spatial join with merged catchments by region to get catID and region on the points dataset\n",
    "5. clip to the region and save in each regional gdb\n",
    "6. merge all point feature classes with siteIDs and catIDs into one table\n",
    "7. join the watershed area from the watershed fcs to the points table using the catID\n",
    "NOT DONE and not sure we need it: 8. join the watershed area from the fac grid to the points table using the catID\n",
    "9. compare the results to ensure that watersheds have been created correctly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# steps 1 and 2\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "regions = [\"Kodiak\", \"Copper_River\", \"Prince_William_Sound\", \"Cook_Inlet\", \"Bristol_Bay\"]\n",
    "\n",
    "wtdList = []\n",
    "\n",
    "for region in regions:\n",
    "    local_gdb = \"W:\\\\GIS\\\\AKSSF\\\\\" + region + \"\\\\\" + region + \".gdb\\\\Watersheds\"\n",
    "    arcpy.env.workspace = local_gdb\n",
    "    wtds = arcpy.ListFeatureClasses()\n",
    "    print(region + \": \" + str(len(wtds)) + \" watersheds\")\n",
    "\n",
    "    for wtd in wtds:\n",
    "        wtdName = wtd[4:20]\n",
    "        print(\"Starting wtd: \" + wtdName)\n",
    "        wtdPath = os.path.join(arcpy.env.workspace, wtd)\n",
    "        # field_names = [f.name for f in arcpy.ListFields(wtdPath)]\n",
    "        # print(field_names)\n",
    "        # if \"Area_km2\" in field_names:\n",
    "        #     print(\"Area already calculated\")\n",
    "        # else:\n",
    "        #     arcpy.AddField_management(wtdPath, \"Area_km2\", \"DOUBLE\")\n",
    "        #     expression1 = \"{0}\".format(\"!SHAPE.area@SQUAREKILOMETERS!\")\n",
    "        #     arcpy.CalculateField_management(wtdPath, \"Area_km2\", expression1, \"PYTHON\", )\n",
    "        wtdArea = [row[0] for row in arcpy.da.SearchCursor(wtdPath, ['Area_km2'])]\n",
    "        # print(\"wtdName: \" + str(wtdArea))\n",
    "        wtdList.append({'Region': region, 'cat_ID': wtdName, 'Area_km2': wtdArea})\n",
    "\n",
    "wtdDf = pd.DataFrame(wtdList)\n",
    "print(wtdDf)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kodiak: 28 watersheds\n",
      "Prince_William_Sound: 19 watersheds\n"
     ]
    }
   ],
   "source": [
    "#merge watersheds into one feature class for each region\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "regions = [\"Kodiak\", \"Copper_River\", \"Prince_William_Sound\", \"Cook_Inlet\", \"Bristol_Bay\"]\n",
    "regions = [\"Kodiak\", \"Prince_William_Sound\"]\n",
    "\n",
    "for region in regions:\n",
    "    local_gdb = \"W:\\\\GIS\\\\AKSSF\\\\\" + region + \"\\\\\" + region + \".gdb\"\n",
    "    arcpy.env.workspace = local_gdb + \"\\\\Watersheds\"\n",
    "    wtds = arcpy.ListFeatureClasses()\n",
    "    print(region + \": \" + str(len(wtds)) + \" watersheds\")\n",
    "\n",
    "    #add cat_ID to each watershed before merging\n",
    "    for wtd in wtds:\n",
    "        wtdName = wtd[4:20]\n",
    "        wtdPath = os.path.join(arcpy.env.workspace, wtd)\n",
    "        arcpy.AddField_management(wtdPath, \"cat_ID\", \"DOUBLE\")\n",
    "        # expression1 = \"{0}\".format(\"!SHAPE.area@SQUAREKILOMETERS!\")\n",
    "        arcpy.CalculateField_management(wtdPath, \"cat_ID\", wtdName, \"PYTHON\")\n",
    "\n",
    "    arcpy.env.workspace = local_gdb\n",
    "    wtdMerge = [local_gdb + \"\\\\Watersheds\\\\\" + s for s in wtds]\n",
    "    wtd_output = \"wtds_merge\"\n",
    "    arcpy.Merge_management(wtdMerge, wtd_output)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(wtdDf))\n",
    "print(len(wtdList))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# step 3 - done\n",
    "\n",
    "import arcpy\n",
    "\n",
    "gdb = \"W:\\\\GIS\\\\AKSSF\\\\AKSSF_Hydrography.gdb\"\n",
    "arcpy.env.workspace = gdb\n",
    "bb_pts = gdb + \"\\\\bb_md_verified_DM\"\n",
    "other_pts = gdb + \"\\\\sites_outside_bb_verified_DM\"\n",
    "\n",
    "output = \"akssf_pts_verified\"\n",
    "arcpy.Merge_management([bb_pts, other_pts], output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# steps 4-6\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "regions_dict = {\"Kodiak\": '!gridcode!', \"Copper_River\": '!NHDPlusID!', \"Prince_William_Sound\": '!gridcode!',\n",
    "                \"Cook_Inlet\": '!NHDPlusID!', \"Bristol_Bay\": '!catID!'}\n",
    "# regions_dict = {\"Bristol_Bay\": '!catID!'}\n",
    "\n",
    "points = \"W:\\\\GIS\\\\AKSSF\\\\AKSSF_Hydrography.gdb\\\\akssf_pts_verified\"\n",
    "# sites_lst = []\n",
    "cats_lst = []\n",
    "\n",
    "for key, value in regions_dict.items():\n",
    "    arcpy.env.workspace = \"W:\\\\GIS\\\\AKSSF\\\\\" + key + \"\\\\\" + key + \".gdb\"\n",
    "    cats = os.path.join(arcpy.env.workspace, \"cats_merge\")\n",
    "    # arcpy.Clip_analysis(points, cats, r\"memory\\sites_clip\")\n",
    "    # arcpy.SpatialJoin_analysis(r\"memory\\sites_clip\", cats, \"sites_sj\")\n",
    "    # # #note that catID is in the BB cats_merge, but only a LONG, need a DOUBLE to account for NHDPlusIDs\n",
    "    # arcpy.AddField_management(\"sites_sj\", \"cat_ID\", \"DOUBLE\")\n",
    "    # arcpy.CalculateField_management(\"sites_sj\", \"cat_ID\", value)\n",
    "    # sites_fc = os.path.join(arcpy.env.workspace, \"sites_sj\")\n",
    "    # count = arcpy.GetCount_management(sites_fc)\n",
    "    # print('{} has {} records'.format(sites_fc, count[0]))\n",
    "    # sites_lst.append(sites_fc)\n",
    "    cats_lst.append(cats)\n",
    "\n",
    "#print(sites_lst)\n",
    "cats_outfile = \"W:\\\\GIS\\\\AKSSF\\\\AKSSF_Hydrography.gdb\\\\all_cats\"\n",
    "arcpy.Merge_management(cats_lst, cats_outfile)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DM - Picked up here and modified to match local data\n",
    "Also want to intersect sites with maximum flow accumulation in small (30 meters) buffer.  Buffers need to be generated\n",
    "iteratively and clipped to their respective catchment to ensure they do not cross pull values from an adjacent catchment.\n",
    "Need to join stream order, stream slope, and upstream downstream km info from vaa/streams merge datasets.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Setting outgdb to location created in Covariate notebook but any temp gdb will work\n",
    "outgdb = r\"D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\"\n",
    "# Set data dir = to folder with regional subfolders and gdbs\n",
    "data_dir = r\"D:\\GIS_Temp\\AKSSF\"\n",
    "arcpy.env.workspace = data_dir\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "regions = arcpy.ListWorkspaces(workspace_type=\"Folder\")\n",
    "\n",
    "# Create lists to store paths to data copies to be merged\n",
    "site_lst = []\n",
    "cats_lst = []\n",
    "vaas_lst = []\n",
    "streams_lst = []\n",
    "wtds_lst = []\n",
    "\n",
    "# Walk through gdbs and select sites_sj, streams and vaas to merge into single fcs\n",
    "# Seperate data by\n",
    "nhdplus_dat = ['Cook_Inlet', 'Copper_River']\n",
    "tauDem_dat = ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound']\n",
    "# Loop through all processing areas\n",
    "rois = nhdplus_dat + tauDem_dat\n",
    "for roi in rois:\n",
    "    # Loop through regional folders\n",
    "    for region in regions:\n",
    "        if roi in str(region):\n",
    "            if roi in nhdplus_dat:\n",
    "                cat_cur_fields = ['cat_ID_txt', 'NHDPlusID', \"cat_ID_con\"]\n",
    "                sj_cur_fields = ['cat_ID_txt', 'cat_ID', \"cat_ID_con\"]\n",
    "                wtd_cur_fields = ['cat_ID_txt', 'cat_ID', \"cat_ID_con\"]\n",
    "                str_cur_fields = ['cat_ID_txt', 'NHDPlusID', \"cat_ID_con\"]\n",
    "                print(\n",
    "                    f'{roi} in {nhdplus_dat} AKSSF list, using cat_fields {cat_cur_fields} and watershed fields {sj_cur_fields}')\n",
    "                print('----------')\n",
    "            # Set data and variables unique to regions with TauDEM Data\n",
    "            elif roi in tauDem_dat:\n",
    "                if roi == 'Bristol_Bay':\n",
    "                    # Fields for update cursor\n",
    "                    cat_cur_fields = ['cat_ID_txt', 'catID', \"cat_ID_con\"]\n",
    "                    sj_cur_fields = ['cat_ID_txt', 'cat_ID', \"cat_ID_con\"]\n",
    "                    wtd_cur_fields = ['cat_ID_txt', 'cat_ID', \"cat_ID_con\"]\n",
    "                    str_cur_fields = ['cat_ID_txt', 'catID', \"cat_ID_con\"]\n",
    "                else:\n",
    "                    cat_cur_fields = ['cat_ID_txt', 'gridcode', \"cat_ID_con\"]\n",
    "                    wtd_cur_fields = ['cat_ID_txt', 'gridcode', \"cat_ID_con\"]\n",
    "                    sj_cur_fields = ['cat_ID_txt', 'gridcode', \"cat_ID_con\"]\n",
    "                    str_cur_fields = ['cat_ID_txt', 'LINKNO', \"cat_ID_con\"]\n",
    "\n",
    "                print(\n",
    "                    f'{roi} in {tauDem_dat} TauDEM list, using cat_fields {cat_cur_fields} and watershed fields {sj_cur_fields}')\n",
    "                print('----------')\n",
    "            print(f'{roi} using data from {region} folder')\n",
    "            arcpy.env.workspace = region\n",
    "            gdb = arcpy.ListWorkspaces(workspace_type='FileGDB')\n",
    "            walk = arcpy.da.Walk(region, datatype=['FeatureClass', 'Table'])\n",
    "            for dirpath, dirnames, filenames in walk:\n",
    "                for filename in filenames:\n",
    "                    # Grab sites\n",
    "                    if filename == 'sites_sj':\n",
    "                        print(f'{dirpath},{filename}')\n",
    "                        sitesource = os.path.join(dirpath, filename)\n",
    "                        sitename = roi + '_' + filename\n",
    "                        sitepath = os.path.join(outgdb, sitename)\n",
    "                        # Make local copy projected in AKAlbers\n",
    "                        if not arcpy.Exists(sitepath):\n",
    "                            print(f'Copying {sitename}...')\n",
    "                            print('----------')\n",
    "                            sitecopy = arcpy.FeatureClassToFeatureClass_conversion(sitesource, outgdb, sitename)\n",
    "                        else:\n",
    "                            print(f'{sitepath} already created')\n",
    "                            sitecopy = sitepath\n",
    "                            print('----------')\n",
    "                        site_lst.append(sitecopy)\n",
    "                        sitefieldnames = []\n",
    "                        sitelstFields = arcpy.ListFields(sitecopy)\n",
    "                        for field in sitelstFields:\n",
    "                            sitefieldnames.append(field.name)\n",
    "                        if str(sj_cur_fields[0]) in sitefieldnames:\n",
    "                            print(f'{sj_cur_fields[0]} field already in dataset')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print(f'Adding {sj_cur_fields[0]} field to watershed dataset {sitecopy}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_txt field and concat cat_ID + region\n",
    "                            arcpy.AddField_management(sitecopy, str(sj_cur_fields[0]), field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(sitecopy, sj_cur_fields[0:2]) as cur:\n",
    "                                for row in cur:\n",
    "                                    strval = str(row[1])\n",
    "                                    row[0] = strval.replace('.0', '')\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del (row)\n",
    "                            del (cur)\n",
    "                        if str(sj_cur_fields[2]) in sitefieldnames:\n",
    "                            print(f'{sj_cur_fields[2]} field already in dataset {sitecopy}')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print(f'Adding {sj_cur_fields[2]} field to watershed dataset {sitecopy}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_con field and concat cat_ID + region\n",
    "                            arcpy.AddField_management(sitecopy, str(sj_cur_fields[2]), field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(sitecopy, sj_cur_fields) as cur:\n",
    "                                for row in cur:\n",
    "                                    strval = str(row[1])\n",
    "                                    row[2] = str(roi) + '_' + strval.replace('.0', '')\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del (row)\n",
    "                            del (cur)\n",
    "                    # Set merged streams dataset\n",
    "                    elif 'streams_merge' == filename:\n",
    "                        strsource = os.path.join(dirpath, filename)\n",
    "                        strname = roi + '_' + filename\n",
    "                        strpath = os.path.join(outgdb, strname)\n",
    "\n",
    "                        if not arcpy.Exists(strpath):\n",
    "                            print(f'Copying {strname}...')\n",
    "                            print('----------')\n",
    "                            # Make local copy projected in AKAlbers\n",
    "                            str_merge = arcpy.FeatureClassToFeatureClass_conversion(strsource, outgdb, strname)\n",
    "                        else:\n",
    "                            print(f'Merged streams dataset {strpath} already created...')\n",
    "                            print('----------')\n",
    "                            str_merge = strpath\n",
    "                        streams_lst.append(str_merge)\n",
    "                        strfieldnames = []\n",
    "                        strlstFields = arcpy.ListFields(str_merge)\n",
    "                        for field in strlstFields:\n",
    "                            strfieldnames.append(field.name)\n",
    "                        if str(str_cur_fields[0]) in strfieldnames:\n",
    "                            print(f'{str_cur_fields[0]} field already in dataset')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print(f'Adding {str_cur_fields[0]} field to streams dataset {str_merge}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_txt field and concat cat_ID + region\n",
    "                            arcpy.AddField_management(str_merge, str(str_cur_fields[0]), field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(str_merge, str_cur_fields[0:2]) as cur:\n",
    "                                for row in cur:\n",
    "                                    strval = str(row[1])\n",
    "                                    row[0] = strval.replace('.0', '')\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del (row)\n",
    "                            del (cur)\n",
    "                        if str(str_cur_fields[2]) in strfieldnames:\n",
    "                            print(f'{str_cur_fields[2]} field already in dataset {str_merge}')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print(f'Adding {str_cur_fields[2]} field to streams dataset {str_merge}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_con field and concat cat_ID + region\n",
    "                            arcpy.AddField_management(str_merge, str(str_cur_fields[2]), field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(str_merge, str_cur_fields) as cur:\n",
    "                                for row in cur:\n",
    "                                    strval = str(row[1])\n",
    "                                    row[2] = str(roi) + '_' + strval.replace(\".0\", \"\")\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del (row)\n",
    "                            del (cur)\n",
    "\n",
    "                    # Set merged VAA Table dataset\n",
    "                    elif 'vaa_merge' == filename:\n",
    "                        vaasource = os.path.join(dirpath, filename)\n",
    "                        vaaname = roi + '_' + filename\n",
    "                        vaa_path = os.path.join(outgdb, vaaname)\n",
    "                        if not arcpy.Exists(vaa_path):\n",
    "                            print(f'Copying {vaaname}...')\n",
    "                            print('----------')\n",
    "                            # Make local copy projected in AKAlbers\n",
    "                            vaa_merge = arcpy.TableToTable_conversion(vaasource, outgdb, vaaname)\n",
    "                        else:\n",
    "                            print(f'Merged VAA Table dataset {vaa_path} already created...')\n",
    "                            print('----------')\n",
    "                            vaa_merge = vaa_path\n",
    "                        vaas_lst.append(vaa_merge)\n",
    "                        vaafieldnames = []\n",
    "                        vaalstFields = arcpy.ListFields(vaa_merge)\n",
    "                        for field in vaalstFields:\n",
    "                            vaafieldnames.append(field.name)\n",
    "                        if str(str_cur_fields[0]) in vaafieldnames:\n",
    "                            print(f'{str_cur_fields[0]} field already in dataset')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print(f'Adding {str_cur_fields[0]} field to VAA Table dataset {vaa_merge}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_txt field and concat cat_ID + region\n",
    "                            arcpy.AddField_management(vaa_merge, str(str_cur_fields[0]), field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(vaa_merge, str_cur_fields[0:2]) as cur:\n",
    "                                for row in cur:\n",
    "                                    strval = str(row[1])\n",
    "                                    row[0] = strval.replace('.0', '')\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del (row)\n",
    "                            del (cur)\n",
    "                        if str(str_cur_fields[2]) in vaafieldnames:\n",
    "                            print(f'{str_cur_fields[2]} field already in dataset {vaa_merge}')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print(f'Adding {str_cur_fields[2]} field to VAA Table dataset {vaa_merge}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_con field and concat cat_ID + region\n",
    "                            arcpy.AddField_management(vaa_merge, str(str_cur_fields[2]), field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(vaa_merge, str_cur_fields) as cur:\n",
    "                                for row in cur:\n",
    "                                    strval = str(row[1])\n",
    "                                    row[2] = str(roi) + '_' + strval.replace(\".0\", \"\")\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del (row)\n",
    "                            del (cur)\n",
    "                    # Set merged watersheds dataset\n",
    "                    elif 'wtds_merge' == filename:\n",
    "                        wtdsource = os.path.join(dirpath, filename)\n",
    "                        wtdname = roi + '_' + filename\n",
    "                        wtdpath = os.path.join(outgdb, wtdname)\n",
    "\n",
    "                        if not arcpy.Exists(wtdpath):\n",
    "                            print(f'Copying {wtdname}...')\n",
    "                            print('----------')\n",
    "                            # Make local copy projected in AKAlbers\n",
    "                            wtd_merge = arcpy.FeatureClassToFeatureClass_conversion(wtdsource, outgdb, wtdname)\n",
    "                        else:\n",
    "                            print(f'Merged watershed dataset {wtdpath} already created...')\n",
    "                            print('----------')\n",
    "                            wtd_merge = wtdpath\n",
    "                        wtds_lst.append(wtd_merge)\n",
    "                        wtdfieldnames = []\n",
    "                        wtdlstFields = arcpy.ListFields(wtd_merge)\n",
    "                        for field in wtdlstFields:\n",
    "                            wtdfieldnames.append(field.name)\n",
    "                        if str(wtd_cur_fields[0]) in wtdfieldnames:\n",
    "                            print(f'{wtd_cur_fields[0]} field already in dataset')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print(f'Adding {wtd_cur_fields[0]} field to watershed dataset {wtd_merge}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_txt field and concat cat_ID + region\n",
    "                            arcpy.AddField_management(wtd_merge, str(wtd_cur_fields[0]), field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(wtd_merge, wtd_cur_fields[0:2]) as cur:\n",
    "                                for row in cur:\n",
    "                                    strval = str(row[1])\n",
    "                                    row[0] = strval.replace('.0', '')\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del (row)\n",
    "                            del (cur)\n",
    "                        if str(wtd_cur_fields[2]) in wtdfieldnames:\n",
    "                            print(f'{wtd_cur_fields[2]} field already in dataset {wtd_merge}')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print(f'Adding {wtd_cur_fields[2]} field to watershed dataset {wtd_merge}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_con field and concat cat_ID + region\n",
    "                            arcpy.AddField_management(wtd_merge, str(wtd_cur_fields[2]), field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(wtd_merge, wtd_cur_fields) as cur:\n",
    "                                for row in cur:\n",
    "                                    strval = str(row[1])\n",
    "                                    row[2] = str(roi) + '_' + strval.replace(\".0\", \"\")\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del (row)\n",
    "                            del (cur)\n",
    "\n",
    "                    # Select catch_int fc (catchments of interest for region) and make a copy\n",
    "                    elif 'cats_intersect' == filename:\n",
    "                        # Make local copy projected in AKAlbers\n",
    "                        catssource = os.path.join(dirpath, filename)\n",
    "                        catsname = roi + \"_\" + filename\n",
    "                        catspath = os.path.join(outgdb, catsname)\n",
    "                        if not arcpy.Exists(catspath):\n",
    "                            print(f'Copying {catsname}...')\n",
    "                            print('----------')\n",
    "                            cats = arcpy.FeatureClassToFeatureClass_conversion(catssource, outgdb, catsname)\n",
    "                        else:\n",
    "                            print(f'{catspath} already created')\n",
    "                            print('----------')\n",
    "                            cats = catspath\n",
    "                        cats_lst.append(cats)\n",
    "                        catlstfields = arcpy.ListFields(cats)\n",
    "                        catfieldnames = []\n",
    "                        for field in catlstfields:\n",
    "                            catfieldnames.append(field.name)\n",
    "                        if str(cat_cur_fields[0]) in catfieldnames:\n",
    "                            print(f'{cat_cur_fields[0]} field already in dataset {cats}')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print(f'Adding {cat_cur_fields[0]} field to catchment dataset {cats}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_txt field\n",
    "                            arcpy.AddField_management(cats, str(cat_cur_fields[0]), field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(cats, cat_cur_fields[0:2]) as cur:\n",
    "                                for row in cur:\n",
    "                                    strval = str(row[1])\n",
    "                                    row[0] = strval.replace('.0', '')\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del (row)\n",
    "                            del (cur)\n",
    "                        if str(cat_cur_fields[2]) in catfieldnames:\n",
    "                            print(f'{cat_cur_fields[2]} field already in dataset {cats}')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print(f'Adding {cat_cur_fields[2]} field to catchment dataset {cats}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_txt field & cat_ID + region concat field\n",
    "                            arcpy.AddField_management(cats, str(cat_cur_fields[2]), field_type='TEXT')\n",
    "                            # populate cat_ID_con\n",
    "                            with arcpy.da.UpdateCursor(cats, cat_cur_fields) as cur:\n",
    "                                for row in cur:\n",
    "                                    strval = str(row[1])\n",
    "                                    row[2] = str(roi) + '_' + strval.replace(\".0\", \"\")\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del (row)\n",
    "                            del (cur)\n",
    "print(cats_lst)\n",
    "print(site_lst)\n",
    "print(vaas_lst)\n",
    "print(streams_lst)\n",
    "print(wtds_lst)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merge collected datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "\n",
    "# Start timing function\n",
    "processStart = time.time()\n",
    "processStartdt = datetime.datetime.now()\n",
    "\n",
    "outcats = os.path.join(outgdb, \"all_cats_int_merge\")\n",
    "outsites = os.path.join(outgdb, \"all_sites_sj_merge\")\n",
    "outvaa = os.path.join(outgdb, \"all_vaas_merge\")\n",
    "outwtd = os.path.join(outgdb, \"all_wtds_merge\")\n",
    "outstreams = os.path.join(outgdb, \"all_taustreams_merge\")\n",
    "print(f\"Merging dataset {outsites}...\\n '----------' \")\n",
    "sitesmerge = arcpy.Merge_management(site_lst, outsites, \"\", \"ADD_SOURCE_INFO\")\n",
    "print(f\"Merging dataset {outcats}...\\n '----------' \")\n",
    "catsmerge = arcpy.Merge_management(cats_lst, outcats, \"\", \"ADD_SOURCE_INFO\")\n",
    "print(f\"Merging dataset {outvaa}...\\n '----------' \")\n",
    "vaasmerge = arcpy.Merge_management(vaas_lst, outvaa, \"\", \"ADD_SOURCE_INFO\")\n",
    "print(f\"Merging dataset {outstreams}...\\n '----------' \")\n",
    "streamsmerge = arcpy.Merge_management(streams_lst, outstreams, \"\", \"ADD_SOURCE_INFO\")\n",
    "print(f\"Merging dataset {outwtd}...\\n '----------' \")\n",
    "wtdsmerge = arcpy.Merge_management(wtds_lst, outwtd, \"\", \"ADD_SOURCE_INFO\")\n",
    "\n",
    "# End timing\n",
    "processEnd = time.time()\n",
    "processElapsed = int(processEnd - processStart)\n",
    "processSuccess_time = datetime.datetime.now()\n",
    "\n",
    "# Report success\n",
    "print(f'Process completed at {processSuccess_time.strftime(\"%Y-%m-%d %H:%M\")} '\n",
    "      f'(Elapsed time: {datetime.timedelta(seconds=processElapsed)})')\n",
    "print('----------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Buffer and clip sites\n",
    "Buffer sites by 30 meters and loop through each site and clip to its respective catchment.  Merge all clips together\n",
    "when complete"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "from arcpy.sa import *\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Setting outgdb to location created in Covariate notebook but any temp gdb will work\n",
    "outgdb = r\"D:\\\\GIS_temp\\\\AKSSF_land_met\\\\AKSSF_land_met.gdb\"\n",
    "# Set data dir = to folder with regional subfolders and gdbs\n",
    "data_dir = r\"D:\\GIS_Temp\\AKSSF\"\n",
    "arcpy.env.workspace = data_dir\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "regions = arcpy.ListWorkspaces(workspace_type=\"Folder\")\n",
    "#buffer value of 30 meters\n",
    "buffval = 30\n",
    "bufpath = os.path.join(outgdb, \"sites_buffer\")\n",
    "buffer = arcpy.Buffer_analysis(sitesmerge, bufpath, \"30 meters\")\n",
    "# this makes sure the buffer does not extend outside of the catchment - cannot use memory/ object as input for zonal stats\n",
    "# must write to gdb\n",
    "clips = []\n",
    "# Start timing function\n",
    "processStart = time.time()\n",
    "processStartdt = datetime.datetime.now()\n",
    "buflyr = arcpy.MakeFeatureLayer_management(bufpath, \"buflyr\")\n",
    "catlayer = arcpy.MakeFeatureLayer_management(catsmerge, 'catlayer')\n",
    "fieldlist = []\n",
    "fields = arcpy.ListFields(bufpath)\n",
    "for field in fields:\n",
    "    fieldlist.append(field.name)\n",
    "row_count = len(list(i for i in arcpy.da.SearchCursor(bufpath, fieldlist)))\n",
    "print(row_count)\n",
    "\n",
    "c = 1\n",
    "with arcpy.da.SearchCursor(bufpath, ['SiteID', 'cat_ID_con', 'OBJECTID']) as cur:\n",
    "    for row in cur:\n",
    "        fieldValue = str(row[0])\n",
    "        if fieldValue.find(\"'\") != -1:\n",
    "            newVal = fieldValue.replace(\"'\", \"''\")\n",
    "            print(f'Apostrophe found in {fieldValue}, replacing with {newVal}')\n",
    "        else:\n",
    "            print(f'Site ID: {fieldValue} OK')\n",
    "            newVal = fieldValue\n",
    "\n",
    "        clause = \"\"\" \"cat_ID_con\" = '%s'\"\"\" % row[1]\n",
    "        clause1 = \"\"\" \"SiteID\" = '%s'\"\"\" % newVal\n",
    "        print(f' {c} of {row_count} -  Clipping {row[0]} to {row[1]}...')\n",
    "        print('----------')\n",
    "        bufselect = arcpy.SelectLayerByAttribute_management(buflyr, 'NEW_SELECTION', where_clause=clause1)\n",
    "        catselect = arcpy.SelectLayerByAttribute_management(catlayer, 'NEW_SELECTION', where_clause=clause)\n",
    "        clipname = 'clip_' + str(c)\n",
    "        clipout = os.path.join(outgdb, clipname)\n",
    "        clip = arcpy.Clip_analysis(bufselect, catselect, clipout)\n",
    "        clips.append(clipout)\n",
    "        c += 1\n",
    "    del (row)\n",
    "del (cur)\n",
    "\n",
    "# End timing\n",
    "processEnd = time.time()\n",
    "processElapsed = int(processEnd - processStart)\n",
    "processSuccess_time = datetime.datetime.now()\n",
    "\n",
    "# Report success\n",
    "print(f'Process completed at {processSuccess_time.strftime(\"%Y-%m-%d %H:%M\")} '\n",
    "      f'(Elapsed time: {datetime.timedelta(seconds=processElapsed)})')\n",
    "print('----------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merge Clips and delete individual clipped buffers\n",
    "clip_path = os.path.join(outgdb, 'all_buffsites_clip')\n",
    "clip_outfile = arcpy.Merge_management(clips, clip_path)\n",
    "\n",
    "delete = clips\n",
    "for d in delete:\n",
    "    arcpy.DeleteFeatures_management(d)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#zonal stats on buffer intersection with flow accumulation grids (need to loop through regions for this)\n",
    "# to get maximum flow accumulation bc some points may not exactly fall on stream grid\n",
    "tbl_lst = []\n",
    "# Seperate data by\n",
    "nhdplus_dat = ['Cook_Inlet', 'Copper_River']\n",
    "tauDem_dat = ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound']\n",
    "\n",
    "# Loop through all processing areas\n",
    "rois = nhdplus_dat + tauDem_dat\n",
    "\n",
    "for roi in rois:\n",
    "    # Loop through regional folders\n",
    "    # Grab stream order from VAA/Streams and join to sites_sj\n",
    "    for region in regions:\n",
    "        arcpy.env.workspace = region\n",
    "        raslist = arcpy.ListRasters()\n",
    "        if roi in str(region):\n",
    "            if roi in nhdplus_dat:\n",
    "                cat_cur_fields = ['cat_ID_txt', 'NHDPlusID', \"cat_ID_con\"]\n",
    "                cf = 25\n",
    "                # grab stream order from vaa - upstream and downstream lengths - and stream gradient\n",
    "                vaa = 'vaa_merge'\n",
    "                print(f'{roi} in {nhdplus_dat} AKSSF list, using cat_fields {cat_cur_fields} and spatial join fields '\n",
    "                      f'{sj_cur_fields}')\n",
    "                print('----------')\n",
    "            # Set data and variables unique to regions with TauDEM Data\n",
    "            elif roi in tauDem_dat:\n",
    "                # grab stream order from tauDEM - upstream and downstream lengths - and stream gradient\n",
    "                streams = \"streams_merge\"\n",
    "                # Fields for update cursor\n",
    "                if roi == 'Bristol_Bay':\n",
    "                    # Fields for update cursor\n",
    "                    cat_cur_fields = ['cat_ID_txt', 'catID', \"cat_ID_con\"]\n",
    "                else:\n",
    "                    cat_cur_fields = ['cat_ID_txt', 'gridcode', \"cat_ID_con\"]\n",
    "                cf = 100\n",
    "                print(f'{roi} in {tauDem_dat} TauDEM list, using cat_fields {cat_cur_fields} and spatial join fields '\n",
    "                      f'{sj_cur_fields}')\n",
    "                print('----------')\n",
    "            print(f'{roi} using data from {region} folder')\n",
    "            print('--------')\n",
    "            fac = os.path.join(region, 'fac.tif')\n",
    "            if arcpy.Exists(fac):\n",
    "                print(f'Flow accumulation grid {fac} selected for {roi}')\n",
    "            else:\n",
    "                print(f'No flow accumulation raster for {region}')\n",
    "            outtable = os.path.join(outgdb, (roi + \"_maxfac\"))\n",
    "            print(f'Calculating {roi} catchment Max flow accumulation stats...')\n",
    "            arcpy.env.snapRaster = fac\n",
    "            arcpy.env.cellSize = fac\n",
    "            try:\n",
    "                field = 'cat_ID_con'\n",
    "                operator = 'LIKE'\n",
    "                value = roi\n",
    "                sba = \"\"\"\"{}\" {} '{}%'\"\"\".format(field, operator, value)\n",
    "                clip_select = arcpy.SelectLayerByAttribute_management(clip_outfile, 'NEW_SELECTION', sba)\n",
    "                zon_start = time.time()\n",
    "                cat_facMax_table = ZonalStatisticsAsTable(in_zone_data=clip_select,\n",
    "                                                          zone_field=cat_cur_fields[2],\n",
    "                                                          in_value_raster=fac,\n",
    "                                                          out_table=outtable,\n",
    "                                                          statistics_type='MAXIMUM'\n",
    "                                                          )\n",
    "                # Add region identifier field for catchment table\n",
    "                arcpy.AddField_management(cat_facMax_table, 'region', field_type='TEXT')\n",
    "                # Add cat_ID_Con field\n",
    "                arcpy.AddField_management(cat_facMax_table, 'cat_ID_txt', field_type='TEXT')\n",
    "                # Update fields\n",
    "                # Add upstream area as calculated from Max flow acc zonal stats for catchment table\n",
    "                arcpy.AddField_management(cat_facMax_table, 'site_acc_sqKm', field_type='DOUBLE')\n",
    "\n",
    "                with arcpy.da.UpdateCursor(cat_facMax_table,\n",
    "                                           ['region', 'cat_ID_txt', 'cat_ID_con', 'MAX', 'site_acc_sqKm']) as cur:\n",
    "                    for row in cur:\n",
    "                        strval = str(row[1])\n",
    "                        row[0] = roi\n",
    "                        row[1] = strval.replace('.0', '')\n",
    "                        row[4] = (cf * row[3]) / 1000000\n",
    "                        # Update\n",
    "                        cur.updateRow(row)\n",
    "                    del (row)\n",
    "                del (cur)\n",
    "                # rename MAX field\n",
    "                arcpy.AlterField_management(cat_facMax_table, \"MAX\", 'site_max_acc', 'site_max_acc')\n",
    "                zon_stop = time.time()\n",
    "                zon_time = int(zon_stop - zon_start)\n",
    "                print(f'Max Flow zonal stats for {roi} Elapsed time: ({datetime.timedelta(seconds=zon_time)})')\n",
    "                print('----------')\n",
    "\n",
    "                tbl_lst.append(outtable)\n",
    "            except:\n",
    "                e = sys.exc_info()[1]\n",
    "                print(e.args[0])\n",
    "                arcpy.AddError(e.args[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merge flow accumulation outtables\n",
    "out_table_merge = os.path.join(outgdb, \"AKSSF_site_sj_maxfac\")\n",
    "flowmerge = arcpy.Merge_management(tbl_lst, out_table_merge, \"\", \"ADD_SOURCE_INFO\")\n",
    "print(\"Merge all maxfac tables completed\")\n",
    "# Merge watersheds and recalculate area_km2 to join back to sites sj - Not all sites had these fields joined\n",
    "wtds_merge_list = []\n",
    "arcpy.env.workspace = outgdb\n",
    "for fc in arcpy.ListFeatureClasses('*wtds_merge'):\n",
    "    print(fc)\n",
    "    arcpy.CalculateField_management(fc, \"Area_km2\", expression=\"!SHAPE.area@SQUAREKILOMETERS!\")\n",
    "    fieldlist = arcpy.ListFields(fc)\n",
    "    wtds_merge_list.append(fc)\n",
    "\n",
    "wtds_merge_path = os.path.join(outgdb, \"all_akssf_wtds\")\n",
    "wtds_merge = arcpy.Merge_management(wtds_merge_list, wtds_merge_path, \"\", \"ADD_SOURCE_INFO\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add stream metrics from VAA table and streams merge\n",
    "### VAA ATTRIBUTES\n",
    "* ARBOLATESU takes advantage of the LengthKM field in NHDFlowline, which measures the length of the flowline in kilometers. ARBOLATESU is determined by accumulating the length of all the upstream flowlines from the bottom of the current flowline and so provides the total length of the upstream drainage network from the bottom of the current flowline.\n",
    "* ARBOLATESU is the total length of the upstream drainage network from the bottom of the current flowline\n",
    "\n",
    "* The PATHLENGTH is the distance from the bottom of a flowline to the bottom of the terminal flowline along the main path as identified in the TERMINALPA field. There may be many pathways between a flowline and the terminal flowline because of divergences in the network so PATHLENGTH is computed by following the main path at each divergence.\n",
    "   * PATHLENGTH is the distance from the bottom of a flowline to the end of the network and is calculated on the main network path\n",
    "   * PATHLENGTH at the terminal Flowline (Isolated, Sinks, Oceans) is equal to zero.\n",
    "\n",
    "* This is what you need to know about STARTFLAG AND TERMINALFL:\n",
    "  * STARTFLAG identifies headwater flowlines\n",
    "  * TERMINALFL identifies network end flowlines\n",
    "  * Values of 0 indicate the feature is not headwater (STARTFLAG) or network end (TERMINALFL)\n",
    "  * values of 1 indicate the feature is a headwater (STARTFLAG) or network end (TERMINALFL)\n",
    "\n",
    "* TotalDrainageAreaSqKm\n",
    "  * Total cumulative area, in square kilometers\n",
    "* AreaSqKm - Catchment area, in square kilometers\n",
    "* Slope - Slope of the flowline from smoothed elevation (unitless)\n",
    "### tauDEM Attributes\n",
    "* DSContArea - Drainage area at the downstream end of the link. Generally this is one grid cell upstream of the downstream end because the drainage area at the downstream end grid cell includes the area of the stream being joined.\n",
    "* USContArea - Drainage area at the upstream end of the link\n",
    "* Slope - Average slope of the link (computed as drop/length)\n",
    "* DOUT_END - Distance to the outlet from the downstream end of the link\n",
    "* DOUT_START - Distance to the outlet from the upstream end of the link\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Join fields from flow acc zonal stats and wtds merge to compare watershed area\n",
    "delete_fields = ['StreamOrde', 'strmOrder', 'Slope', 'Slope_1', 'HUC8_ID', 'HUC8', 'Join_Count', 'TARGET_FID',\n",
    "                 'proc_reg']\n",
    "flow_join_fields = ['site_max_acc', 'site_acc_sqKm']\n",
    "wtds_join_fields = ['Area_km2']\n",
    "vaa_join_fields = ['StreamOrde', 'Slope', 'AreaSqKm', 'ArbolateSu', 'PathLength', 'StartFlag', 'TerminalPa',\n",
    "                   'TotalDrainageAreaSqKm']\n",
    "streams_join_fields = ['strmOrder', 'Slope', 'USContArea', 'DSContArea', 'LINKNO', 'DOUTEND', 'DOUTSTART']\n",
    "#join flow acc\n",
    "arcpy.JoinField_management(sitesmerge, sj_cur_fields[2], out_table_merge, sj_cur_fields[2], flow_join_fields)\n",
    "#join wtds merge\n",
    "arcpy.JoinField_management(sitesmerge, sj_cur_fields[2], wtds_merge, sj_cur_fields[2], wtds_join_fields)\n",
    "#join NHDPlus vaa table data\n",
    "arcpy.JoinField_management(sitesmerge, sj_cur_fields[2], vaasmerge, str_cur_fields[2], vaa_join_fields)\n",
    "# join TauDEM streams merge data\n",
    "arcpy.JoinField_management(sitesmerge, sj_cur_fields[2], streamsmerge, str_cur_fields[2], streams_join_fields)\n",
    "# Add fields as necessary\n",
    "arcpy.AddField_management(sitesmerge, \"area_diff\", field_type=\"DOUBLE\")\n",
    "arcpy.AddField_management(sitesmerge, \"str_slope\", field_type=\"DOUBLE\")\n",
    "arcpy.AddField_management(sitesmerge, \"str_ord\", field_type=\"SHORT\")\n",
    "\n",
    "# Make list of fields to keep\n",
    "str_fields = []\n",
    "for field in arcpy.ListFields(sitesmerge):\n",
    "    str_fields.append(field.name)\n",
    "keep_fields = [item for item in str_fields if item not in delete_fields]\n",
    "with arcpy.da.UpdateCursor(sitesmerge,\n",
    "                           [\"area_diff\", 'Area_km2', 'site_acc_sqKm', 'str_slope', 'Slope', 'Slope_1', 'str_ord',\n",
    "                            'StreamOrde', 'strmOrder']) as cur:\n",
    "    for row in cur:\n",
    "        # Set conditions for na values in vaa/tau slope\n",
    "        if row[4] != None:\n",
    "            slope = row[4]\n",
    "        else:\n",
    "            slope = row[5]\n",
    "        # Set conditions for na values in vaa/tau strOrder\n",
    "        if row[7] != None:\n",
    "            ord = row[7]\n",
    "        else:\n",
    "            ord = row[8]\n",
    "        row[3] = slope\n",
    "        row[6] = ord\n",
    "        # Calculate difference in area as watershed polygon area in sqKm minus area calculated from max flow accumulation in sqKm\n",
    "        row[0] = row[1] - row[2]\n",
    "        cur.updateRow(row)\n",
    "    del (row)\n",
    "del (cur)\n",
    "print('Join Fields Complete')\n",
    "print('----------')\n",
    "# Calculate lat/lon in Nad83\n",
    "# arcpy.management.CalculateGeometryAttributes(\"all_sites_sj_merge\", \"lat POINT_Y;lon POINT_X\", '', '','GEOGCS[\"GCS_North_American_1983\",DATUM[\"D_North_American_1983\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]]', \"DD\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Delete unecessary fields\n",
    "for field in arcpy.ListFields(sitesmerge):\n",
    "    if field.name not in keep_fields:\n",
    "        arcpy.DeleteField_management(sitesmerge, field.name)\n",
    "        print(f'Dropping field: {field.name}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Export CSV to read into R\n",
    "outdir = r\"C:\\Users\\dwmerrigan\\Documents\\GitHub\\AKSSF\\data_preparation\\sensitivity_drivers\"\n",
    "sj_outname = 'AKSSF_sites_sj_maxfac.csv'\n",
    "arcpy.conversion.TableToTable(sitesmerge, outdir, sj_outname)\n",
    "print('CSV export complete')\n",
    "print('----------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Must remove fields with null values or give null meaning values for numpy array\n",
    "# Make site merge df\n",
    "sitemerge_df = pd.DataFrame()\n",
    "fields = ['SiteID', 'cat_ID_con', 'area_diff', 'lat', 'lon', 'site_max_acc', 'site_acc_sqKm', 'Area_km2', 'str_slope',\n",
    "          'str_ord']\n",
    "#fields = keep_fields\n",
    "sitesmerge_arr = arcpy.da.TableToNumPyArray(sitesmerge, fields)\n",
    "sitemerge_df = pd.DataFrame(sitesmerge_arr)\n",
    "sitemerge_df = sitemerge_df.set_index('cat_ID_con')\n",
    "\n",
    "sitemerge_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}