{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a id='top'></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Watershed metrics for all Huc12 subwatersheds that intersect AWC recorded streams\n",
    "Iterate over AKSSF regions and identify all HUC12 sub-watersheds that intersect an AWC recorded stream. Identify the downstream-most/outlet catchment for each Huc12 from this pool and convert the polygon to INSIDE centroid point.  Calculate the distance to coastline as the straight line distance in Km from centroid point to NHD recorded coastline and export this as a feature class/table.  Next use the outlet catchments unique identifier to query the appropriate dataset and build watersheds for each outlet catchment.  Calculate watershed metrics listed in the covariate section and export final merged csv using the catchment unique identifier field \"cat_ID_con\" to link the metric back to the source catchment/HUC12.  Merge watersheds together and use to calculate covariates.\n",
    "## Covariates\n",
    "Covariates needed for prediction on AWC-HUC12 outlets are as follows:\n",
    "### Summer Precipitation\n",
    "To be calculated in R using the outlet catchment centroid point feature class exported during outlet identification process.\n",
    "### Watershed Slope Metrics\n",
    "Regional Slope grids created in AKSSF_merge_grids.ipynb script.\n",
    "Run zonal statistics on slope grid using merged watershed as zone feature.\n",
    "Field names and descriptions:\n",
    "* **awc_huc12s_wtd_slope_mn = mean watershed slope**\n",
    "* **awc_huc12s_wtd_slope_min = min watershed slope**\n",
    "* **awc_huc12s_wtd_slope_max = max watershed slope**\n",
    "* **awc_huc12s_wtd_slope_sd (or cv) = standard deviation of watershed slope**\n",
    "### Watershed Percent North Aspect\n",
    "Regional North grids created in AKSSF_merge_grids.ipynb scripts.\n",
    "North = aspects from 315-45 degrees and calculate the percentage of land area facing north for each watershed. Run tabulate area on north grid using merged watershed as zone feature and calculate percentage from area.\n",
    "Field names and descriptions:\n",
    "* **awc_huc12s_north_wtd = percent watershed with north aspect**\n",
    "### Watershed Percent Lake Cover\n",
    "Lakes feature classes for each network datatype (NHDPlus vs TauDEM) stored in AKSSF hydrography database on the T:\n",
    "Calculate percentage of watershed that is covered by lakes/ponds using tabulate interesection between lake features and watersheds.\n",
    "Field names and descriptions:\n",
    "* **awc_huc12s_wtd_lake_per = percent watershed covered by lakes**\n",
    "### Watershed Percent Glacier Cover\n",
    "Use input glacier fc (from previous covariate calculations) stored in regional gdbs an calculate percent of watershed with glacial coverage using tabulate intersection between lake features and watersheds.\n",
    "Field names and descriptions:\n",
    "* **awc_huc12s_wtd_glac_per = percent watershed covered by glaciers**\n",
    "### Watershed LCLD\n",
    "LCLD rasters created in AKSSF_MODIS_lcld_ipynb.\n",
    "Iterate over LCLD input rasters to produce yearly means for watersheds using zonal statistics.\n",
    "Field names and descriptions:\n",
    "* **awc_huc12s_wtd_lcld_mn_YYYY = mean lcld**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import modules\n",
    "Set initial environments and import modules\n",
    "Print system paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports complete\n",
      "----------------------------------------------------------------------------------------------------\n",
      "sys paths ['C:\\\\Users\\\\dwmerrigan\\\\Documents\\\\GitHub\\\\AKSSF\\\\data_preparation\\\\sensitivity_drivers\\\\landcover', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\Resources\\\\ArcPy', 'C:\\\\Users\\\\dwmerrigan\\\\Documents\\\\GitHub\\\\AKSSF', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\python37.zip', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\DLLs', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3', '', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\Resources\\\\ArcToolbox\\\\Scripts', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\Resources\\\\ArcToolBox\\\\Scripts', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\Resources\\\\ArcToolBox\\\\Scripts\\\\archydro', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\Resources\\\\ArcToolBox\\\\Scripts\\\\GRAIP', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\future-0.18.2-py3.7.egg', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\pytz-2020.1-py3.7.egg', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\pywin32_ctypes-0.2.0-py3.7.egg', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\pywin32security', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\sympy-1.5.1-py3.7.egg', 'C:\\\\Users\\\\dwmerrigan\\\\AppData\\\\Local\\\\Programs\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\dwmerrigan\\\\.ipython']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Python Environment set to - C:\\Users\\dwmerrigan\\AppData\\Local\\Programs\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2022-02-09 17:16:38.731405\n",
      "CSV table output directory set to C:\\Users\\dwmerrigan\\Documents\\GitHub\\AKSSF\\data_preparation\\sensitivity_drivers\n"
     ]
    }
   ],
   "source": [
    "import os, arcpy, sys,datetime\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "\n",
    "print('imports complete')\n",
    "print(f'{(\"-\"*100)}')\n",
    "print(f'sys paths {sys.path}')\n",
    "print(f'{(\"-\"*100)}')\n",
    "print(f'Python Environment set to - {sys.base_exec_prefix}')\n",
    "print(f'{(\"-\"*100)}')\n",
    "print (datetime.datetime.now())\n",
    "outdir = os.path.dirname(os.getcwd())\n",
    "print(f'CSV table output directory set to {outdir}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions\n",
    "Define any functions that will be used"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Function to add key, value pairs to dictionary\n",
    "def append_value(dict_obj, key, value):\n",
    "    # Check if key exist in dict or not\n",
    "    if key in dict_obj:\n",
    "        # Key exist in dict.\n",
    "        # Check if type of value of key is list or not\n",
    "        if not isinstance(dict_obj[key], list):\n",
    "            # If type is not list then make it list\n",
    "            dict_obj[key] = [dict_obj[key]]\n",
    "        # Append the value in list\n",
    "        dict_obj[key].append(value)\n",
    "    else:\n",
    "        # As key is not in dict,\n",
    "        # so, add key-value pair\n",
    "        dict_obj[key] = value\n",
    "# Function to remove parenthesis from user inputs\n",
    "def replace_all(userinput, dic):\n",
    "    for i, j in dic.items():\n",
    "        userinput = userinput.replace(i, j)\n",
    "    return userinput\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 1\n",
    "### Set input datasets, output locations, and scratch workspaces\n",
    "User to input paths for necessary input data and output locations\n",
    "Scratch workspaces and output workspaces will be automatically created if they do not already exist."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHDPlus lakes set to D:\\Basedata\\AKSSF_Basedata\\AKSSF_Basedata.gdb\\AKSSF_NHDPlus_LakePond_alb\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "NHD_H_Alaska lakes for TauDEM regions set to D:\\Basedata\\AKSSF_Basedata\\AKSSF_Basedata.gdb\\AKSSF_NHDPlus_LakePond_alb\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "AKSSF parent directory set to D:\\GIS\\AKSSF\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "AWC events feature class set to D:\\Basedata\\AWC\\AWC_2021_SpeciesEvents.gdb\\awcEventArcs\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Output locations will be created at D:\\GIS\\\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "LCLD subfolders located at D:\\Basedata\\LCLD_rasters_archive\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Working Folder already created D:\\GIS\\AKSSF_awcHuc12_cv\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Output location already exists D:\\GIS\\AKSSF_awcHuc12_cv\\AKSSF_awcHuc12_cv.gdb\n",
      " ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get user inputs\n",
    "# Used to format user inputs\n",
    "inputDict = {\"'\":\"\",'\"':\"\"}\n",
    "\n",
    "# Specify path to nhdPlus lakes\n",
    "while True:\n",
    "    try:\n",
    "        userinput7 = replace_all((input('Input path to NHDPlus lakes feature class.\\nHydrography database on T: has copy\\nLeave blank and hit enter to use the default location.\\nDefault = D:\\\\Basedata\\\\AKSSF_Basedata\\\\AKSSF_Basedata.gdb\\\\AKSSF_NHDPlus_LakePond_alb') or 'D:\\\\Basedata\\\\AKSSF_Basedata\\\\AKSSF_Basedata.gdb\\\\AKSSF_NHDPlus_LakePond_alb'),inputDict)\n",
    "        if not arcpy.Exists(userinput7):\n",
    "            print('Path specified does not exist!\\nPlease re-enter a valid path')\n",
    "            continue\n",
    "        else:\n",
    "            nhd_lakes_fc = userinput7\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted!')\n",
    "        sys.exit()\n",
    "\n",
    "print(f'NHDPlus lakes set to {nhd_lakes_fc}\\n {\"-\"*100}')\n",
    "\n",
    "# Specify path to nhd lakes for tau regions\n",
    "while True:\n",
    "    try:\n",
    "        userinput8 = replace_all((input('Input path to NHD_H_Alaska_State_GDB lakes feature class.\\nHydrography database on T: has copy\\nLeave blank and hit enter to use the default location.\\nDefault = D:\\\\Basedata\\\\AKSSF_Basedata\\\\AKSSF_Basedata.gdb\\\\AKSSF_NHD_LakesPonds_alb') or 'D:\\\\Basedata\\\\AKSSF_Basedata\\\\AKSSF_Basedata.gdb\\\\AKSSF_NHD_LakesPonds_alb'),inputDict)\n",
    "        if not arcpy.Exists(userinput8):\n",
    "            print('Path specified does not exist!\\nPlease re-enter a valid path')\n",
    "            continue\n",
    "        else:\n",
    "            tau_lakes_fc = userinput8\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted!')\n",
    "        sys.exit()\n",
    "print(f'NHD_H_Alaska lakes for TauDEM regions set to {nhd_lakes_fc}\\n {\"-\"*100}')\n",
    "\n",
    "# Specify path to AKSSF parent directory\n",
    "while True:\n",
    "    try:\n",
    "        userinput = replace_all((input('Input AKSSF parent directory containing regional sub-folders.\\nLeave blank and hit enter to use the default location.\\nDefault = D:\\\\GIS\\\\AKSSF\\\\') or 'D:\\\\GIS\\\\AKSSF'),inputDict)\n",
    "        if not arcpy.Exists(userinput):\n",
    "            print('Path specified does not exist!\\nPlease re-enter a valid path')\n",
    "            continue\n",
    "        else:\n",
    "            data_dir = userinput\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted!')\n",
    "        sys.exit()\n",
    "\n",
    "print(f'AKSSF parent directory set to {data_dir}\\n {\"-\"*100}')\n",
    "\n",
    "# Specify path to AWC events fc\n",
    "while True:\n",
    "    try:\n",
    "        userinput2 = replace_all((input('Input path to awc events feature class or shapefile.\\nLeave blank and hit enter to use the default location.\\nDefault = D:\\\\Basedata\\\\AWC\\\\AWC_2021_SpeciesEvents.gdb\\\\awcEventArcs') or \"D:\\\\Basedata\\\\AWC\\\\AWC_2021_SpeciesEvents.gdb\\\\awcEventArcs\"), inputDict)\n",
    "        if not arcpy.Exists(userinput2):\n",
    "            print('Path specified does not exist!\\nPlease re-enter a valid path')\n",
    "            continue\n",
    "        else:\n",
    "            awc_events = userinput2\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted!')\n",
    "        sys.exit()\n",
    "print(f'AWC events feature class set to {awc_events}\\n {\"-\"*100}')\n",
    "\n",
    "# Enter output destination  - to create working folders and gdbs\n",
    "while True:\n",
    "    try:\n",
    "        userinput3 = replace_all((input('Input path to create working folders.\\nLeave blank and hit enter to use the default location.\\nDefault = D:\\\\GIS\\\\') or 'D:\\\\GIS\\\\'),inputDict)\n",
    "        if not arcpy.Exists(userinput):\n",
    "            print('Path specified does not exist!\\nPlease re-enter a valid path')\n",
    "            continue\n",
    "        else:\n",
    "            temp_path = userinput3\n",
    "            print(f'Output locations will be created at {temp_path}\\n {\"-\"*100}')\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted!')\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "# Path to lcld rasters\n",
    "lcld_folder = r'D:\\\\Basedata\\\\LCLD_rasters_archive'\n",
    "# Enter output destination  - to create working folders and gdbs\n",
    "while True:\n",
    "    try:\n",
    "        userinput4 = replace_all((input('Input path to LCLD raster parent folder.\\nLeave blank and hit enter to use the default location.\\nDefault = D:\\\\Basedata\\\\LCLD_rasters_archive\\\\') or 'D:\\\\Basedata\\\\LCLD_rasters_archive'),inputDict)\n",
    "        if not arcpy.Exists(userinput):\n",
    "            print('Path specified does not exist!\\nPlease re-enter a valid path')\n",
    "            continue\n",
    "        else:\n",
    "            lcld_folder = userinput4\n",
    "            print(f'LCLD subfolders located at {lcld_folder}\\n {\"-\"*100}')\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted!')\n",
    "        sys.exit()\n",
    "\n",
    "## Create working output location to store intermediate data\n",
    "dirname = 'AKSSF_awcHuc12_cv'\n",
    "tempgdbname = 'AKSSF_awcHuc12_cv.gdb'\n",
    "temp_dir = os.path.join(temp_path, dirname)\n",
    "\n",
    "# Create temporary working gdb\n",
    "if not arcpy.Exists(temp_dir):\n",
    "    os.makedirs(temp_dir)\n",
    "else:\n",
    "    print(f'Working Folder already created {temp_dir}\\n {\"-\"*100}')\n",
    "\n",
    "outcheck = os.path.join(temp_dir, tempgdbname)\n",
    "\n",
    "if arcpy.Exists(outcheck):\n",
    "    print (f'Output location already exists {outcheck}\\n {\"-\"*100}')\n",
    "    outgdb = outcheck\n",
    "if not arcpy.Exists(outcheck):\n",
    "    print(f'Creating output GDB\\n {\"-\"*100}')\n",
    "    tempgdb = arcpy.CreateFileGDB_management(temp_dir,tempgdbname)\n",
    "    print (f'Output geodatabase created at {outcheck}\\n {\"-\"*100}')\n",
    "    outgdb = tempgdb.getOutput(0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 1.1\n",
    "### Set and create local copies of additional input data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tau Region Hucs D:\\GIS\\AKSSF_awcHuc12_cv\\AKSSF_awcHuc12_cv.gdb\\NHD_H_HUC12 located and exists = True\n",
      "NHDPlus Hucs D:\\GIS\\AKSSF_awcHuc12_cv\\AKSSF_awcHuc12_cv.gdb\\NHDPlusHUC12 located and exists = True\n"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "\n",
    "nhdplusfol = []\n",
    "tahuc12=[]\n",
    "\n",
    "# Create and set HUC12 data if it does not already exist\n",
    "nhdplushucs = os.path.join(outgdb, 'NHDPlusHUC12')\n",
    "tauhucs = os.path.join(outgdb, 'NHD_H_HUC12')\n",
    "\n",
    "if not arcpy.Exists(tauhucs):\n",
    "    print(f'Huc12 data for Tau Regions not yet created')\n",
    "    #Enter path to WBDHU12 from NHD_H gdb\n",
    "    while True:\n",
    "        try:\n",
    "            userinput6 = replace_all((input('Input path to source WBDHU12 for state of Alaska.\\nLeave blank and hit enter to use the default location.\\nDefault = D:\\\\Basedata\\\\NHD_H_Alaska_State_GDB.gdb\\\\WBD\\\\WBDHU12') or 'D:\\\\Basedata\\\\NHD_H_Alaska_State_GDB.gdb\\\\WBD\\\\WBDHU12'),inputDict)\n",
    "            if not arcpy.Exists(userinput6):\n",
    "                print('Path specified does not exist!\\nPlease re-enter a valid path')\n",
    "                continue\n",
    "            else:\n",
    "                tauhuc12 = userinput6\n",
    "                arcpy.CopyFeatures_management(tauhuc12,tauhucs)\n",
    "                print(f'WBD Huc12  copied to {tauhucs}\\n {\"-\"*100}')\n",
    "                break\n",
    "        except KeyboardInterrupt:\n",
    "            print('interrupted!')\n",
    "            sys.exit()\n",
    "\n",
    "else:\n",
    "    print(f'Tau Region Hucs {tauhucs} located and exists = {arcpy.Exists(tauhucs)}')\n",
    "\n",
    "if not arcpy.Exists(nhdplushucs):\n",
    "    print(f'Huc12 data for NHDPlus Regions not yet created')\n",
    "    #Enter NHDplus data folder\n",
    "    while True:\n",
    "        try:\n",
    "            userinput5 = replace_all((input('Input path to source NHDPlus parent folder.\\nLeave blank and hit enter to use the default location.\\nDefault = D:\\\\Basedata\\\\NHDPlus') or 'D:\\\\Basedata\\\\NHDPlus'),inputDict)\n",
    "            if not arcpy.Exists(userinput5):\n",
    "                print('Path specified does not exist!\\nPlease re-enter a valid path')\n",
    "                continue\n",
    "            else:\n",
    "                nhdplusfol = userinput5\n",
    "                print(f'NHD HUC12 will be copied to {nhdplushucs}\\n {\"-\"*100}')\n",
    "                hucs = []\n",
    "                walk = arcpy.da.Walk(nhdplusfol, datatype=\"FeatureClass\", type=\"Polygon\")\n",
    "\n",
    "                for dirpath, dirnames, filenames in walk:\n",
    "                    for filename in filenames:\n",
    "                        if filename == 'WBDHU12':\n",
    "                            hucs.append(os.path.join(dirpath, filename))\n",
    "                arcpy.Merge_management(hucs,nhdplushucs,'','ADD_SOURCE_INFO')\n",
    "                break\n",
    "        except KeyboardInterrupt:\n",
    "            print('interrupted!')\n",
    "            sys.exit()\n",
    "else:\n",
    "    print(f'NHDPlus Hucs {nhdplushucs} located and exists = {arcpy.Exists(nhdplushucs)}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 2\n",
    "### By Region\n",
    "Identify downstream-most catchment for each Huc 12\n",
    " * Select by location and select catchment with most us contributing area\n",
    "    * NHDPlus\n",
    "        * Use update cursor to join TotalDrainageAreaSqKm from vaa table to catchment\n",
    "        * Find max value from selection and save as outlet catchment for that HUC12\n",
    "    * TauDEM\n",
    "        * DSContArea - Drainage area at the downstream end of the link. Generally this is one grid cell upstream of the downstream end because the drainage area at the downstream end grid cell includes the area of the stream being joined.\n",
    " * Generate Centroid point and append to centroid dataset\n",
    "    * Retain cat_id and Huc12-id\n",
    " * Append to HUC12 catchment dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prince_William_Sound\n",
      "Prince_William_Sound using data from Prince_William_Sound folder\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Region Prince_William_Sound will not be processed\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "158 huc12s in Prince_William_Sound\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "112 Huc12s in Prince_William_Sound intersect awc events input\n",
      "****************************************************************************************************\n",
      "Processing HUC 190202011602\n",
      "1. Finding outlet for HUC 190202011602 out of 253 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011604\n",
      "2. Finding outlet for HUC 190202011604 out of 105 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012506\n",
      "3. Finding outlet for HUC 190202012506 out of 132 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011106\n",
      "4. Finding outlet for HUC 190202011106 out of 155 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012304\n",
      "5. Finding outlet for HUC 190202012304 out of 490 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010602\n",
      "6. Finding outlet for HUC 190202010602 out of 123 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012502\n",
      "7. Finding outlet for HUC 190202012502 out of 161 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011307\n",
      "8. Finding outlet for HUC 190202011307 out of 656 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012001\n",
      "9. Finding outlet for HUC 190202012001 out of 88 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010301\n",
      "10. Finding outlet for HUC 190202010301 out of 277 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011603\n",
      "11. Finding outlet for HUC 190202011603 out of 38 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012309\n",
      "12. Finding outlet for HUC 190202012309 out of 261 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011108\n",
      "13. Finding outlet for HUC 190202011108 out of 130 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011503\n",
      "14. Finding outlet for HUC 190202011503 out of 234 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011804\n",
      "15. Finding outlet for HUC 190202011804 out of 481 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012508\n",
      "16. Finding outlet for HUC 190202012508 out of 342 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011601\n",
      "17. Finding outlet for HUC 190202011601 out of 82 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012101\n",
      "18. Finding outlet for HUC 190202012101 out of 102 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010703\n",
      "19. Finding outlet for HUC 190202010703 out of 128 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011107\n",
      "20. Finding outlet for HUC 190202011107 out of 192 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010105\n",
      "21. Finding outlet for HUC 190202010105 out of 281 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012103\n",
      "22. Finding outlet for HUC 190202012103 out of 50 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010603\n",
      "23. Finding outlet for HUC 190202010603 out of 59 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011403\n",
      "24. Finding outlet for HUC 190202011403 out of 145 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010907\n",
      "25. Finding outlet for HUC 190202010907 out of 272 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010701\n",
      "26. Finding outlet for HUC 190202010701 out of 244 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011105\n",
      "27. Finding outlet for HUC 190202011105 out of 395 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012204\n",
      "28. Finding outlet for HUC 190202012204 out of 288 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012307\n",
      "29. Finding outlet for HUC 190202012307 out of 97 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010905\n",
      "30. Finding outlet for HUC 190202010905 out of 289 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011308\n",
      "31. Finding outlet for HUC 190202011308 out of 137 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012102\n",
      "32. Finding outlet for HUC 190202012102 out of 185 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011504\n",
      "33. Finding outlet for HUC 190202011504 out of 186 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011805\n",
      "34. Finding outlet for HUC 190202011805 out of 460 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011505\n",
      "35. Finding outlet for HUC 190202011505 out of 525 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010704\n",
      "36. Finding outlet for HUC 190202010704 out of 90 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011906\n",
      "37. Finding outlet for HUC 190202011906 out of 321 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010601\n",
      "38. Finding outlet for HUC 190202010601 out of 113 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010804\n",
      "39. Finding outlet for HUC 190202010804 out of 149 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011102\n",
      "40. Finding outlet for HUC 190202011102 out of 286 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011401\n",
      "41. Finding outlet for HUC 190202011401 out of 253 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012507\n",
      "42. Finding outlet for HUC 190202012507 out of 148 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012505\n",
      "43. Finding outlet for HUC 190202012505 out of 91 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010702\n",
      "44. Finding outlet for HUC 190202010702 out of 166 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011103\n",
      "45. Finding outlet for HUC 190202011103 out of 154 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011402\n",
      "46. Finding outlet for HUC 190202011402 out of 159 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012501\n",
      "47. Finding outlet for HUC 190202012501 out of 123 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010102\n",
      "48. Finding outlet for HUC 190202010102 out of 337 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010401\n",
      "49. Finding outlet for HUC 190202010401 out of 85 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012301\n",
      "50. Finding outlet for HUC 190202012301 out of 390 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010302\n",
      "51. Finding outlet for HUC 190202010302 out of 149 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010908\n",
      "52. Finding outlet for HUC 190202010908 out of 603 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011109\n",
      "53. Finding outlet for HUC 190202011109 out of 146 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011110\n",
      "54. Finding outlet for HUC 190202011110 out of 74 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012104\n",
      "55. Finding outlet for HUC 190202012104 out of 332 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011104\n",
      "56. Finding outlet for HUC 190202011104 out of 508 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012002\n",
      "57. Finding outlet for HUC 190202012002 out of 328 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011606\n",
      "58. Finding outlet for HUC 190202011606 out of 85 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010604\n",
      "59. Finding outlet for HUC 190202010604 out of 325 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010402\n",
      "60. Finding outlet for HUC 190202010402 out of 212 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011101\n",
      "61. Finding outlet for HUC 190202011101 out of 131 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011202\n",
      "62. Finding outlet for HUC 190202011202 out of 219 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010200\n",
      "63. Finding outlet for HUC 190202010200 out of 402 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012504\n",
      "64. Finding outlet for HUC 190202012504 out of 217 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012510\n",
      "65. Finding outlet for HUC 190202012510 out of 185 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012310\n",
      "66. Finding outlet for HUC 190202012310 out of 281 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011404\n",
      "67. Finding outlet for HUC 190202011404 out of 128 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012105\n",
      "68. Finding outlet for HUC 190202012105 out of 350 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011201\n",
      "69. Finding outlet for HUC 190202011201 out of 134 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012509\n",
      "70. Finding outlet for HUC 190202012509 out of 156 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010705\n",
      "71. Finding outlet for HUC 190202010705 out of 461 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012003\n",
      "72. Finding outlet for HUC 190202012003 out of 400 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012511\n",
      "73. Finding outlet for HUC 190202012511 out of 67 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012306\n",
      "74. Finding outlet for HUC 190202012306 out of 97 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202012308\n",
      "75. Finding outlet for HUC 190202012308 out of 105 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010503\n",
      "76. Finding outlet for HUC 190202010503 out of 502 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010303\n",
      "77. Finding outlet for HUC 190202010303 out of 48 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202010906\n",
      "78. Finding outlet for HUC 190202010906 out of 231 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202011605\n",
      "79. Finding outlet for HUC 190202011605 out of 209 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030105\n",
      "80. Finding outlet for HUC 190202030105 out of 302 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030208\n",
      "81. Finding outlet for HUC 190202030208 out of 85 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030302\n",
      "82. Finding outlet for HUC 190202030302 out of 46 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030204\n",
      "83. Finding outlet for HUC 190202030204 out of 65 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030205\n",
      "84. Finding outlet for HUC 190202030205 out of 144 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030304\n",
      "85. Finding outlet for HUC 190202030304 out of 232 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030202\n",
      "86. Finding outlet for HUC 190202030202 out of 135 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030207\n",
      "87. Finding outlet for HUC 190202030207 out of 93 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030701\n",
      "88. Finding outlet for HUC 190202030701 out of 278 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030104\n",
      "89. Finding outlet for HUC 190202030104 out of 108 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030502\n",
      "90. Finding outlet for HUC 190202030502 out of 99 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030503\n",
      "91. Finding outlet for HUC 190202030503 out of 377 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030206\n",
      "92. Finding outlet for HUC 190202030206 out of 48 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030201\n",
      "93. Finding outlet for HUC 190202030201 out of 103 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030409\n",
      "94. Finding outlet for HUC 190202030409 out of 218 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030403\n",
      "95. Finding outlet for HUC 190202030403 out of 159 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030101\n",
      "96. Finding outlet for HUC 190202030101 out of 476 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030203\n",
      "97. Finding outlet for HUC 190202030203 out of 122 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030406\n",
      "98. Finding outlet for HUC 190202030406 out of 445 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030305\n",
      "99. Finding outlet for HUC 190202030305 out of 47 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030102\n",
      "100. Finding outlet for HUC 190202030102 out of 284 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030306\n",
      "101. Finding outlet for HUC 190202030306 out of 67 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030702\n",
      "102. Finding outlet for HUC 190202030702 out of 1016 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030209\n",
      "103. Finding outlet for HUC 190202030209 out of 147 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030303\n",
      "104. Finding outlet for HUC 190202030303 out of 157 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030103\n",
      "105. Finding outlet for HUC 190202030103 out of 233 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030408\n",
      "106. Finding outlet for HUC 190202030408 out of 188 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030402\n",
      "107. Finding outlet for HUC 190202030402 out of 240 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030405\n",
      "108. Finding outlet for HUC 190202030405 out of 207 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030401\n",
      "109. Finding outlet for HUC 190202030401 out of 106 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030404\n",
      "110. Finding outlet for HUC 190202030404 out of 62 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030301\n",
      "111. Finding outlet for HUC 190202030301 out of 88 catchments.\n",
      "************************************************************\n",
      "Processing HUC 190202030407\n",
      "112. Finding outlet for HUC 190202030407 out of 136 catchments.\n",
      "************************************************************\n",
      "Creating copy of 112 outlet catchments for Region Prince_William_Sound at D:\\GIS\\AKSSF_awcHuc12_cv\\AKSSF_awcHuc12_cv.gdb\\Prince_William_Sound_TauAwcH12_cats_outlets\n",
      "****************************************************************************************************\n",
      "Prince_William_Sound Elapsed time: (0:00:30)\n",
      "************************************************************\n",
      "Process complete\n",
      "Process completed at 2022-02-09 18:12 (Elapsed time: 0:00:30)\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "import arcpy, time, os, datetime, operator\n",
    "\n",
    "arcpy.env.workspace = data_dir\n",
    "regions = arcpy.ListWorkspaces()\n",
    "\n",
    "# Dictionaries and lists\n",
    "nhdplusoutlets = []\n",
    "tauoutlets = []\n",
    "nhdplusawccatouts = []\n",
    "tauawccatouts = []\n",
    "vaaDict = {}\n",
    "strDict = {}\n",
    "catsDict = {}\n",
    "huc12Dict = {}\n",
    "nhdidDict = {}\n",
    "tauidDict = {}\n",
    "tauhuc12Dict = {}\n",
    "\n",
    "# Separate data by source type\n",
    "nhdplus_dat = ['Cook_Inlet','Copper_River']\n",
    "tauDem_dat = ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound']\n",
    "\n",
    "# Loop through all processing areas\n",
    "# rois = nhdplus_dat + tauDem_dat\n",
    "\n",
    "# Or comment above and specify below specific subset\n",
    "#regions = ['D:\\\\GIS\\\\AKSSF\\\\Cook_Inlet', 'D:\\\\GIS\\\\AKSSF\\\\Copper_River' ]\n",
    "regions = ['Prince_William_Sound']\n",
    "# Start timing function\n",
    "processStart = time.time()\n",
    "processStartdt = datetime.datetime.now()\n",
    "\n",
    "for region in regions:\n",
    "    roi = os.path.basename(region)\n",
    "    print(roi)\n",
    "    if roi in nhdplus_dat:\n",
    "        # Start roi time\n",
    "        roi_start = time.time()\n",
    "        hucs = nhdplushucs\n",
    "        catsList = []\n",
    "        outletList = []\n",
    "        print(f'{roi} using data from {region} folder')\n",
    "        # Set workspace to region folder\n",
    "        arcpy.env.workspace = region\n",
    "        gdb = arcpy.ListWorkspaces(workspace_type='FileGDB')\n",
    "        sourcegdb = gdb[0]\n",
    "        walk = arcpy.da.Walk(sourcegdb, datatype = ['FeatureClass','Table'])\n",
    "        for dirpath, dirnames, filenames in walk:\n",
    "            for filename in filenames:\n",
    "                if filename == 'cats_merge':\n",
    "                    cats  = os.path.join(dirpath, filename)\n",
    "                    append_value(catsDict,roi,cats)\n",
    "                elif filename == 'vaa_merge':\n",
    "                    vaas = os.path.join(dirpath, filename)\n",
    "                    append_value(vaaDict, roi, vaas)\n",
    "        #Output names and paths\n",
    "        outletcatsname = roi + '_NhdAwcH12_cats_outlets'\n",
    "        outcatspath = os.path.join(outgdb,outletcatsname)\n",
    "        outcatspath2 = os.path.join(sourcegdb,'awc_huc12_catchment_outlets')\n",
    "        outletcatptsname = roi + '_NhdAwcH12_cats_outlets_pts'\n",
    "        outcatptspath = os.path.join(outgdb,outletcatptsname)\n",
    "        outcatptspath2 = os.path.join(sourcegdb,'awc_huc12_catchment_outlets_pts')\n",
    "\n",
    "        if not arcpy.Exists(outcatspath):\n",
    "            # Build Value dictionary to relate NHDPlus id to contributing area\n",
    "            fields = ['NHDPlusID','TotDASqKm']\n",
    "            fields2 = fields + ['cat_ID_con']\n",
    "            valueDict = {int(r[0]):(r[1]) for r in arcpy.da.SearchCursor(vaas, fields)}\n",
    "            where_clause=f'\"MERGE_SRC\" LIKE \\'%{roi}%\\''\n",
    "            print(f'where_clause = {where_clause}')\n",
    "            huclayer = arcpy.MakeFeatureLayer_management(hucs,'huclayer',where_clause = where_clause)\n",
    "            print(f'{arcpy.GetCount_management(huclayer)} huc12s in {roi}')\n",
    "            print(('*'*100))\n",
    "            hucselect = arcpy.SelectLayerByLocation_management(huclayer,'INTERSECT',awc_events,'','SUBSET_SELECTION')\n",
    "            print(('*'*100))\n",
    "            print(f'{arcpy.GetCount_management(hucselect)} Huc12s in {roi} intersect awc events input')\n",
    "            print(('*'*100))\n",
    "            hucFields = [f for f in arcpy.ListFields(hucselect)]\n",
    "            vcount =1\n",
    "            with arcpy.da.SearchCursor(hucselect,['HUC12','SHAPE@']) as cur:\n",
    "                for row in cur:\n",
    "                    print(f'Processing HUC {row[0]}')\n",
    "                    inhuc = row[1]\n",
    "                    cat_layer = arcpy.MakeFeatureLayer_management(cats,'cat_layer')\n",
    "                    # Select by location using awc and huc 12\n",
    "                    arcpy.SelectLayerByLocation_management(cat_layer,'HAVE_THEIR_CENTER_IN',inhuc,'','NEW_SELECTION')\n",
    "                    print(f'{vcount}. Finding outlet for HUC {row[0]} out of {arcpy.GetCount_management(cat_layer)} catchments.\\n{(\"*\" * 60)}')\n",
    "                    catList = [r[0] for r in arcpy.da.SearchCursor(cat_layer, 'NHDPlusID')]\n",
    "                    intersect = list(set(catList).intersection(valueDict))\n",
    "                    catDict = {int(i):(valueDict[i]) for i in intersect}\n",
    "                    # Find Catchment with max drainage area\n",
    "                    outcatch = max(catDict.items(), key = operator.itemgetter(1))[0]\n",
    "                    append_value(huc12Dict, row[0], [int(outcatch),roi,valueDict[int(outcatch)]])\n",
    "                    append_value(nhdidDict,int(outcatch),[roi,row[0], valueDict[int(outcatch)]])\n",
    "                    outletList.append(int(outcatch))\n",
    "                    vcount+=1\n",
    "                del(row)\n",
    "            del(cur)\n",
    "\n",
    "            outlet_cats = arcpy.MakeFeatureLayer_management(cats,'outlet_cats')\n",
    "            out_expression ='\"NHDPlusID\" IN ' + str(tuple(outletList))\n",
    "            #print(out_expression)\n",
    "            outlet_cats_select = arcpy.SelectLayerByAttribute_management(outlet_cats,'NEW_SELECTION', out_expression)\n",
    "            print(f'Creating copy of {arcpy.GetCount_management(outlet_cats)} outlet catchments for Region {roi} at {outcatspath}')\n",
    "            print(('*'*100))\n",
    "\n",
    "            # Copy outputs\n",
    "            arcpy.FeatureClassToFeatureClass_conversion(outlet_cats_select,outgdb,outletcatsname)\n",
    "            arcpy.FeatureToPoint_management(outcatspath, outcatptspath, 'INSIDE')\n",
    "            # Create Copies to akssf data_dir regional gdbs also\n",
    "            arcpy.FeatureClassToFeatureClass_conversion(outlet_cats_select,sourcegdb,'awc_huc12_catchment_outlets')\n",
    "            arcpy.FeatureToPoint_management(outcatspath2, outcatptspath2, 'INSIDE')\n",
    "            nhdplusoutlets.append(outcatptspath)\n",
    "            nhdplusawccatouts.append(outcatspath)\n",
    "            # Add total drainage km from value dict to feature classes and cat_ID_con from regDict\n",
    "            upfcs = [outcatspath, outcatptspath,outcatptspath2,outcatptspath2]\n",
    "            for upfc in upfcs:\n",
    "                arcpy.AddField_management(upfc,fields[1],'TEXT')\n",
    "                arcpy.AddField_management(upfc,fields2[2],'TEXT')\n",
    "                with arcpy.da.UpdateCursor(upfc,fields2) as cur:\n",
    "                    for row in cur:\n",
    "                        row[1] = valueDict[row[0]]\n",
    "                        row[2] = roi + '_' + str(int(row[0]))\n",
    "                        cur.updateRow(row)\n",
    "                    del(row)\n",
    "                del(cur)\n",
    "\n",
    "            # End roi time\n",
    "            roi_stop = time.time()\n",
    "            roi_time = int (roi_stop - roi_start)\n",
    "            print(f'{roi} Elapsed time: ({datetime.timedelta(seconds=roi_time)})')\n",
    "            print(f'{\"*\"*60}')\n",
    "        else:\n",
    "            print(f'Catchments for {roi} already created at {outcatspath2}')\n",
    "\n",
    "    elif roi in tauDem_dat:\n",
    "        # Start roi time\n",
    "        roi_start = time.time()\n",
    "        hucs = tauhucs\n",
    "        catsList = []\n",
    "        outletList = []\n",
    "        print(f'{roi} using data from {region} folder')\n",
    "        # Set workspace to region folder\n",
    "        arcpy.env.workspace = region\n",
    "        gdb = arcpy.ListWorkspaces(workspace_type='FileGDB')\n",
    "        sourcegdb = gdb[0]\n",
    "        walk = arcpy.da.Walk(sourcegdb, datatype = ['FeatureClass','Table'])\n",
    "        for dirpath, dirnames, filenames in walk:\n",
    "            for filename in filenames:\n",
    "                if filename == 'cats_merge':\n",
    "                    cats  = os.path.join(dirpath, filename)\n",
    "                    append_value(catsDict,roi,cats)\n",
    "                elif filename == 'streams_merge':\n",
    "                    streams = os.path.join(dirpath, filename)\n",
    "                    append_value(strDict, roi, streams)\n",
    "\n",
    "        #Output names and paths\n",
    "        outletcatsname = roi + '_TauAwcH12_cats_outlets'\n",
    "        outcatspath = os.path.join(outgdb,outletcatsname)\n",
    "        outcatspath2 = os.path.join(sourcegdb,'awc_huc12_catchment_outlets')\n",
    "        outletcatptsname = roi + '_TauAwcH12_cats_outlets_pts'\n",
    "        outcatptspath = os.path.join(outgdb,outletcatptsname)\n",
    "        outcatptspath2 = os.path.join(sourcegdb,'awc_huc12_catchment_outlets_pts')\n",
    "        print(('-'*100),'\\n')\n",
    "        if not arcpy.Exists(outcatspath):\n",
    "            # Build Value dictionary to relate NHDPlus id to contributing area\n",
    "            fields = ['LINKNO','DSContArea']\n",
    "            fields2 = fields + ['cat_ID_con']\n",
    "            fields3 = ['gridcode','DSContArea','cat_ID_con']\n",
    "            valueDict = {int(r[0]):(r[1]) for r in arcpy.da.SearchCursor(streams, fields)}\n",
    "            huclayer = arcpy.MakeFeatureLayer_management(hucs,'huclayer')\n",
    "            hucselect_reg = arcpy.SelectLayerByLocation_management(huclayer,'INTERSECT',streams,'','NEW_SELECTION')\n",
    "            print(f'{arcpy.GetCount_management(huclayer)} huc12s in {roi}')\n",
    "            print(('*'*100))\n",
    "            hucselect = arcpy.SelectLayerByLocation_management(hucselect_reg,'INTERSECT',awc_events,'','SUBSET_SELECTION')\n",
    "            print(('*'*100))\n",
    "            print(f'{arcpy.GetCount_management(hucselect)} Huc12s in {roi} intersect awc events input')\n",
    "            print(('*'*100))\n",
    "            hucFields = [f for f in arcpy.ListFields(hucselect)]\n",
    "            vcount =1\n",
    "            with arcpy.da.SearchCursor(hucselect,['HUC12','SHAPE@']) as cur:\n",
    "                for row in cur:\n",
    "                    print(f'Processing HUC {row[0]}')\n",
    "                    inhuc = row[1]\n",
    "                    cat_layer = arcpy.MakeFeatureLayer_management(cats,'cat_layer')\n",
    "                    # Select by location using awc and huc 12\n",
    "                    arcpy.SelectLayerByLocation_management(cat_layer,'HAVE_THEIR_CENTER_IN',inhuc,'','NEW_SELECTION')\n",
    "                    print(f'{vcount}. Finding outlet for HUC {row[0]} out of {arcpy.GetCount_management(cat_layer)} catchments.\\n{(\"*\" * 60)}')\n",
    "                    catList = [r[0] for r in arcpy.da.SearchCursor(cat_layer, 'gridcode')]\n",
    "                    intersect = list(set(catList).intersection(valueDict))\n",
    "                    catDict = {int(i):(valueDict[i]) for i in intersect}\n",
    "                    # Find Catchment with max drainage area\n",
    "                    outcatch = max(catDict.items(), key = operator.itemgetter(1))[0]\n",
    "                    append_value(tauhuc12Dict, row[0], [int(outcatch),roi,valueDict[int(outcatch)]])\n",
    "                    append_value(tauidDict,int(outcatch),[roi,row[0], valueDict[int(outcatch)]])\n",
    "                    outletList.append(int(outcatch))\n",
    "                    vcount+=1\n",
    "                del(row)\n",
    "            del(cur)\n",
    "            outlet_cats = arcpy.MakeFeatureLayer_management(cats,'outlet_cats')\n",
    "            out_expression ='\"gridcode\" IN ' + str(tuple(outletList))\n",
    "            #print(out_expression)\n",
    "            outlet_cats_select = arcpy.SelectLayerByAttribute_management(outlet_cats,'NEW_SELECTION', out_expression)\n",
    "            print(f'Creating copy of {arcpy.GetCount_management(outlet_cats)} outlet catchments for Region {roi} at {outcatspath}')\n",
    "            print(('*'*100))\n",
    "\n",
    "            # Copy outputs\n",
    "            arcpy.FeatureClassToFeatureClass_conversion(outlet_cats_select,outgdb,outletcatsname)\n",
    "            arcpy.FeatureToPoint_management(outcatspath, outcatptspath, 'INSIDE')\n",
    "            # Create Copies to akssf data_dir regional gdbs also\n",
    "            arcpy.FeatureClassToFeatureClass_conversion(outlet_cats_select,sourcegdb,'awc_huc12_catchment_outlets')\n",
    "            arcpy.FeatureToPoint_management(outcatspath2, outcatptspath2, 'INSIDE')\n",
    "            tauoutlets.append(outcatptspath)\n",
    "            tauawccatouts.append(outcatspath)\n",
    "            # Add total drainage km from value dict to feature classes and cat_ID_con from regDict\n",
    "            upfcs = [outcatspath, outcatptspath,outcatptspath2,outcatptspath2]\n",
    "            for upfc in upfcs:\n",
    "                arcpy.AddField_management(upfc,fields[1],'TEXT')\n",
    "                arcpy.AddField_management(upfc,fields2[2],'TEXT')\n",
    "                with arcpy.da.UpdateCursor(upfc,fields3) as cur:\n",
    "                    for row in cur:\n",
    "                        row[1] = valueDict[row[0]]\n",
    "                        row[2] = roi + '_' + str(int(row[0]))\n",
    "                        cur.updateRow(row)\n",
    "                    del(row)\n",
    "                del(cur)\n",
    "\n",
    "            # End roi time\n",
    "            roi_stop = time.time()\n",
    "            roi_time = int (roi_stop - roi_start)\n",
    "            print(f'{roi} Elapsed time: ({datetime.timedelta(seconds=roi_time)})')\n",
    "            print(f'{\"*\"*60}')\n",
    "\n",
    "# End timing\n",
    "processEnd = time.time()\n",
    "processElapsed = int(processEnd - processStart)\n",
    "processSuccess_time = datetime.datetime.now()\n",
    "print(f'Process complete')\n",
    "\n",
    "# Report success\n",
    "print(f'Process completed at {processSuccess_time.strftime(\"%Y-%m-%d %H:%M\")} '\n",
    "      f'(Elapsed time: {datetime.timedelta(seconds=processElapsed)})')\n",
    "print(f'{\"*\"*100}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 2.1\n",
    "### Merge all outlet points together and calculate distance to coastline\n",
    "Calculate Distance to Coast from outlet catchment point to the nearest coastline as a straight line distance\n",
    " * Generate near table and export as seperate csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NHDPlus Section\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlet points already created at D:\\GIS\\AKSSF_awcHuc12_cv\\AKSSF_awcHuc12_cv.gdb\\AKSSF_NHDPlus_awcHuc12_outlet_cats_points\n"
     ]
    }
   ],
   "source": [
    "import arcpy, datetime\n",
    "import numpy as pd\n",
    "\n",
    "# Input path to coastline\n",
    "coast = r\"D:\\\\Basedata\\\\AKSSF_Basedata\\\\AKSSF_Basedata.gdb\\\\NHD_H_Alaska_Coastline_alb\"\n",
    "\n",
    "# Merge all catchment outlet centroids together\n",
    "nhdoutletsname = 'AKSSF_NHDPlus_awcHuc12_outlet_cats_points'\n",
    "nhdoutletspath = os.path.join(outgdb, nhdoutletsname)\n",
    "\n",
    "if not arcpy.Exists(nhdoutletspath):\n",
    "    all_nhd_outlet_pts = arcpy.Merge_management(nhdplusoutlets,nhdoutletspath)\n",
    "    # Start timing function\n",
    "    start = datetime.datetime.now()\n",
    "    print(f'Getting distance to coast {datetime.datetime.now()}...')\n",
    "    arcpy.analysis.Near(all_nhd_outlet_pts, coast, None, \"NO_LOCATION\", \"NO_ANGLE\", \"GEODESIC\", \"NEAR_DIST NEAR_DIST\")\n",
    "    arcpy.AlterField_management(all_nhd_outlet_pts,'NEAR_DIST','dist_catch_coast_km','dist_catch_coast_km' )\n",
    "    arcpy.AddField_management(all_nhd_outlet_pts,'HUC12','TEXT')\n",
    "\n",
    "    # Convert distance in meters to km\n",
    "    with arcpy.da.UpdateCursor(all_nhd_outlet_pts,['dist_catch_coast_km','NHDPlusID','HUC12']) as cur:\n",
    "        for row in cur:\n",
    "            row[0] = row[0] * 0.001\n",
    "            row[2] = nhdidDict[row[1]][1]\n",
    "            cur.updateRow(row)\n",
    "        del(row)\n",
    "    del(cur)\n",
    "    print(f'Process complete')\n",
    "else:\n",
    "    print(f'Outlet points already created at {nhdoutletspath}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TauDEM Section\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting distance to coast 2022-02-09 18:22:59.855894...\n",
      "Process complete\n"
     ]
    }
   ],
   "source": [
    "import arcpy, datetime\n",
    "import numpy as pd\n",
    "\n",
    "# Input path to coastline\n",
    "coast = r\"D:\\\\Basedata\\\\AKSSF_Basedata\\\\AKSSF_Basedata.gdb\\\\NHD_H_Alaska_Coastline_alb\"\n",
    "\n",
    "# Merge all catchment outlet centroids together\n",
    "tauoutname = 'AKSSF_TauDEM_awcHuc12_outlet_cats_points'\n",
    "tauoutpath = os.path.join(outgdb, tauoutname)\n",
    "\n",
    "if not arcpy.Exists(tauoutpath):\n",
    "    all_tau_outpts = arcpy.Merge_management(tauoutlets,tauoutpath)\n",
    "    # Start timing function\n",
    "    start = datetime.datetime.now()\n",
    "    print(f'Getting distance to coast {datetime.datetime.now()}...')\n",
    "    arcpy.analysis.Near(all_tau_outpts, coast, None, \"NO_LOCATION\", \"NO_ANGLE\", \"GEODESIC\", \"NEAR_DIST NEAR_DIST\")\n",
    "    arcpy.AlterField_management(all_tau_outpts,'NEAR_DIST','dist_catch_coast_km','dist_catch_coast_km' )\n",
    "    arcpy.AddField_management(all_tau_outpts,'HUC12','TEXT')\n",
    "    arcpy.AddField_management(all_tau_outpts,'DSContAreaSqKM','DOUBLE')\n",
    "\n",
    "    # Convert distance in meters to km\n",
    "    with arcpy.da.UpdateCursor(all_tau_outpts,['dist_catch_coast_km','gridcode','HUC12','DSContArea','DSContAreaSqKM']) as cur:\n",
    "        for row in cur:\n",
    "            row[0] = row[0] * 0.001\n",
    "            row[2] = tauidDict[row[1]][1]\n",
    "            row[4] = int(row[3])/1000000 #convert sq meters to sq km\n",
    "            cur.updateRow(row)\n",
    "        del(row)\n",
    "    del(cur)\n",
    "    print(f'Process complete')\n",
    "else:\n",
    "    print(f'Outlet points already created at {tauoutpath}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Merge NHD and Tau points together and export as CSV"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging all available catchment outlet points\n"
     ]
    }
   ],
   "source": [
    "# NHDPoints\n",
    "nhdoutletsname = 'AKSSF_NHDPlus_awcHuc12_outlet_cats_points'\n",
    "nhdoutletspath = os.path.join(outgdb, nhdoutletsname)\n",
    "\n",
    "# Taupoints\n",
    "tauoutname = 'AKSSF_TauDEM_awcHuc12_outlet_cats_points'\n",
    "tauoutpath = os.path.join(outgdb, tauoutname)\n",
    "\n",
    "# All points\n",
    "catpointsname = 'AKSSF_awcHuc12_outlet_cats_points'\n",
    "catpointspath = os.path.join(outgdb, catpointsname)\n",
    "\n",
    "# Create FieldMappings object to manage merge output fields\n",
    "out_fms = arcpy.FieldMappings()\n",
    "\n",
    "# Add all fields from both point fcs\n",
    "out_fms.addTable(nhdoutletspath)\n",
    "out_fms.addTable(tauoutpath)\n",
    "\n",
    "# Add input fields\n",
    "out_fm_dsdrain = arcpy.FieldMap()\n",
    "out_fm_dsdrain.addInputField(nhdoutletspath,'TotDASqKm')\n",
    "out_fm_dsdrain.addInputField(tauoutpath,'DSContAreaSqKM')\n",
    "\n",
    "# Set name of new output field \"Street_Name\"\n",
    "dsdrain = out_fm_dsdrain.outputField\n",
    "dsdrain.name = \"DsContAreaSqKm\"\n",
    "out_fm_dsdrain.outputField = dsdrain\n",
    "\n",
    "# add to field mappings\n",
    "out_fms.addFieldMap(out_fm_dsdrain)\n",
    "\n",
    "for field in out_fms.fields:\n",
    "    if field.name not in ['cat_ID_con', 'DsContAreaSqKm','dist_catch_coast_km', 'HUC12']:\n",
    "        out_fms.removeFieldMap(out_fms.findFieldMapIndex(field.name))\n",
    "\n",
    "#if not arcpy.Exists(catpointspath):\n",
    "addSourceInfo = \"ADD_SOURCE_INFO\"\n",
    "cats_outlets = arcpy.Merge_management([nhdoutletspath,tauoutpath],\n",
    "                                      catpointspath,\n",
    "                                      out_fms,\n",
    "                                      addSourceInfo)\n",
    "print(f'Merging all available catchment outlet points')\n",
    "#else:\n",
    "#    print(f'AKSSF AWC Catchment outlets already identified and exported to {catpointspath}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert to df and examine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJECTID\n",
      "Shape\n",
      "cat_ID_con\n",
      "dist_catch_coast_km\n",
      "HUC12\n",
      "DsContAreaSqKm\n",
      "MERGE_SRC\n"
     ]
    },
    {
     "data": {
      "text/plain": "                            dist_catch_coast_km DsContAreaSqKm         HUC12\ncat_ID_con                                                                  \nCook_Inlet_75004200000901             21.633384    36.96257508  190202020501\nCook_Inlet_75004200001724             17.599013   170.81217492  190202020503\nCook_Inlet_75004200001726              0.128861    570.1149751  190202020508\nCook_Inlet_75004200001493              0.147499         9.6406  190202020303\nCook_Inlet_75004200004105              0.137137    14.02302496  190202020102\n...                                         ...            ...           ...\nPrince_William_Sound_91741            17.543384      572.85728  190202010906\nPrince_William_Sound_91751            17.338805     574.793792  190202010905\nPrince_William_Sound_91921            12.177571     739.347904  190202010907\nPrince_William_Sound_92151             0.047172     920.540288  190202010908\nPrince_William_Sound_93261             0.020080       117.1584  190202011102\n\n[1018 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dist_catch_coast_km</th>\n      <th>DsContAreaSqKm</th>\n      <th>HUC12</th>\n    </tr>\n    <tr>\n      <th>cat_ID_con</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Cook_Inlet_75004200000901</th>\n      <td>21.633384</td>\n      <td>36.96257508</td>\n      <td>190202020501</td>\n    </tr>\n    <tr>\n      <th>Cook_Inlet_75004200001724</th>\n      <td>17.599013</td>\n      <td>170.81217492</td>\n      <td>190202020503</td>\n    </tr>\n    <tr>\n      <th>Cook_Inlet_75004200001726</th>\n      <td>0.128861</td>\n      <td>570.1149751</td>\n      <td>190202020508</td>\n    </tr>\n    <tr>\n      <th>Cook_Inlet_75004200001493</th>\n      <td>0.147499</td>\n      <td>9.6406</td>\n      <td>190202020303</td>\n    </tr>\n    <tr>\n      <th>Cook_Inlet_75004200004105</th>\n      <td>0.137137</td>\n      <td>14.02302496</td>\n      <td>190202020102</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>Prince_William_Sound_91741</th>\n      <td>17.543384</td>\n      <td>572.85728</td>\n      <td>190202010906</td>\n    </tr>\n    <tr>\n      <th>Prince_William_Sound_91751</th>\n      <td>17.338805</td>\n      <td>574.793792</td>\n      <td>190202010905</td>\n    </tr>\n    <tr>\n      <th>Prince_William_Sound_91921</th>\n      <td>12.177571</td>\n      <td>739.347904</td>\n      <td>190202010907</td>\n    </tr>\n    <tr>\n      <th>Prince_William_Sound_92151</th>\n      <td>0.047172</td>\n      <td>920.540288</td>\n      <td>190202010908</td>\n    </tr>\n    <tr>\n      <th>Prince_William_Sound_93261</th>\n      <td>0.020080</td>\n      <td>117.1584</td>\n      <td>190202011102</td>\n    </tr>\n  </tbody>\n</table>\n<p>1018 rows  3 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "# Make catchment points df\n",
    "cat_df = pd.DataFrame()\n",
    "cat_field_list = []\n",
    "\n",
    "for field in arcpy.ListFields(catpointspath):\n",
    "    print(field.name)\n",
    "    cat_field_list.append(field.name)\n",
    "cat_arr = arcpy.da.TableToNumPyArray(catpointspath, ['cat_ID_con','dist_catch_coast_km','DsContAreaSqKm','HUC12'])\n",
    "cat_df = pd.DataFrame(cat_arr)\n",
    "cat_df = cat_df.set_index('cat_ID_con')\n",
    "cat_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export csv of outlet points for NHDPlus regions\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV export complete\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#Export CSV to read into R\n",
    "catpts_outname = 'AKSSF_awcHuc12_dist_catch_coast_km.csv'\n",
    "outlets_csv = os.path.join(outdir,catpts_outname)\n",
    "if not arcpy.Exists(outlets_csv):\n",
    "    arcpy.da.NumPyArrayToTable(cat_arr,outlets_csv)\n",
    "    print('CSV export complete')\n",
    "else:\n",
    "    print(f'Csv of catchment outlet points already exported to {outlets_csv}')\n",
    "print('----------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 3\n",
    "### NHDPlus Watersheds\n",
    "Generate Watersheds\n",
    "* If watersheds have already been created there is no need to run this section again in order for subsequent process to run."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prince_William_Sound\n",
      "D:\\GIS\\AKSSF\\Prince_William_Sound\\Prince_William_Sound.gdb\\cats_merge Indexed\n",
      "OBJECTID\n",
      "Shape\n",
      "LINKNO\n",
      "DSLINKNO\n",
      "USLINKNO1\n",
      "USLINKNO2\n",
      "DSNODEID\n",
      "strmOrder\n",
      "Length\n",
      "Magnitude\n",
      "DSContArea\n",
      "strmDrop\n",
      "Slope\n",
      "StraightL\n",
      "USContArea\n",
      "WSNO\n",
      "DOUTEND\n",
      "DOUTSTART\n",
      "DOUTMID\n",
      "Shape_Length\n",
      "1. Starting watershed for HUC 3038 (111 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "2. Starting watershed for HUC 13227 (110 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "3. Starting watershed for HUC 17027 (109 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "4. Starting watershed for HUC 17697 (108 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "5. Starting watershed for HUC 18357 (107 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "6. Starting watershed for HUC 18547 (106 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "7. Starting watershed for HUC 18737 (105 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "8. Starting watershed for HUC 18747 (104 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "9. Starting watershed for HUC 22134 (103 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "10. Starting watershed for HUC 23854 (102 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "11. Starting watershed for HUC 25516 (101 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "12. Starting watershed for HUC 26246 (100 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "13. Starting watershed for HUC 26334 (99 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "14. Starting watershed for HUC 27856 (98 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "15. Starting watershed for HUC 28576 (97 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "16. Starting watershed for HUC 29954 (96 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "17. Starting watershed for HUC 30084 (95 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "18. Starting watershed for HUC 30114 (94 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "19. Starting watershed for HUC 30596 (93 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "20. Starting watershed for HUC 30774 (92 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "21. Starting watershed for HUC 30886 (91 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "22. Starting watershed for HUC 30934 (90 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "23. Starting watershed for HUC 31226 (89 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "24. Starting watershed for HUC 31916 (88 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "25. Starting watershed for HUC 32766 (87 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "26. Starting watershed for HUC 33106 (86 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "27. Starting watershed for HUC 33396 (85 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "28. Starting watershed for HUC 33616 (84 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "29. Starting watershed for HUC 34003 (83 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "30. Starting watershed for HUC 34455 (82 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "31. Starting watershed for HUC 37373 (81 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "32. Starting watershed for HUC 37815 (80 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "33. Starting watershed for HUC 38615 (79 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "34. Starting watershed for HUC 39065 (78 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "35. Starting watershed for HUC 39565 (77 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "36. Starting watershed for HUC 40615 (76 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "37. Starting watershed for HUC 40743 (75 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "38. Starting watershed for HUC 41123 (74 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "39. Starting watershed for HUC 41143 (73 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "40. Starting watershed for HUC 41515 (72 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "41. Starting watershed for HUC 41525 (71 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "42. Starting watershed for HUC 41963 (70 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "43. Starting watershed for HUC 41995 (69 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "44. Starting watershed for HUC 42005 (68 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "45. Starting watershed for HUC 42455 (67 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "46. Starting watershed for HUC 42595 (66 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "47. Starting watershed for HUC 43085 (65 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "48. Starting watershed for HUC 43125 (64 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "49. Starting watershed for HUC 43195 (63 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "50. Starting watershed for HUC 43255 (62 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "51. Starting watershed for HUC 43473 (61 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "52. Starting watershed for HUC 43513 (60 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:01)\n",
      "************************************************************\n",
      "53. Starting watershed for HUC 43535 (59 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "54. Starting watershed for HUC 43705 (58 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "55. Starting watershed for HUC 43793 (57 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "56. Starting watershed for HUC 43933 (56 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "57. Starting watershed for HUC 44113 (55 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "58. Starting watershed for HUC 44273 (54 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "59. Starting watershed for HUC 44503 (53 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "60. Starting watershed for HUC 44545 (52 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "61. Starting watershed for HUC 44595 (51 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "62. Starting watershed for HUC 44605 (50 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "63. Starting watershed for HUC 44633 (49 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "64. Starting watershed for HUC 44663 (48 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "65. Starting watershed for HUC 45473 (47 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "66. Starting watershed for HUC 45835 (46 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "67. Starting watershed for HUC 45863 (45 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "68. Starting watershed for HUC 45915 (44 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "69. Starting watershed for HUC 45925 (43 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "70. Starting watershed for HUC 46065 (42 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "71. Starting watershed for HUC 46103 (41 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "72. Starting watershed for HUC 46293 (40 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "73. Starting watershed for HUC 46483 (39 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:03)\n",
      "************************************************************\n",
      "74. Starting watershed for HUC 46513 (38 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:03)\n",
      "************************************************************\n",
      "75. Starting watershed for HUC 46613 (37 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "76. Starting watershed for HUC 46623 (36 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "77. Starting watershed for HUC 62232 (35 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "78. Starting watershed for HUC 64192 (34 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "79. Starting watershed for HUC 66912 (33 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "80. Starting watershed for HUC 67552 (32 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "81. Starting watershed for HUC 70762 (31 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "82. Starting watershed for HUC 71952 (30 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "83. Starting watershed for HUC 72402 (29 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "84. Starting watershed for HUC 72972 (28 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "85. Starting watershed for HUC 74042 (27 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "86. Starting watershed for HUC 74542 (26 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "87. Starting watershed for HUC 74602 (25 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "88. Starting watershed for HUC 75002 (24 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "89. Starting watershed for HUC 75422 (23 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "90. Starting watershed for HUC 75432 (22 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "91. Starting watershed for HUC 76622 (21 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "92. Starting watershed for HUC 76652 (20 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "93. Starting watershed for HUC 76692 (19 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "94. Starting watershed for HUC 76812 (18 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "95. Starting watershed for HUC 77292 (17 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "96. Starting watershed for HUC 77992 (16 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:03)\n",
      "************************************************************\n",
      "97. Starting watershed for HUC 78522 (15 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "98. Starting watershed for HUC 79531 (14 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "99. Starting watershed for HUC 79792 (13 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "100. Starting watershed for HUC 79972 (12 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "101. Starting watershed for HUC 80112 (11 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:05)\n",
      "************************************************************\n",
      "102. Starting watershed for HUC 80272 (10 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "103. Starting watershed for HUC 82471 (9 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "104. Starting watershed for HUC 84641 (8 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:03)\n",
      "************************************************************\n",
      "105. Starting watershed for HUC 86871 (7 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "106. Starting watershed for HUC 87851 (6 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "107. Starting watershed for HUC 89681 (5 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:03)\n",
      "************************************************************\n",
      "108. Starting watershed for HUC 91741 (4 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:04)\n",
      "************************************************************\n",
      "109. Starting watershed for HUC 91751 (3 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:04)\n",
      "************************************************************\n",
      "110. Starting watershed for HUC 91921 (2 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:04)\n",
      "************************************************************\n",
      "111. Starting watershed for HUC 92151 (1 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:04)\n",
      "************************************************************\n",
      "112. Starting watershed for HUC 93261 (0 remaining)\n",
      "Starting dissolve\n",
      "Elapsed time: (0:00:02)\n",
      "************************************************************\n",
      "A column was specified that does not exist.\n",
      "Process completed at 2022-02-09 21:08 (Elapsed time: 0:04:26)\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# NHDPLUS Watersheds\n",
    "\n",
    "import arcpy, time, datetime, os\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import arcpy, time, os, datetime, operator\n",
    "\n",
    "arcpy.env.workspace = data_dir\n",
    "regions = arcpy.ListWorkspaces()\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.qualifiedFieldNames = False\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "\n",
    "wtdDict = {}\n",
    "\n",
    "# Separate data by source type\n",
    "nhdplus_dat = ['Cook_Inlet','Copper_River']\n",
    "# Limit to Cook inlet for testing\n",
    "# regions = ['D:\\\\GIS\\\\AKSSF\\\\Copper_River','D:\\\\GIS\\\\AKSSF\\\\Cook_Inlet']\n",
    "regions = ['D:\\\\GIS\\\\AKSSF\\\\Prince_William_Sound']\n",
    "\n",
    "# Start timing function\n",
    "processStart = time.time()\n",
    "processStartdt = datetime.datetime.now()\n",
    "\n",
    "for region in regions:\n",
    "    reg_start = time.time()\n",
    "    roi = os.path.basename(region)\n",
    "    print(roi)\n",
    "    if roi in nhdplus_dat:\n",
    "        try:\n",
    "            wtdList = []\n",
    "            arcpy.env.workspace = region\n",
    "            gdb = arcpy.ListWorkspaces(workspace_type='FileGDB')\n",
    "            ingdb = gdb[0]\n",
    "            # set inputs\n",
    "            vaa = os.path.join(ingdb, \"vaa_merge\")\n",
    "            cats = os.path.join(ingdb, \"cats_merge\")\n",
    "            streams = os.path.join(ingdb, \"NHDFlowline_merge\")\n",
    "            outcats = os.path.join(ingdb, \"awc_huc12_catchment_outlets\")\n",
    "            # Create list of nhdplus ids for outlet catchments\n",
    "            idList = [int(row[0]) for row in arcpy.da.SearchCursor(outcats,'NHDPlusID')]\n",
    "            #Make test list of few small catchments\n",
    "            #idList = [75004400004166,75004400004344, 75004400010328]\n",
    "            # Get list of index names for cats merge and add index if not already created\n",
    "            index_names = [i.name for i in arcpy.ListIndexes(cats)]\n",
    "            print(index_names)\n",
    "            if 'NHDPlusID_index' not in index_names:\n",
    "                print (f'Creating index for {cats}')\n",
    "                arcpy.AddIndex_management(cats,'NHDPlusID','NHDPlusID_index')\n",
    "            else:\n",
    "                print(f'{cats} Indexed')\n",
    "\n",
    "            #watersheds feature dataset for storing fcs\n",
    "            fdatname = roi + '_Watersheds'\n",
    "            fdat = os.path.join(outgdb,fdatname)\n",
    "            if not arcpy.Exists(fdat):\n",
    "                arcpy.management.CreateFeatureDataset(outgdb, fdatname, sr)\n",
    "            else:\n",
    "                print(f'{fdat} exists for {roi}')\n",
    "\n",
    "            vaa_df1 = pd.DataFrame(arcpy.da.TableToNumPyArray(vaa, (\"NHDPlusID\", \"FromNode\", \"ToNode\", \"StartFlag\")))\n",
    "            stream_df = pd.DataFrame(arcpy.da.TableToNumPyArray(streams, (\"NHDPlusID\", \"FType\")))\n",
    "            dfs = [vaa_df1, stream_df]\n",
    "            vaa_df = reduce(lambda left,right: pd.merge(left,right,on='NHDPlusID',how=\"outer\"), dfs)\n",
    "            # remove pipelines\n",
    "            vaa_df = vaa_df[(vaa_df['FType'] != 428 )]\n",
    "            vaa_df\n",
    "\n",
    "            c=1\n",
    "            for id in idList:\n",
    "                iteration_start = time.time()\n",
    "                print(f'{c}. Starting watershed for HUC {str(id)} ({(len(idList) - c)} remaining)')\n",
    "                rec = [id]\n",
    "                up_ids = []\n",
    "                up_ids.append(rec)\n",
    "                rec_len = len(rec)\n",
    "                hws_sum = 0\n",
    "\n",
    "                while rec_len != hws_sum:\n",
    "                    fromnode = vaa_df.loc[vaa_df[\"NHDPlusID\"].isin(rec), \"FromNode\"]\n",
    "                    rec = vaa_df.loc[vaa_df[\"ToNode\"].isin(fromnode), \"NHDPlusID\"]\n",
    "                    rec_len = len(rec)\n",
    "                    rec_hws = vaa_df.loc[vaa_df[\"ToNode\"].isin(fromnode), \"StartFlag\"]\n",
    "                    hws_sum = sum(rec_hws)\n",
    "                    up_ids.append(rec)\n",
    "                #up_ids is a list with more than numbers, use extend to only keep numeric nhdplusids\n",
    "                newup_ids = []\n",
    "                for x in up_ids:\n",
    "                    newup_ids.extend(x)\n",
    "\n",
    "                tempLayer = \"catsLyr\"\n",
    "                expression = '\"NHDPlusID\" IN ({0})'.format(', '.join(map(str, newup_ids)) or 'NULL')\n",
    "                arcpy.MakeFeatureLayer_management(cats, tempLayer, where_clause=expression)\n",
    "                outdis = \"memory/wtd_\" + str(round(id))\n",
    "                outwtd = os.path.join(fdat,f'{roi}_wtd_{str(int(id))}')\n",
    "                dis = arcpy.Dissolve_management(tempLayer, outdis)\n",
    "                watershed = arcpy.EliminatePolygonPart_management(dis, outwtd,\"PERCENT\", \"0 SquareKilometers\", 90, \"CONTAINED_ONLY\")\n",
    "                wtdList.append(outwtd)\n",
    "                append_value(wtdDict,roi,outwtd)\n",
    "\n",
    "                # Stop iteration timer\n",
    "                iteration_stop = time.time()\n",
    "                iter_time = int (iteration_stop - iteration_start)\n",
    "                print(f'Elapsed time: ({datetime.timedelta(seconds=iter_time)})')\n",
    "                print(f'{\"*\"*60}')\n",
    "                c+=1\n",
    "\n",
    "            wtd_merge = arcpy.Merge_management(wtdList, os.path.join(ingdb,'awc_huc12_wtds_merge'),'','ADD_SOURCE_INFO')\n",
    "            arcpy.AddField_management(wtd_merge,'cat_ID_con','TEXT')\n",
    "            arcpy.AddField_management(wtd_merge,'cat_ID','DOUBLE')\n",
    "            arcpy.AddField_management(wtd_merge,'cat_ID_txt','TEXT')\n",
    "            arcpy.AddField_management(wtd_merge,'NHDPlusID','DOUBLE')\n",
    "            with arcpy.da.UpdateCursor(wtd_merge,['MERGE_SRC','NHDPlusID','cat_ID_con','cat_ID','cat_ID_txt']) as cur:\n",
    "                for row in cur:\n",
    "                    # Pull nhdplus id from merge source and calculate fields\n",
    "                    nhdplusid= int(row[0].split('_')[-1])\n",
    "                    row[1] = nhdplusid\n",
    "                    row[2] = roi + '_' + str(nhdplusid)\n",
    "                    row[3] = nhdplusid\n",
    "                    row[4] = str(nhdplusid)\n",
    "                    cur.updateRow(row)\n",
    "                del(row)\n",
    "            del(cur)\n",
    "            arcpy.CopyFeatures_management(wtd_merge,os.path.join(outgdb,f'{roi}_NhdAwcH12_wtds_merge' ))\n",
    "\n",
    "            # Stop iteration timer\n",
    "            reg_stop = time.time()\n",
    "            reg_time = int (reg_stop - reg_start)\n",
    "            print(f'{roi} Elapsed time: ({datetime.timedelta(seconds=reg_time)})')\n",
    "            print(f'{\"*\"*100}')\n",
    "\n",
    "        except:\n",
    "            e = sys.exc_info()[1]\n",
    "            print(e.args[0])\n",
    "            arcpy.AddError(e.args[0])\n",
    "\n",
    "    elif roi in tauDem_dat:\n",
    "        try:\n",
    "            reg_start = time.time()\n",
    "            wtdList = []\n",
    "            arcpy.env.workspace = region\n",
    "            gdb = arcpy.ListWorkspaces(workspace_type='FileGDB')\n",
    "            ingdb = gdb[0]\n",
    "            # set inputs\n",
    "            cats = os.path.join(ingdb, \"cats_merge\")\n",
    "            streams = os.path.join(ingdb, \"streams_merge\")\n",
    "            outcats = os.path.join(ingdb, \"awc_huc12_catchment_outlets\")\n",
    "            # Create list of nhdplus ids for outlet catchments\n",
    "            idList = [int(row[0]) for row in arcpy.da.SearchCursor(outcats,'gridcode')]\n",
    "            index_names = [i.name for i in arcpy.ListIndexes(cats)]\n",
    "            if 'catid_index' not in index_names:\n",
    "                print (f'Creating index for {cats}')\n",
    "                arcpy.AddIndex_management(cats, \"catID\", \"catid_index\")\n",
    "            else:\n",
    "                print(f'{cats} Indexed')\n",
    "            #watersheds feature dataset for storing fcs\n",
    "            fdatname = roi + '_Watersheds'\n",
    "            fdat = os.path.join(outgdb,fdatname)\n",
    "            if not arcpy.Exists(fdat):\n",
    "                arcpy.management.CreateFeatureDataset(outgdb, fdatname, sr)\n",
    "            else:\n",
    "                print(f'{fdat} exists for {roi}')\n",
    "            fields = arcpy.ListFields(streams)\n",
    "            str_df = pd.DataFrame(arcpy.da.FeatureClassToNumPyArray(streams, (\"LINKNO\", \"USLINKNO1\", \"USLINKNO2\")))\n",
    "            hws_codes = [-1]\n",
    "\n",
    "            # Generate watersheds\n",
    "            c=1\n",
    "            for id in idList:\n",
    "                iteration_start = time.time()\n",
    "                print(f'{c}. Starting watershed for HUC {str(id)} ({(len(idList) - c)} remaining)')\n",
    "                rec = [id]\n",
    "                up_ids = []\n",
    "                sum_rec = sum(rec)\n",
    "                while(sum_rec > 0):\n",
    "                    up_ids.append(rec)\n",
    "                    rec = str_df.loc[str_df[\"LINKNO\"].isin(rec), (\"USLINKNO1\", \"USLINKNO2\")]\n",
    "                    rec = pd.concat([rec['USLINKNO1'], rec['USLINKNO2']])\n",
    "                    sum_rec = sum(rec)\n",
    "                # up_ids is a list with more than numbers, use extend to only keep numeric nhdplusids\n",
    "                newup_ids = []\n",
    "                for x in up_ids:\n",
    "                    newup_ids.extend(x)\n",
    "\n",
    "                tempLayer = \"catsLyr\"\n",
    "                expression = '\"gridcode\" IN ({0})'.format(', '.join(map(str, newup_ids)) or 'NULL')\n",
    "                arcpy.MakeFeatureLayer_management(cats, tempLayer)\n",
    "                arcpy.management.SelectLayerByAttribute(tempLayer, \"NEW_SELECTION\", expression, None)\n",
    "                print(\"Starting dissolve\")\n",
    "                outdis = \"memory/wtd_\" + str(round(id))\n",
    "                outwtd = os.path.join(fdat,f'{roi}_wtd_{str(int(id))}')\n",
    "                dis = arcpy.Dissolve_management(tempLayer, outdis)\n",
    "                watershed = arcpy.EliminatePolygonPart_management(dis, outwtd,\"PERCENT\", \"0 SquareKilometers\", 90, \"CONTAINED_ONLY\")\n",
    "                wtdList.append(watershed)\n",
    "                append_value(wtdDict,roi,outwtd)\n",
    "\n",
    "                # Stop iteration timer\n",
    "                iteration_stop = time.time()\n",
    "                iter_time = int (iteration_stop - iteration_start)\n",
    "                print(f'Elapsed time: ({datetime.timedelta(seconds=iter_time)})')\n",
    "                print(f'{\"*\"*60}')\n",
    "                c+=1\n",
    "\n",
    "            wtd_merge = arcpy.Merge_management(wtdList, os.path.join(ingdb,'awc_huc12_wtds_merge'),'','ADD_SOURCE_INFO')\n",
    "            arcpy.AddField_management(wtd_merge,'cat_ID_con','TEXT')\n",
    "            arcpy.AddField_management(wtd_merge,'cat_ID','DOUBLE')\n",
    "            arcpy.AddField_management(wtd_merge,'cat_ID_txt','TEXT')\n",
    "            with arcpy.da.UpdateCursor(wtd_merge,['gridcode','cat_ID_con','cat_ID','cat_ID_txt']) as cur:\n",
    "                for row in cur:\n",
    "                    row[1] = roi + '_' + str(row[0])\n",
    "                    row[2] = int(row[0])\n",
    "                    row[4] = str(row[0])\n",
    "                    cur.updateRow(row)\n",
    "                del(row)\n",
    "            del(cur)\n",
    "            arcpy.CopyFeatures_management(wtd_merge,os.path.join(outgdb, f'{roi}_NhdAwcH12_wtds_merge' ))\n",
    "\n",
    "            # Stop iteration timer\n",
    "            reg_stop = time.time()\n",
    "            reg_time = int (reg_stop - reg_start)\n",
    "            print(f'{roi} Elapsed time: ({datetime.timedelta(seconds=reg_time)})')\n",
    "            print(f'{\"*\"*100}')\n",
    "\n",
    "        except:\n",
    "            e = sys.exc_info()[1]\n",
    "            print(e.args[0])\n",
    "            arcpy.AddError(e.args[0])\n",
    "    else:\n",
    "        print(f'{roi} not found - check inputs')\n",
    "        sys.exit(f'{roi} not found - check inputs')\n",
    "\n",
    "# End timing\n",
    "processEnd = time.time()\n",
    "processElapsed = int(processEnd - processStart)\n",
    "processSuccess_time = datetime.datetime.now()\n",
    "\n",
    "# Report success\n",
    "print(f'Process completed at {processSuccess_time.strftime(\"%Y-%m-%d %H:%M\")} '\n",
    "      f'(Elapsed time: {datetime.timedelta(seconds=processElapsed)})')\n",
    "print(f'{\"*\"*100}')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TROUBLESHOOTING BLOCK\n",
    "### Zonal statistics as table is failing with unknown error when run on watershed_merge and slope/elev rasters if using 'ALL' statistics.\n",
    "Try alternative methods.  Below is test chunk for iterating over a list of stats individually and join results back to a copy of the merged watershed table.\n",
    "* Cannot Run ZonalStatistics because tool does not process overlapping polygons as individual features whereas ZonalStatistics as table will\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "### TEST CHUNK###\n",
    "\n",
    "import os, arcpy,time, datetime\n",
    "from arcpy.sa import *\n",
    "arcpy.env.overwriteOutput = True\n",
    "#\n",
    "# testoutgdb = r\"C:\\\\Users\\\\dwmerrigan\\\\Documents\\\\GitHub\\\\ArcGIS_Default\\\\ArcGIS_Default.gdb\"\n",
    "# wtd_merge = r\"D:\\\\GIS\\\\AKSSF_awcHuc12_cv\\\\AKSSF_awcHuc12_cv.gdb\\\\Cook_Inlet_NhdAwcH12_wtds_merge\"\n",
    "# wtd_cur_fields = ['cat_ID_txt', 'cat_ID',\"cat_ID_con\"]\n",
    "# elev_rast = r\"D:\\\\GIS\\\\AKSSF\\\\Cook_Inlet\\\\elev.tif\"\n",
    "# zstats = ['MIN_MAX_MEAN','STD']\n",
    "# roi = 'Cook_Inlet'\n",
    "#\n",
    "# # Elevation variables\n",
    "# wtd_merge_elev_table_name = roi + \"_Watersheds_Merge_ElevZstats\"\n",
    "# wtd_merge_elev_table_path = os.path.join(testoutgdb, wtd_merge_elev_table_name)\n",
    "#\n",
    "# # list to store zonal stat tables\n",
    "# wtdelevstats =[]\n",
    "#\n",
    "# # Create field mappings\n",
    "# elev_fm = arcpy.FieldMap()\n",
    "# elev_fms = arcpy.FieldMappings()\n",
    "# for field in arcpy.ListFields(wtd_merge)[6:]:\n",
    "#     elev_fm = arcpy.FieldMap()\n",
    "#     elev_fm.addInputField(wtd_merge,field.name)\n",
    "#     elev_fm.mergeRule = 'First'\n",
    "#     # Set properties of the output name.\n",
    "#     f_name = elev_fm.outputField\n",
    "#     f_name.name = field.name\n",
    "#     f_name.aliasName = field.name\n",
    "#     elev_fm.outputField = f_name\n",
    "#     elev_fms.addFieldMap(elev_fm)\n",
    "#\n",
    "# # Make copy of watershed merge input as table to join stats fields\n",
    "# wtd_elev_metrics_table = arcpy.TableToTable_conversion(wtd_merge,\n",
    "#                                                        testoutgdb,\n",
    "#                                                        wtd_merge_elev_table_name,\n",
    "#                                                        '',\n",
    "#                                                        elev_fms,\n",
    "#                                                        )\n",
    "# # Add region identifier field for watershed tables                                                )\n",
    "# arcpy.AddField_management(wtd_elev_metrics_table,'region',field_type='TEXT')\n",
    "# # expression to calculate region field with roi name\n",
    "# exp =  '\"'+roi+'\"'\n",
    "# arcpy.CalculateField_management(wtd_elev_metrics_table,'region',exp)\n",
    "#\n",
    "# zstat_start = time.time()\n",
    "# for stat in zstats:\n",
    "#     outstattable = os.path.join(testoutgdb,f'{roi}_wtdElev{stat}')\n",
    "#     zstat_start1 = time.time()\n",
    "#     try:\n",
    "#         print (f'running {stat}')\n",
    "#         stat_table = ZonalStatisticsAsTable(in_zone_data = wtdmerge,\n",
    "#                                             zone_field = wtd_cur_fields[0],\n",
    "#                                             in_value_raster = elev_rast,\n",
    "#                                             out_table = outstattable,\n",
    "#                                             statistics_type=stat\n",
    "#                                             )\n",
    "#\n",
    "#         stat_fields = [f.name for f in arcpy.ListFields(stat_table)]\n",
    "#         arcpy.JoinField_management(wtd_elev_metrics_table,\n",
    "#                                wtd_cur_fields[0],\n",
    "#                                stat_table,\n",
    "#                                wtd_cur_fields[0],\n",
    "#                                stat_fields[5:]\n",
    "#                                )\n",
    "#\n",
    "#         # Report time\n",
    "#         zstat_stop1 = time.time()\n",
    "#         zstat_time1 = int (zstat_stop1 - zstat_start1)\n",
    "#         print(f'Watershed elev Zonal Stats for {stat} Elapsed time: ({datetime.timedelta(seconds=zstat_time1)})')\n",
    "#         print(f'{\"*\"*100}')\n",
    "#     except:\n",
    "#         e = sys.exc_info()[1]\n",
    "#         print(e.args[0])\n",
    "#         arcpy.AddError(e.args[0])\n",
    "# # Report time\n",
    "# zstat_stop = time.time()\n",
    "# zstat_time = int (zstat_stop - zstat_start)\n",
    "# print(f'Watershed elev Zonal Stats for {roi} Elapsed time: ({datetime.timedelta(seconds=zstat_time)})')\n",
    "# print(f'{\"*\"*100}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 4\n",
    "Calculate Covariates\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prince_William_Sound in ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound'] TauDEM list, using cat_fields ['cat_ID_txt', 'gridcode', 'cat_ID_con'] and watershed fields ['cat_ID_txt', 'cat_ID', 'cat_ID_con']\n",
      "****************************************************************************************************\n",
      "Adding cat_ID_txt field to catchment dataset D:\\GIS\\AKSSF\\Prince_William_Sound\\Prince_William_Sound.gdb\\awc_huc12_catchment_outlets\n",
      "****************************************************************************************************\n",
      "Adding cat_ID_con field to catchment dataset D:\\GIS\\AKSSF\\Prince_William_Sound\\Prince_William_Sound.gdb\\awc_huc12_catchment_outlets\n",
      "****************************************************************************************************\n",
      "Merged watershed dataset awc_huc12_wtds_merge found\n",
      "****************************************************************************************************\n",
      "cat_ID_txt field already in dataset\n",
      "****************************************************************************************************\n",
      "cat_ID_con field already in dataset D:\\GIS\\AKSSF\\Prince_William_Sound\\Prince_William_Sound.gdb\\awc_huc12_wtds_merge\n",
      "****************************************************************************************************\n",
      "Calculating topographic metrics for catchments & watersheds of interest in Prince_William_Sound region\n",
      "----------\n",
      "Geodatabase: D:\\GIS\\AKSSF_awcHuc12_cv\\AKSSF_awcHuc12_cv.gdb\n",
      "----------\n",
      "Watershed Merge: D:\\GIS\\AKSSF\\Prince_William_Sound\\Prince_William_Sound.gdb\\awc_huc12_wtds_merge\n",
      "  Projection NAD_1983_Alaska_Albers\n",
      "----------\n",
      "HUC12 Catchment Outlets: D:\\GIS\\AKSSF\\Prince_William_Sound\\Prince_William_Sound.gdb\\awc_huc12_catchment_outlets\n",
      "  Projection NAD_1983_Alaska_Albers\n",
      "----------\n",
      "Elevation Raster: D:\\GIS\\AKSSF\\Prince_William_Sound\\elev.tif\n",
      "  Projection: NAD_1983_Alaska_Albers\n",
      "----------\n",
      "North Aspect Raster: D:\\GIS\\AKSSF\\Prince_William_Sound\\north.tif\n",
      "  Projection: NAD_1983_Alaska_Albers\n",
      "----------\n",
      "Wetlands Raster: D:\\GIS\\AKSSF\\Prince_William_Sound\\wetlands.tif\n",
      "  Projection NAD_1983_Alaska_Albers\n",
      "----------\n",
      "Slope Raster: D:\\GIS\\AKSSF\\Prince_William_Sound\\slope.tif\n",
      "  Projection NAD_1983_Alaska_Albers\n",
      "----------\n",
      "Lakes Ponds fc: D:\\Basedata\\AKSSF_Basedata\\AKSSF_Basedata.gdb\\AKSSF_NHD_LakesPonds_alb\n",
      "  Projection NAD_1983_Alaska_Albers\n",
      "----------\n",
      "Glaciers fc: D:\\GIS\\AKSSF\\Prince_William_Sound\\Prince_William_Sound.gdb\\glaciers \n",
      "  Projection NAD_1983_Alaska_Albers\n",
      "----------\n",
      "112 Watersheds to process\n",
      "----------\n",
      "Catchment intersect D:\\GIS\\AKSSF\\Prince_William_Sound\\Prince_William_Sound.gdb\\awc_huc12_catchment_outlets selected\n",
      "----------\n",
      "Begin Slope zonal statistics min/mean/max std dev for watersheds and catchments in Prince_William_Sound region\n",
      "Calculating Prince_William_Sound watershed slope zonal stats...\n",
      "running MIN_MAX_MEAN\n",
      "ERRFLAG!!! = ERROR 999999: Something unexpected caused the tool to fail. Contact Esri Technical Support (http://esriurl.com/support) to Report a Bug, and refer to the error help for potential solutions or workarounds.\n",
      "Failed to execute (ZonalStatisticsAsTable).\n",
      "\n",
      "All Covariates for Prince_William_Sound completed.\n",
      "Elapsed time: (0:00:22)\n",
      "****************************************************************************************************\n",
      "Process completed at 2022-02-09 21:19 (Elapsed time: 0:00:22)\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from arcpy.sa import *\n",
    "arcpy.env.workspace = data_dir\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338) #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "regions  = arcpy.ListWorkspaces(workspace_type=\"Folder\")\n",
    "\n",
    "# Lists for variables not needed at present time\n",
    "#cat_asp_ztables = []\n",
    "#wtd_asp_ztables = []\n",
    "#cat_pernorth_taba_tables=[]\n",
    "\n",
    "# Lists to store output tables\n",
    "wtd_pernorth_taba_tables=[]\n",
    "wtd_lp_tabint_tables = []\n",
    "wtd_glac_tabint_tables = []\n",
    "wtd_wet_taba_tables = []\n",
    "cat_elev_ztables = []\n",
    "wtd_elev_ztables = []\n",
    "cat_slope_ztables = []\n",
    "wtd_slope_ztables = []\n",
    "lcld_Ztables = []\n",
    "\n",
    "# Clear lists\n",
    "cat_cur_fields = []\n",
    "wtd_cur_fields = []\n",
    "\n",
    "# Start timing function\n",
    "processStart = time.time()\n",
    "processStartdt = datetime.datetime.now()\n",
    "\n",
    "# Split data by type\n",
    "nhdplus_dat = ['Cook_Inlet','Copper_River']\n",
    "tauDem_dat = ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound']\n",
    "\n",
    "#Limit to ci for testing\n",
    "#regions = ['D:\\\\GIS\\\\AKSSF\\\\Cook_Inlet','D:\\\\GIS\\\\AKSSF\\\\Copper_River']\n",
    "regions = ['D:\\\\GIS\\\\AKSSF\\\\Prince_William_Sound']\n",
    "\n",
    "for region in regions:\n",
    "    roi = os.path.basename(region)\n",
    "    # expression to calculate region field with roi name\n",
    "    exp =  '\"'+roi+'\"'\n",
    "    if roi in nhdplus_dat:\n",
    "        lakes_fc = nhd_lakes_fc\n",
    "        # Fields for update cursor\n",
    "        cat_cur_fields = ['cat_ID_txt', 'NHDPlusID',\"cat_ID_con\"]\n",
    "        wtd_cur_fields = ['cat_ID_txt', 'cat_ID',\"cat_ID_con\"]\n",
    "        print (f'{roi} in {nhdplus_dat} AKSSF list, using cat_fields {cat_cur_fields} and watershed fields {wtd_cur_fields}')\n",
    "        print(f'{\"*\"*100}')\n",
    "    # Set data and variables unique to regions with TauDEM Data\n",
    "    elif roi in tauDem_dat:\n",
    "        lakes_fc = tau_lakes_fc\n",
    "        # Fields for update cursor\n",
    "        if roi == 'Bristol_Bay':\n",
    "            cat_cur_fields = ['cat_ID_txt', 'catID',\"cat_ID_con\"]\n",
    "            wtd_cur_fields = ['cat_ID_txt', 'cat_ID',\"cat_ID_con\"]\n",
    "        else:\n",
    "            cat_cur_fields = ['cat_ID_txt', 'gridcode',\"cat_ID_con\"]\n",
    "            wtd_cur_fields = ['cat_ID_txt', 'cat_ID',\"cat_ID_con\"]\n",
    "        print (f'{roi} in {tauDem_dat} TauDEM list, using cat_fields {cat_cur_fields} and watershed fields {wtd_cur_fields}')\n",
    "        print(f'{\"*\"*100}')\n",
    "    # Start iter timing function\n",
    "    iteration_start = time.time()\n",
    "    # Set workspace to region folder\n",
    "    arcpy.env.workspace = region\n",
    "    walk = arcpy.da.Walk(region, datatype = ['FeatureClass','RasterDataset'])\n",
    "    for dirpath, dirnames, filenames in walk:\n",
    "        for filename in filenames:\n",
    "            # Set merged watersheds dataset\n",
    "            if 'awc_huc12_wtds_merge'== filename:\n",
    "                wtdpath = os.path.join(dirpath,filename)\n",
    "                wtdname = roi +'_'+ filename\n",
    "                # Make local copy projected in AKAlbers\n",
    "                wtd_merge = os.path.join(dirpath, filename)\n",
    "                print(f'Merged watershed dataset {filename} found')\n",
    "                print(f'{\"*\"*100}')\n",
    "                wtdfieldnames = []\n",
    "                wtdlstFields = arcpy.ListFields(wtd_merge)\n",
    "                for field in wtdlstFields:\n",
    "                    wtdfieldnames.append(field.name)\n",
    "                if str(wtd_cur_fields[0]) in wtdfieldnames:\n",
    "                    print (f'{wtd_cur_fields[0]} field already in dataset')\n",
    "                    print(f'{\"*\"*100}')\n",
    "                else:\n",
    "                    print (f'Adding {wtd_cur_fields[0]} field to watershed dataset {wtd_merge}')\n",
    "                    print(f'{\"*\"*100}')\n",
    "                    # add cat_ID_txt field and concat cat_ID + region\n",
    "                    arcpy.AddField_management(wtd_merge, str(wtd_cur_fields[0]),field_type='TEXT')\n",
    "                    # populate cat_ID_txt\n",
    "                    with arcpy.da.UpdateCursor(wtd_merge, wtd_cur_fields[0:2]) as cur:\n",
    "                        for row in cur:\n",
    "                            strval = str(row[1])\n",
    "                            row[0] = strval.replace('.0',\"\")\n",
    "                            # Update rows\n",
    "                            cur.updateRow(row)\n",
    "                        del(row)\n",
    "                    del(cur)\n",
    "                if str(wtd_cur_fields[2]) in wtdfieldnames:\n",
    "                    print (f'{wtd_cur_fields[2]} field already in dataset {wtd_merge}')\n",
    "                    print(f'{\"*\"*100}')\n",
    "                else:\n",
    "                    print (f'Adding {wtd_cur_fields[2]} field to watershed dataset {wtd_merge}')\n",
    "                    print(f'{\"*\"*100}')\n",
    "                    # add cat_ID_con field and concat cat_ID + region\n",
    "                    arcpy.AddField_management(wtd_merge, str(wtd_cur_fields[2]),field_type='TEXT')\n",
    "                    # populate cat_ID_txt\n",
    "                    with arcpy.da.UpdateCursor(wtd_merge, wtd_cur_fields) as cur:\n",
    "                        for row in cur:\n",
    "                            strval = str(row[1])\n",
    "                            row[2] = str(roi) +'_'+ strval.replace(\".0\",\"\")\n",
    "                            # Update rows\n",
    "                            cur.updateRow(row)\n",
    "                        del(row)\n",
    "                    del(cur)\n",
    "\n",
    "            # Select glaciers fc\n",
    "            elif 'glaciers' == filename:\n",
    "                # Make local copy projected in AKAlbers\n",
    "                glacpath = os.path.join(dirpath, filename)\n",
    "                glacname = roi+'_'+filename\n",
    "                glac_fc = glacpath\n",
    "\n",
    "            # Select elevation raster\n",
    "            elif 'elev.tif' == filename:\n",
    "                elev_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "            # # Select aspect raster\n",
    "            # elif 'aspect' in filename:\n",
    "            #     asp_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "            # Select north raster\n",
    "            elif 'north.tif' == filename:\n",
    "                nor_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "            # Select slope raster\n",
    "            elif 'slope.tif' == filename:\n",
    "                slope_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "            # Select wetland raster\n",
    "            elif 'wetlands.tif' == filename:\n",
    "                wet_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "            # Select catch_int fc (catchments of interest for region) and make a copy\n",
    "            elif 'awc_huc12_catchment_outlets' == filename:\n",
    "                # Make local copy projected in AKAlbers\n",
    "                catspath = os.path.join(dirpath,filename)\n",
    "                catsname = roi +\"_\"+filename\n",
    "                cats = catspath\n",
    "                catlstfields = arcpy.ListFields(cats)\n",
    "                catfieldnames = []\n",
    "                for field in catlstfields:\n",
    "                    catfieldnames.append(field.name)\n",
    "                if str(cat_cur_fields[0]) in catfieldnames:\n",
    "                    print (f'{cat_cur_fields[0]} field already in dataset {cats}')\n",
    "                    print(f'{\"*\"*100}')\n",
    "                else:\n",
    "                    print (f'Adding {cat_cur_fields[0]} field to catchment dataset {cats}')\n",
    "                    print(f'{\"*\"*100}')\n",
    "                    # add cat_ID_txt field\n",
    "                    arcpy.AddField_management(cats, str(cat_cur_fields[0]), field_type='TEXT')\n",
    "                    # populate cat_ID_txt\n",
    "                    with arcpy.da.UpdateCursor(cats, cat_cur_fields[0:2]) as cur:\n",
    "                        for row in cur:\n",
    "                            strval = str(row[1])\n",
    "                            row[0] = strval.replace('.0',\"\")\n",
    "                            # Update rows\n",
    "                            cur.updateRow(row)\n",
    "                        del(row)\n",
    "                    del(cur)\n",
    "                if str(cat_cur_fields[2]) in catfieldnames:\n",
    "                    print (f'{cat_cur_fields[2]} field already in dataset {cats}')\n",
    "                    print(f'{\"*\"*100}')\n",
    "                else:\n",
    "                    print (f'Adding {cat_cur_fields[2]} field to catchment dataset {cats}')\n",
    "                    print(f'{\"*\"*100}')\n",
    "                    # add cat_ID_txt field & cat_ID + region concat field\n",
    "                    arcpy.AddField_management(cats,str(cat_cur_fields[2]),field_type='TEXT')\n",
    "                    # populate cat_ID_con\n",
    "                    with arcpy.da.UpdateCursor(cats, cat_cur_fields) as cur:\n",
    "                        for row in cur:\n",
    "                            strval = str(row[1])\n",
    "                            row[2] = str(roi) +'_'+ strval.replace('.0',\"\")\n",
    "                            # Update rows\n",
    "                            cur.updateRow(row)\n",
    "                        del(row)\n",
    "                    del(cur)\n",
    "\n",
    "    print (f'Calculating topographic metrics for catchments & watersheds of interest in {roi} region')\n",
    "    print ('----------')\n",
    "    print(f'Geodatabase: {outgdb}')\n",
    "    print ('----------')\n",
    "    print (f'Watershed Merge: {wtd_merge}')\n",
    "    print (f'  Projection {arcpy.Describe(wtd_merge).spatialReference.name}')\n",
    "    print ('----------')\n",
    "    print (f'HUC12 Catchment Outlets: {cats}')\n",
    "    print (f'  Projection {arcpy.Describe(cats).spatialReference.name}')\n",
    "    print ('----------')\n",
    "    print (f'Elevation Raster: {elev_rast}')\n",
    "    print (f'  Projection: {arcpy.Describe(elev_rast).spatialReference.name}')\n",
    "    print ('----------')\n",
    "    print (f'North Aspect Raster: {nor_rast}')\n",
    "    print (f'  Projection: {arcpy.Describe(nor_rast).spatialReference.name}')\n",
    "    print ('----------')\n",
    "    print (f'Wetlands Raster: {wet_rast}')\n",
    "    print (f'  Projection {arcpy.Describe(wet_rast).spatialReference.name}')\n",
    "    print ('----------')\n",
    "    print (f'Slope Raster: {slope_rast}')\n",
    "    print (f'  Projection {arcpy.Describe(slope_rast).spatialReference.name}')\n",
    "    print ('----------')\n",
    "    print (f'Lakes Ponds fc: {lakes_fc}')\n",
    "    print (f'  Projection {arcpy.Describe(lakes_fc).spatialReference.name}')\n",
    "    print ('----------')\n",
    "    print (f'Glaciers fc: {glac_fc} ')\n",
    "    print (f'  Projection {arcpy.Describe(glac_fc).spatialReference.name}')\n",
    "    print ('----------')\n",
    "    print (f'{arcpy.GetCount_management(wtd_merge)} Watersheds to process')\n",
    "    print ('----------')\n",
    "    print (f'Catchment intersect {cats} selected')\n",
    "    print ('----------')\n",
    "\n",
    "    # # Aspect variables\n",
    "    # wtd_merge_asp_table_name = roi + \"_NhdAwcH12_wtd_mer_AspectZstats\"\n",
    "    # wtd_merge_asp_table_path = os.path.join(outgdb, wtd_merge_asp_table_name)\n",
    "    # cat_asp_table_name = roi + \"_NhdAwcH12_cats_AspectZstats\"\n",
    "    # cat_asp_table_path = os.path.join(outgdb, cat_asp_table_name)\n",
    "\n",
    "    # Percent North variables\n",
    "    wtd_merge_pernorth_table_name = roi + \"_NhdAwcH12_wtd_mer_PerNorth\"\n",
    "    wtd_merge_pernorth_table_path = os.path.join(outgdb, wtd_merge_pernorth_table_name)\n",
    "    # cat_pernorth_table_name = roi + \"_NhdAwcH12_cats_PercentNorth\"\n",
    "    # cat_pernorth_table_path = os.path.join(outgdb, cat_pernorth_table_name)\n",
    "\n",
    "    # Elevation variables\n",
    "    wtd_merge_elev_table_name = roi + \"_NhdAwcH12_wtd_mer_ElevZstats\"\n",
    "    wtd_merge_elev_table_path = os.path.join(outgdb, wtd_merge_elev_table_name)\n",
    "    cat_elev_table_name = roi + \"_NhdAwcH12_cats_ElevZstats\"\n",
    "    cat_elev_table_path = os.path.join(outgdb, cat_elev_table_name)\n",
    "\n",
    "    # Slope variables\n",
    "    wtd_merge_slope_table_name = roi + \"_NhdAwcH12_wtd_mer_SlopeZstats\"\n",
    "    wtd_merge_slope_table_path = os.path.join(outgdb, wtd_merge_slope_table_name)\n",
    "    cat_slope_table_name = roi + \"_NhdAwcH12_cats_SlopeZstats\"\n",
    "    cat_slope_table_path = os.path.join(outgdb, cat_slope_table_name)\n",
    "\n",
    "    # Lakes Ponds variables\n",
    "    wtd_merge_lp_table_name = roi + \"_NhdAwcH12_wtd_mer_PerLakes\"\n",
    "    wtd_merge_lp_table_path = os.path.join(outgdb, wtd_merge_lp_table_name)\n",
    "    cat_lp_table_name = roi + \"_NhdAwcH12_cats_PerLakes\"\n",
    "    cat_lp_path = os.path.join(outgdb, cat_lp_table_name)\n",
    "\n",
    "    # Wetlands variables\n",
    "    wtd_merge_wetlands_table_name = roi + \"_NhdAwcH12_wtd_mer_PerWet\"\n",
    "    wtd_merge_wetlands_table_path = os.path.join(outgdb, wtd_merge_wetlands_table_name)\n",
    "    cat_wetlands_table_name = roi + \"NhdAwcH12_cats_PerWet\"\n",
    "    cat_wetlands_table_path = os.path.join(outgdb, cat_wetlands_table_name)\n",
    "\n",
    "    # Glaciers\n",
    "    wtd_merge_glac_table_name = roi + \"_NhdAwcH12_wtd_mer_PerGlac\"\n",
    "    wtd_merge_glac_table_path = os.path.join(outgdb, wtd_merge_glac_table_name)\n",
    "    cat_glac_table_name = roi + \"_NhdAwcH12_cats_Glaciers\"\n",
    "    cat_glac_table_path = os.path.join(outgdb, cat_glac_table_name)\n",
    "\n",
    "    try: # Zonal Stats section\n",
    "        print(f'Begin Slope zonal statistics min/mean/max std dev for watersheds and catchments in {roi}'\n",
    "              f' region')\n",
    "        # Statistics to run for watersheds - 'ALL' is not an option at this time as tool will fail with unknown error\n",
    "        zstats = ['MIN_MAX_MEAN','STD']\n",
    "        # Begin Zonal Stats\n",
    "        zstat_start = time.time()\n",
    "        zstat_start1 = time.time()\n",
    "\n",
    "        # Watershed slope Zonal Statistics\n",
    "        print(f'Calculating {roi} watershed slope zonal stats...')\n",
    "        arcpy.env.snapRaster = slope_rast\n",
    "        arcpy.env.cellSize = slope_rast\n",
    "\n",
    "        # Create field mappings\n",
    "        slope_fm = arcpy.FieldMap()\n",
    "        slope_fms = arcpy.FieldMappings()\n",
    "        for field in arcpy.ListFields(wtd_merge)[6:]:\n",
    "            slope_fm = arcpy.FieldMap()\n",
    "            slope_fm.addInputField(wtd_merge,field.name)\n",
    "            slope_fm.mergeRule = 'First'\n",
    "            # Set properties of the output name.\n",
    "            f_name = slope_fm.outputField\n",
    "            f_name.name = field.name\n",
    "            f_name.aliasName = field.name\n",
    "            slope_fm.outputField = f_name\n",
    "            slope_fms.addFieldMap(slope_fm)\n",
    "\n",
    "        # Make copy of watershed merge input as table to join stats fields\n",
    "        wtd_slope_metrics_table = arcpy.TableToTable_conversion(wtd_merge,\n",
    "                                                               outgdb,\n",
    "                                                               wtd_merge_slope_table_name,\n",
    "                                                               '',\n",
    "                                                               slope_fms,\n",
    "                                                               )\n",
    "        # Add region identifier field for watershed tables                                                )\n",
    "        arcpy.AddField_management(wtd_slope_metrics_table,'region',field_type='TEXT')\n",
    "        arcpy.CalculateField_management(wtd_slope_metrics_table,'region',exp)\n",
    "\n",
    "        for stat in zstats:\n",
    "            outstattable = os.path.join(outgdb,f'{roi}_wtdSlope_{stat}')\n",
    "            zstat_start1 = time.time()\n",
    "            print (f'running {stat}')\n",
    "            stat_table = ZonalStatisticsAsTable(in_zone_data = wtd_merge,\n",
    "                                                zone_field = wtd_cur_fields[0],\n",
    "                                                in_value_raster = slope_rast,\n",
    "                                                out_table = outstattable,\n",
    "                                                statistics_type=stat\n",
    "                                                )\n",
    "\n",
    "            stat_fields = [f.name for f in arcpy.ListFields(stat_table)]\n",
    "            arcpy.JoinField_management(wtd_slope_metrics_table,\n",
    "                                   wtd_cur_fields[0],\n",
    "                                   stat_table,\n",
    "                                   wtd_cur_fields[0],\n",
    "                                   stat_fields[5:] # Keep only stat field/s\n",
    "                                   )\n",
    "\n",
    "            # Report time\n",
    "            zstat_stop1 = time.time()\n",
    "            zstat_time1 = int (zstat_stop1 - zstat_start1)\n",
    "            print(f'Watershed Slope Zonal {stat} for {roi} complete.\\nElapsed time: ({datetime.timedelta(seconds=zstat_time1)})')\n",
    "            print(f'{\"*\"*100}')\n",
    "\n",
    "        # Append watershed slope table to list\n",
    "        wtd_slope_ztables.append(wtd_slope_metrics_table)\n",
    "\n",
    "\n",
    "        # Elevation Zonal statistics  for watersheds\n",
    "        print(f'Begin Elevation zonal statistics min/mean/max std dev for watersheds and catchments in {roi}'\n",
    "              f' region')\n",
    "        zstat_start2 = time.time()\n",
    "        arcpy.env.snapRaster = elev_rast\n",
    "        arcpy.env.cellSize = elev_rast\n",
    "\n",
    "        # Create field mappings\n",
    "        elev_fm = arcpy.FieldMap()\n",
    "        elev_fms = arcpy.FieldMappings()\n",
    "        for field in arcpy.ListFields(wtd_merge)[6:]:\n",
    "            elev_fm = arcpy.FieldMap()\n",
    "            elev_fm.addInputField(wtd_merge,field.name)\n",
    "            elev_fm.mergeRule = 'First'\n",
    "            # Set properties of the output name.\n",
    "            f_name = elev_fm.outputField\n",
    "            f_name.name = field.name\n",
    "            f_name.aliasName = field.name\n",
    "            elev_fm.outputField = f_name\n",
    "            elev_fms.addFieldMap(elev_fm)\n",
    "\n",
    "        # Make copy of watershed merge input as table to join stats fields\n",
    "        wtd_elev_metrics_table = arcpy.TableToTable_conversion(wtd_merge,\n",
    "                                                               outgdb,\n",
    "                                                               wtd_merge_elev_table_name,\n",
    "                                                               '',\n",
    "                                                               elev_fms,\n",
    "                                                               )\n",
    "        # Add region identifier field for watershed tables                                                )\n",
    "        arcpy.AddField_management(wtd_elev_metrics_table,'region',field_type='TEXT')\n",
    "        arcpy.CalculateField_management(wtd_elev_metrics_table,'region',exp)\n",
    "\n",
    "        for stat in zstats:\n",
    "            outstattable = os.path.join(outgdb,f'{roi}_wtdElev_{stat}')\n",
    "            zstat_start1 = time.time()\n",
    "            print (f'running {stat}')\n",
    "            stat_table = ZonalStatisticsAsTable(in_zone_data = wtd_merge,\n",
    "                                                zone_field = wtd_cur_fields[0],\n",
    "                                                in_value_raster = elev_rast,\n",
    "                                                out_table = outstattable,\n",
    "                                                statistics_type=stat\n",
    "                                                )\n",
    "\n",
    "            stat_fields = [f.name for f in arcpy.ListFields(stat_table)]\n",
    "            arcpy.JoinField_management(wtd_elev_metrics_table,\n",
    "                                   wtd_cur_fields[0],\n",
    "                                   stat_table,\n",
    "                                   wtd_cur_fields[0],\n",
    "                                   stat_fields[5:] # Keep only stat field/s\n",
    "                                   )\n",
    "\n",
    "            # Report time\n",
    "            zstat_stop2 = time.time()\n",
    "            zstat_time2 = int (zstat_stop2 - zstat_start2)\n",
    "            print(f'Watershed Elevation Zonal {stat} for {roi} complete.\\nElapsed time: ({datetime.timedelta(seconds=zstat_time2)})')\n",
    "            print(f'{\"*\"*100}')\n",
    "        # Append watershed elev table to list\n",
    "        wtd_elev_ztables.append(wtd_elev_metrics_table)\n",
    "\n",
    "\n",
    "        # Elevation zonal statistics for catchments\n",
    "        print(f'Calculating {roi} catchment elevation zonal stats...')\n",
    "        zstat_start3 = time.time()\n",
    "        arcpy.env.snapRaster = elev_rast\n",
    "        arcpy.env.cellSize = elev_rast\n",
    "        cat_elev_metrics_table = ZonalStatisticsAsTable(in_zone_data = cats ,\n",
    "                                                        zone_field = cat_cur_fields[0],\n",
    "                                                        in_value_raster = elev_rast,\n",
    "                                                        out_table = cat_elev_table_path,\n",
    "                                                        statistics_type='ALL'\n",
    "                                                        )\n",
    "        # Add region identifier field for catchment table\n",
    "        arcpy.AddField_management(cat_elev_metrics_table,'region',field_type='TEXT')\n",
    "        # Add cat_ID_Con field\n",
    "        arcpy.AddField_management(cat_elev_metrics_table,'cat_ID_con',field_type='TEXT')\n",
    "\n",
    "        # Update fields\n",
    "        with arcpy.da.UpdateCursor(cat_elev_metrics_table,['region','cat_ID_txt','cat_ID_con']) as cur:\n",
    "            for row in cur:\n",
    "                row[0] = roi\n",
    "                strval = str(row[1])\n",
    "                row[2] = roi+\"_\"+strval.replace(\".0\",\"\")\n",
    "                # Update\n",
    "                cur.updateRow(row)\n",
    "            del(row)\n",
    "        del(cur)\n",
    "        # Append catchment elev table to list\n",
    "        cat_elev_ztables.append(cat_elev_metrics_table)\n",
    "        # Report time\n",
    "        zstat_stop3 = time.time()\n",
    "        zstat_time3 = int (zstat_stop3 - zstat_start3)\n",
    "        print(f'Elevation Zonal Stats for {roi} catchments complete.\\nElapsed time: ({datetime.timedelta(seconds=zstat_time3)})')\n",
    "        print(f'{\"*\"*100}')\n",
    "\n",
    "        # Slope zonal statistics for catchments\n",
    "        zstat_start4 = time.time()\n",
    "        print(f'Calculating {roi} catchment slope zonal stats...')\n",
    "        arcpy.env.snapRaster = slope_rast\n",
    "        arcpy.env.cellSize = slope_rast\n",
    "        cat_slope_metrics_table = ZonalStatisticsAsTable(in_zone_data = cats ,\n",
    "                                                        zone_field = cat_cur_fields[0],\n",
    "                                                        in_value_raster = slope_rast,\n",
    "                                                        out_table = cat_slope_table_path,\n",
    "                                                        statistics_type='ALL'\n",
    "                                                        )\n",
    "        # Add region identifier field for catchment table\n",
    "        arcpy.AddField_management(cat_slope_metrics_table,'region',field_type='TEXT')\n",
    "        # Add cat_ID_Con field\n",
    "        arcpy.AddField_management(cat_slope_metrics_table,'cat_ID_con',field_type='TEXT')\n",
    "\n",
    "        # Update region field\n",
    "        with arcpy.da.UpdateCursor(cat_slope_metrics_table,['region','cat_ID_txt','cat_ID_con']) as cur:\n",
    "            for row in cur:\n",
    "                row[0] = roi\n",
    "                strval =str(row[1])\n",
    "                row[2] = roi+\"_\"+strval.replace(\".0\",\"\")\n",
    "                # Update\n",
    "                cur.updateRow(row)\n",
    "            del(row)\n",
    "        del(cur)\n",
    "        # Append catchment slope table to list\n",
    "        cat_slope_ztables.append(cat_slope_metrics_table)\n",
    "        # Report time\n",
    "        zstat_stop4 = time.time()\n",
    "        zstat_time4 = int (zstat_stop4 - zstat_start4)\n",
    "        print(f'Slope Zonal Stats for {roi} catchments complete.\\nElapsed time: ({datetime.timedelta(seconds=zstat_time4)})')\n",
    "        print(f'{\"*\"*100}')\n",
    "\n",
    "\n",
    "        # # Aspect Zonal statistics  for watersheds\n",
    "        # print(f'Calculating {roi} watershed aspect zonal stats...')\n",
    "        # wtd_asp_metrics_table = ZonalStatisticsAsTable(in_zone_data = wtd_merge, zone_field =\"cat_ID_txt\",\n",
    "        #                                                in_value_raster = asp_rast, out_table = wtd_merge_asp_table_path,\n",
    "        #                                                statistics_type='ALL')\n",
    "        # arcpy.AddField_management(wtd_asp_metrics_table, 'region', field_type='TEXT')\n",
    "        # Add cat_ID_Con field\n",
    "        # arcpy.AddField_management(wtd_asp_metrics_table,'cat_ID_con',field_type='TEXT')\n",
    "        # arcpy.CalculateField_management(wtd_asp_metrics_table, 'region', 'roi')\n",
    "        # Update region field\n",
    "        # with arcpy.da.UpdateCursor(wtd_asp_metrics_table,['region','cat_ID_txt','cat_ID_con']) as cur:\n",
    "        #     for row in cur:\n",
    "        #         row[0] = roi\n",
    "        #         strval = str(row[1])\n",
    "        #         row[2] = roi+\"_\"+strval.replace(\".0\",\"\")\n",
    "        #         # Update\n",
    "        #         cur.updateRow(row)\n",
    "        #     del(row)\n",
    "        # del(cur)\n",
    "        # wtd_asp_ztables.append(wtd_asp_metrics_table)\n",
    "\n",
    "        # # Aspect Zonal statistics for catchments\n",
    "        # print(f'Calculating {roi} catchment aspect zonal stats...')\n",
    "        # cat_asp_metrics_table = ZonalStatisticsAsTable(in_zone_data = cats, zone_field =\"cat_ID_txt\",\n",
    "        #                                                in_value_raster = asp_rast, out_table = cat_asp_table_path,\n",
    "        #                                                statistics_type='ALL')\n",
    "        # arcpy.AddField_management(cat_asp_metrics_table, 'region', field_type='TEXT')\n",
    "        # Add cat_ID_Con field\n",
    "        # arcpy.AddField_management(cat_asp_metrics_table,'cat_ID_con',field_type='TEXT')\n",
    "        # arcpy.CalculateField_management(cat_asp_metrics_table, 'region', 'roi')\n",
    "        # Update region field\n",
    "        # with arcpy.da.UpdateCursor(cat_asp_metrics_table,['region','cat_ID_txt','cat_ID_con']) as cur:\n",
    "        #     for row in cur:\n",
    "        #         strval = str(row[1])\n",
    "        #         row[2] = roi+\"_\"+strval.replace(\".0\",\"\")\n",
    "        #         # Update\n",
    "        #         cur.updateRow(row)\n",
    "        #     del(row)\n",
    "        # del(cur)\n",
    "        # cat_asp_ztables.append(cat_asp_metrics_table)\n",
    "\n",
    "        zstat_stop = time.time()\n",
    "        zstat_time = int (zstat_stop - zstat_start)\n",
    "        print(f'All Zonal Stats for {roi} Elapsed time: ({datetime.timedelta(seconds=zstat_time)})')\n",
    "        print(f'{\"*\"*100}')\n",
    "\n",
    "        # Tabulate Area with north grid and watersheds\n",
    "        tabarea_start = time.time()\n",
    "        tabarea_start1 = time.time()\n",
    "        print(f'Begin tabulate area of north facing cells for watersheds and catchments in {roi} region')\n",
    "        print(f'{\"*\"*100}')\n",
    "        # Percent North Tabulate area for watersheds\n",
    "        wtd_per_north_tabarea = arcpy.sa.TabulateArea(in_zone_data= wtd_merge,\n",
    "                                                      zone_field= wtd_cur_fields[0],\n",
    "                                                      in_class_data=nor_rast,\n",
    "                                                      class_field=\"Value\",\n",
    "                                                      out_table = wtd_merge_pernorth_table_path\n",
    "                                                      )\n",
    "        # Add region and percent north fields\n",
    "        arcpy.AlterField_management(wtd_per_north_tabarea,'CAT_ID_TXT','CAT_ID_TXT_DEL','CAT_ID_TXT_DEL')\n",
    "        arcpy.AddField_management(wtd_per_north_tabarea, 'region', field_type='TEXT')\n",
    "        arcpy.AddField_management(wtd_per_north_tabarea, 'NhdAwcH12_wtd_north_per', field_type='Float')\n",
    "        arcpy.AddField_management(wtd_per_north_tabarea, wtd_cur_fields[0], field_type='TEXT')\n",
    "        arcpy.AddField_management(wtd_per_north_tabarea, wtd_cur_fields[2], field_type='TEXT')\n",
    "        wtdnorfields = [f.name for f in arcpy.ListFields(wtd_per_north_tabarea)]\n",
    "        #print (wtdnorfields)\n",
    "        with arcpy.da.UpdateCursor(wtd_per_north_tabarea, wtdnorfields) as cur:\n",
    "            for row in cur:\n",
    "                strval = str(row[1])\n",
    "                row[4] = roi\n",
    "                row[5] = row[3]/(row[3]+row[2])*100\n",
    "                row[6] = strval.replace('.0','')\n",
    "                row[7] = roi +'_'+ strval.replace(\".0\",\"\")\n",
    "                # Update\n",
    "                cur.updateRow(row)\n",
    "            del(row)\n",
    "        del(cur)\n",
    "        # Drop UPPERCASE field form tab area\n",
    "        arcpy.DeleteField_management(wtd_per_north_tabarea,'CAT_ID_TXT_DEL')\n",
    "        # Append watershed percent north table to list\n",
    "        wtd_pernorth_taba_tables.append(wtd_per_north_tabarea)\n",
    "        # Report tab area times\n",
    "        tabarea_stop1 = time.time()\n",
    "        tabarea_time1 = int (tabarea_stop1 - tabarea_start1)\n",
    "        print(f'Watershed percent north Tabulate area/intersections for {roi} complete.\\nElapsed time: ({datetime.timedelta(seconds=tabarea_time1)})')\n",
    "        print(f'{\"*\"*100}')\n",
    "\n",
    "        # Percent Lakes Ponds using Tabulate Intersection for watersheds\n",
    "        print(f'Begin watershed percent lakes ponds for {roi}')\n",
    "        tabarea_start2 = time.time()\n",
    "        wtd_lp_tabint = arcpy.TabulateIntersection_analysis(wtd_merge,\n",
    "                                                            zone_fields=wtd_cur_fields[0],\n",
    "                                                            in_class_features=lakes_fc,\n",
    "                                                            out_table=wtd_merge_lp_table_path,\n",
    "                                                            class_fields='Ftype',\n",
    "                                                            out_units=\"SQUARE_METERS\"\n",
    "                                                            )\n",
    "        # Add region and cat id fields\n",
    "        arcpy.AlterField_management(wtd_lp_tabint,'PERCENTAGE','NhdAwcH12_wtd_lake_per','NhdAwcH12_wtd_lake_per')\n",
    "        arcpy.AlterField_management(wtd_lp_tabint,'AREA','NhdAwcH12_wtd_lake_area_sqm','NhdAwcH12_wtd_lake_area_sqm')\n",
    "        arcpy.AddField_management(wtd_lp_tabint, 'region', field_type='TEXT')\n",
    "        arcpy.AddField_management(wtd_lp_tabint, wtd_cur_fields[1], field_type='TEXT')\n",
    "        arcpy.AddField_management(wtd_lp_tabint, wtd_cur_fields[2], field_type='TEXT')\n",
    "        wtdlpfields = [f.name for f in arcpy.ListFields(wtd_lp_tabint)]\n",
    "        #print (wtdlpfields)\n",
    "        with arcpy.da.UpdateCursor(wtd_lp_tabint, wtdlpfields) as cur:\n",
    "            for row in cur:\n",
    "                strval = str(row[1])\n",
    "                row[5] = roi\n",
    "                row[6] = strval.replace('.0','')\n",
    "                row[7] = roi +'_'+ strval.replace(\".0\",\"\")\n",
    "                # Update\n",
    "                cur.updateRow(row)\n",
    "            del(row)\n",
    "        del(cur)\n",
    "\n",
    "        # Append watershed lakes ponds table to list\n",
    "        wtd_lp_tabint_tables.append(wtd_lp_tabint)\n",
    "        # Report tab area times\n",
    "        tabarea_stop2 = time.time()\n",
    "        tabarea_time2 = int (tabarea_stop2 - tabarea_start2)\n",
    "        print(f'Percent Lakes Tabulate area/intersections for {roi} complete.\\nElapsed time: ({datetime.timedelta(seconds=tabarea_time2)})')\n",
    "        print(f'{\"*\"*100}')\n",
    "\n",
    "        # Percent glaciers using Tabulate Intersection for watersheds\n",
    "        tabarea_start3 = time.time()\n",
    "        print(f'Begin tabulate intersection between {glac_fc} and watersheds in {roi} region')\n",
    "        print(f'{\"*\"*100}')\n",
    "        wtd_glac_tabint = arcpy.TabulateIntersection_analysis(wtd_merge,\n",
    "                                                            zone_fields=wtd_cur_fields[0],\n",
    "                                                            in_class_features=glac_fc,\n",
    "                                                            out_table=wtd_merge_glac_table_path,\n",
    "                                                            class_fields='O1Region',\n",
    "                                                            out_units=\"SQUARE_METERS\"\n",
    "                                                            )\n",
    "        # Add region and cat id fields\n",
    "        arcpy.AlterField_management(wtd_glac_tabint,'PERCENTAGE','NhdAwcH12_wtd_glacier_per','NhdAwcH12_wtd_glacier_per')\n",
    "        arcpy.AlterField_management(wtd_glac_tabint,'AREA','NhdAwcH12_wtd_glacier_area_sqm','NhdAwcH12_wtd_glacier_area_sqm')\n",
    "        arcpy.AddField_management(wtd_glac_tabint, 'region', field_type='TEXT')\n",
    "        arcpy.AddField_management(wtd_glac_tabint, wtd_cur_fields[1], field_type='TEXT')\n",
    "        arcpy.AddField_management(wtd_glac_tabint, wtd_cur_fields[2], field_type='TEXT')\n",
    "        wtdglacfields = [f.name for f in arcpy.ListFields(wtd_glac_tabint)]\n",
    "        #print (wtdglacfields)\n",
    "        with arcpy.da.UpdateCursor(wtd_glac_tabint, wtdglacfields) as cur:\n",
    "            for row in cur:\n",
    "                strval = str(row[1])\n",
    "                row[5] = roi\n",
    "                row[6] = strval.replace('.0','')\n",
    "                row[7] = roi +'_'+ strval.replace(\".0\",\"\")\n",
    "                # Update\n",
    "                cur.updateRow(row)\n",
    "            del(row)\n",
    "        del(cur)\n",
    "        # Append watershed percent glacier table to list\n",
    "        wtd_glac_tabint_tables.append(wtd_glac_tabint)\n",
    "        # Report tab area times\n",
    "        tabarea_stop3 = time.time()\n",
    "        tabarea_time3 = int (tabarea_stop3 - tabarea_start3)\n",
    "        print(f'Percent Glacier Tabulate area/intersections for {roi} complete.\\nElapsed time: ({datetime.timedelta(seconds=tabarea_time3)})')\n",
    "        print(f'{\"*\"*100}')\n",
    "\n",
    "        # Tabulate Area with wetlands grid and watersheds\n",
    "        tabarea_start4 = time.time()\n",
    "        print(f'Begin tabulate intersection between {wet_rast} and watersheds in {roi} region')\n",
    "        print(f'{\"*\"*100}')\n",
    "        # Wetlands tabulate area for watersheds\n",
    "        wtd_per_wet_tabarea = arcpy.sa.TabulateArea(in_zone_data= wtd_merge,\n",
    "                                                      zone_field= wtd_cur_fields[0],\n",
    "                                                      in_class_data=wet_rast,\n",
    "                                                      class_field=\"Value\",\n",
    "                                                      out_table=wtd_merge_wetlands_table_path\n",
    "                                                      )\n",
    "        # Add region and percent wet fields\n",
    "        arcpy.AlterField_management(wtd_per_wet_tabarea,'CAT_ID_TXT','CAT_ID_TXT_DEL','CAT_ID_TXT_DEL')\n",
    "        arcpy.AddField_management(wtd_per_wet_tabarea, 'region', field_type='TEXT')\n",
    "        arcpy.AddField_management(wtd_per_wet_tabarea, 'NhdAwcH12_wtd_wet_per', field_type='Float')\n",
    "        arcpy.AddField_management(wtd_per_wet_tabarea, wtd_cur_fields[0], field_type='TEXT')\n",
    "        arcpy.AddField_management(wtd_per_wet_tabarea, wtd_cur_fields[2], field_type='TEXT')\n",
    "        wtdwetfields = [f.name for f in arcpy.ListFields(wtd_per_wet_tabarea)]\n",
    "        #print (wtdwetfields)\n",
    "        with arcpy.da.UpdateCursor(wtd_per_wet_tabarea, wtdwetfields) as cur:\n",
    "            for row in cur:\n",
    "                strval = str(row[1])\n",
    "                row[4] = roi\n",
    "                row[5] = row[3]/(row[3]+row[2])*100\n",
    "                row[6] = strval.replace('.0','')\n",
    "                row[7] = roi +'_'+ strval.replace(\".0\",\"\")\n",
    "                # Update\n",
    "                cur.updateRow(row)\n",
    "            del(row)\n",
    "        del(cur)\n",
    "        # Drop UPPERCASE field form tab area\n",
    "        arcpy.DeleteField_management(wtd_per_wet_tabarea,'CAT_ID_TXT_DEL')\n",
    "        # Append watershed percent wetlands table to list\n",
    "        wtd_wet_taba_tables.append(wtd_per_wet_tabarea)\n",
    "        # Report tab area times\n",
    "        tabarea_stop4 = time.time()\n",
    "        tabarea_time4 = int (tabarea_stop4 - tabarea_start4)\n",
    "        print(f'Percent Wetlands Tabulate area/intersections for {roi} complete.\\nElapsed time: ({datetime.timedelta(seconds=tabarea_time4)})')\n",
    "        print(f'{\"*\"*100}')\n",
    "\n",
    "        # # Percent North Tabulate Area for catchments\n",
    "        # cat_per_north_tabarea = arcpy.sa.TabulateArea(in_zone_data= cats, zone_field='cat_ID_con',\n",
    "        #                                             in_class_data=nor_rast,\"Value\",\n",
    "        #                                             out_table=cat_pernorth_table_path)\n",
    "\n",
    "        # # Add and calculate region identifier field for catchment table\n",
    "        # arcpy.AlterField_management(cat_per_north_tabarea,'CAT_ID_TXT','CAT_ID_TXT_DEL','CAT_ID_TXT_DEL')\n",
    "        # arcpy.AddField_management(cat_per_north_tabarea, 'region', field_type='TEXT')\n",
    "        # arcpy.AddField_management(cat_per_north_tabarea, 'cat_north_per', field_type='Float')\n",
    "        # arcpy.AddField_management(cat_per_north_tabarea, cat_cur_fields[0], field_type='TEXT')\n",
    "        # arcpy.AddField_management(cat_per_north_tabarea, cat_cur_fields[2], field_type='TEXT')\n",
    "        # catnorfields = [f.name for f in arcpy.ListFields(cat_per_north_tabarea)]\n",
    "        # print (catnorfields)\n",
    "        # with arcpy.da.UpdateCursor(cat_per_north_tabarea,catnorfields) as cur:\n",
    "        #     for row in cur:\n",
    "        #         strval = str(row[1])\n",
    "        #         row[4] = roi\n",
    "        #         row[5] = row[3]/(row[3]+row[2])*100\n",
    "        #         row[6] = strval.replace('.0','')\n",
    "        #         row[7] = roi +'_'+ strval.replace(\".0\",\"\")\n",
    "        #         # Update\n",
    "        #         cur.updateRow(row)\n",
    "        #     del(row)\n",
    "        # del(cur)\n",
    "        # Drop UPPERCASE field form tab area\n",
    "        # arcpy.DeleteField_management(cat_per_north_tabarea,'CAT_ID_TXT_DEL')\n",
    "        # # Append catchment percent north table to list\n",
    "        # cat_pernorth_taba_tables.append(cat_per_north_tabarea)\n",
    "        # Report tab area times\n",
    "        tabarea_stop = time.time()\n",
    "        tabarea_time = int (tabarea_stop - tabarea_start)\n",
    "        print(f'Tabulate area/intersections for {roi} complete\\nElapsed time: ({datetime.timedelta(seconds=tabarea_time)})')\n",
    "        print(f'{\"*\"*100}')\n",
    "\n",
    "        # Begin LCLD calculations\n",
    "        walk = arcpy.da.Walk(lcld_folder, datatype='RasterDataset')\n",
    "        for dirpath, dirnames, filenames in walk:\n",
    "            for filename in filenames:\n",
    "                raspath = os.path.join(dirpath, filename)\n",
    "                year = filename[0:4]\n",
    "                lcld_outname = roi+'NhdAwcH12_lcld_'+str(year)+'_zStats'\n",
    "                lcld_outpath = os.path.join(outgdb, lcld_outname)\n",
    "                print(f'Year: {year} - raster path {raspath}')\n",
    "                colname = 'NhdAwcH12_wtd_lcld_mn_' + str(year)\n",
    "                # lcld zonal statistics as table for all akssf watersheds\n",
    "                print(f'Calculating {filename} zonal stats for all {roi} watersheds...')\n",
    "                #arcpy.env.snapRaster = raspath\n",
    "                #arcpy.env.cellSize = raspath\n",
    "\n",
    "                # Begin Zonal Stats\n",
    "                lcldzstat_start = time.time()\n",
    "                print(f'Begin zonal stats for {filename}')\n",
    "                lcld_table = ZonalStatisticsAsTable(in_zone_data = wtd_merge,\n",
    "                                                                zone_field = 'cat_ID_con',\n",
    "                                                                in_value_raster = raspath,\n",
    "                                                                out_table = lcld_outpath,\n",
    "                                                                statistics_type='MEAN'\n",
    "                                                                )\n",
    "                # Append zTable to table list\n",
    "                lcld_Ztables.append(lcld_outpath)\n",
    "                arcpy.AlterField_management(lcld_table,'MEAN', colname,colname)\n",
    "                proc_list = [row[0] for row in arcpy.da.SearchCursor(lcld_table,'cat_ID_con')]\n",
    "                lcldzstat_stop = time.time()\n",
    "                lcldzstat_time = int (lcldzstat_stop - lcldzstat_start)\n",
    "                print(f'Zonal Stats for {filename} - Elapsed time: ({datetime.timedelta(seconds=lcldzstat_time)})')\n",
    "\n",
    "\n",
    "    except:\n",
    "        e = sys.exc_info()[1]\n",
    "        print(f'ERRFLAG!!! = {e.args[0]}')\n",
    "        arcpy.AddError(e.args[0])\n",
    "\n",
    "    iter_stop = time.time()\n",
    "    iter_time = int(iter_stop - iteration_start)\n",
    "    print(f'All Covariates for {roi} completed.\\nElapsed time: ({datetime.timedelta(seconds=iter_time)})')\n",
    "    print(f'{\"*\"*100}')\n",
    "\n",
    "# End timing\n",
    "processEnd = time.time()\n",
    "processElapsed = int(processEnd - processStart)\n",
    "processSuccess_time = datetime.datetime.now()\n",
    "\n",
    "# Report success\n",
    "print(f'Process completed at {processSuccess_time.strftime(\"%Y-%m-%d %H:%M\")} '\n",
    "      f'(Elapsed time: {datetime.timedelta(seconds=processElapsed)})')\n",
    "print(f'{\"*\"*100}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Examine LCLD tables and merge/export\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcld_2001_zStats\n",
      "lcld_2002_zStats\n",
      "lcld_2003_zStats\n",
      "lcld_2004_zStats\n",
      "lcld_2005_zStats\n",
      "lcld_2006_zStats\n",
      "lcld_2007_zStats\n",
      "lcld_2008_zStats\n",
      "lcld_2009_zStats\n",
      "lcld_2010_zStats\n",
      "lcld_2011_zStats\n",
      "lcld_2012_zStats\n",
      "lcld_2013_zStats\n",
      "lcld_2014_zStats\n",
      "lcld_2015_zStats\n",
      "lcld_2016_zStats\n",
      "lcld_2017_zStats\n",
      "lcld_2018_zStats\n",
      "lcld_2019_zStats\n",
      "lcld_2001_zStats\n",
      "lcld_2002_zStats\n",
      "lcld_2003_zStats\n",
      "lcld_2004_zStats\n",
      "lcld_2005_zStats\n",
      "lcld_2006_zStats\n",
      "lcld_2007_zStats\n",
      "lcld_2008_zStats\n",
      "lcld_2009_zStats\n",
      "lcld_2010_zStats\n",
      "lcld_2011_zStats\n",
      "lcld_2012_zStats\n",
      "lcld_2013_zStats\n",
      "lcld_2014_zStats\n",
      "lcld_2015_zStats\n",
      "lcld_2016_zStats\n",
      "lcld_2017_zStats\n",
      "lcld_2018_zStats\n",
      "lcld_2019_zStats\n"
     ]
    },
    {
     "data": {
      "text/plain": "                             NhdAwcH12_wtd_lcld_mn_2001_x  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                      529.096139   \nCook_Inlet_75004200001724                      532.838425   \nCook_Inlet_75004200001726                      532.696767   \nCook_Inlet_75004200001493                      537.966327   \nCook_Inlet_75004200004105                      552.605999   \n...                                                   ...   \nCopper_River_75003900029086                           NaN   \nCopper_River_75003900044552                           NaN   \nCopper_River_75003900054944                           NaN   \nCopper_River_75003900029096                           NaN   \nCopper_River_75003900047600                           NaN   \n\n                             NhdAwcH12_wtd_lcld_mn_2002_x  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                      522.720965   \nCook_Inlet_75004200001724                      526.509814   \nCook_Inlet_75004200001726                      528.730766   \nCook_Inlet_75004200001493                      532.807992   \nCook_Inlet_75004200004105                      547.518107   \n...                                                   ...   \nCopper_River_75003900029086                           NaN   \nCopper_River_75003900044552                           NaN   \nCopper_River_75003900054944                           NaN   \nCopper_River_75003900029096                           NaN   \nCopper_River_75003900047600                           NaN   \n\n                             NhdAwcH12_wtd_lcld_mn_2003_x  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                      522.882768   \nCook_Inlet_75004200001724                      516.782917   \nCook_Inlet_75004200001726                      518.955074   \nCook_Inlet_75004200001493                      506.473526   \nCook_Inlet_75004200004105                      520.284118   \n...                                                   ...   \nCopper_River_75003900029086                           NaN   \nCopper_River_75003900044552                           NaN   \nCopper_River_75003900054944                           NaN   \nCopper_River_75003900029096                           NaN   \nCopper_River_75003900047600                           NaN   \n\n                             NhdAwcH12_wtd_lcld_mn_2004_x  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                      514.113400   \nCook_Inlet_75004200001724                      518.399955   \nCook_Inlet_75004200001726                      522.615145   \nCook_Inlet_75004200001493                      522.566349   \nCook_Inlet_75004200004105                      527.237473   \n...                                                   ...   \nCopper_River_75003900029086                           NaN   \nCopper_River_75003900044552                           NaN   \nCopper_River_75003900054944                           NaN   \nCopper_River_75003900029096                           NaN   \nCopper_River_75003900047600                           NaN   \n\n                             NhdAwcH12_wtd_lcld_mn_2005_x  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                      520.218164   \nCook_Inlet_75004200001724                      516.323943   \nCook_Inlet_75004200001726                      519.107225   \nCook_Inlet_75004200001493                      504.502532   \nCook_Inlet_75004200004105                      515.436355   \n...                                                   ...   \nCopper_River_75003900029086                           NaN   \nCopper_River_75003900044552                           NaN   \nCopper_River_75003900054944                           NaN   \nCopper_River_75003900029096                           NaN   \nCopper_River_75003900047600                           NaN   \n\n                             NhdAwcH12_wtd_lcld_mn_2006_x  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                      533.944318   \nCook_Inlet_75004200001724                      528.420731   \nCook_Inlet_75004200001726                      529.118583   \nCook_Inlet_75004200001493                      518.689099   \nCook_Inlet_75004200004105                      531.036268   \n...                                                   ...   \nCopper_River_75003900029086                           NaN   \nCopper_River_75003900044552                           NaN   \nCopper_River_75003900054944                           NaN   \nCopper_River_75003900029096                           NaN   \nCopper_River_75003900047600                           NaN   \n\n                             NhdAwcH12_wtd_lcld_mn_2007_x  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                      510.872831   \nCook_Inlet_75004200001724                      519.110808   \nCook_Inlet_75004200001726                      524.365134   \nCook_Inlet_75004200001493                      526.816053   \nCook_Inlet_75004200004105                      534.917731   \n...                                                   ...   \nCopper_River_75003900029086                           NaN   \nCopper_River_75003900044552                           NaN   \nCopper_River_75003900054944                           NaN   \nCopper_River_75003900029096                           NaN   \nCopper_River_75003900047600                           NaN   \n\n                             NhdAwcH12_wtd_lcld_mn_2008_x  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                      535.508999   \nCook_Inlet_75004200001724                      537.231471   \nCook_Inlet_75004200001726                      542.061418   \nCook_Inlet_75004200001493                      553.797604   \nCook_Inlet_75004200004105                      561.502107   \n...                                                   ...   \nCopper_River_75003900029086                           NaN   \nCopper_River_75003900044552                           NaN   \nCopper_River_75003900054944                           NaN   \nCopper_River_75003900029096                           NaN   \nCopper_River_75003900047600                           NaN   \n\n                             NhdAwcH12_wtd_lcld_mn_2009_x  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                      513.403234   \nCook_Inlet_75004200001724                      514.988760   \nCook_Inlet_75004200001726                      518.244161   \nCook_Inlet_75004200001493                      519.326562   \nCook_Inlet_75004200004105                      536.340433   \n...                                                   ...   \nCopper_River_75003900029086                           NaN   \nCopper_River_75003900044552                           NaN   \nCopper_River_75003900054944                           NaN   \nCopper_River_75003900029096                           NaN   \nCopper_River_75003900047600                           NaN   \n\n                             NhdAwcH12_wtd_lcld_mn_2010_x  ...  \\\ncat_ID_con                                                 ...   \nCook_Inlet_75004200000901                      529.247769  ...   \nCook_Inlet_75004200001724                      528.820847  ...   \nCook_Inlet_75004200001726                      530.271466  ...   \nCook_Inlet_75004200001493                      542.378073  ...   \nCook_Inlet_75004200004105                      550.691235  ...   \n...                                                   ...  ...   \nCopper_River_75003900029086                           NaN  ...   \nCopper_River_75003900044552                           NaN  ...   \nCopper_River_75003900054944                           NaN  ...   \nCopper_River_75003900029096                           NaN  ...   \nCopper_River_75003900047600                           NaN  ...   \n\n                             NhdAwcH12_wtd_lcld_mn_2010_y  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                             NaN   \nCook_Inlet_75004200001724                             NaN   \nCook_Inlet_75004200001726                             NaN   \nCook_Inlet_75004200001493                             NaN   \nCook_Inlet_75004200004105                             NaN   \n...                                                   ...   \nCopper_River_75003900029086                    472.782352   \nCopper_River_75003900044552                    505.846055   \nCopper_River_75003900054944                    475.444300   \nCopper_River_75003900029096                    511.948393   \nCopper_River_75003900047600                    526.071806   \n\n                             NhdAwcH12_wtd_lcld_mn_2011_y  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                             NaN   \nCook_Inlet_75004200001724                             NaN   \nCook_Inlet_75004200001726                             NaN   \nCook_Inlet_75004200001493                             NaN   \nCook_Inlet_75004200004105                             NaN   \n...                                                   ...   \nCopper_River_75003900029086                    458.794501   \nCopper_River_75003900044552                    494.667803   \nCopper_River_75003900054944                    476.375077   \nCopper_River_75003900029096                    515.507792   \nCopper_River_75003900047600                    530.774082   \n\n                             NhdAwcH12_wtd_lcld_mn_2012_y  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                             NaN   \nCook_Inlet_75004200001724                             NaN   \nCook_Inlet_75004200001726                             NaN   \nCook_Inlet_75004200001493                             NaN   \nCook_Inlet_75004200004105                             NaN   \n...                                                   ...   \nCopper_River_75003900029086                    485.641902   \nCopper_River_75003900044552                    529.897337   \nCopper_River_75003900054944                    505.542194   \nCopper_River_75003900029096                    543.378319   \nCopper_River_75003900047600                    561.578317   \n\n                             NhdAwcH12_wtd_lcld_mn_2013_y  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                             NaN   \nCook_Inlet_75004200001724                             NaN   \nCook_Inlet_75004200001726                             NaN   \nCook_Inlet_75004200001493                             NaN   \nCook_Inlet_75004200004105                             NaN   \n...                                                   ...   \nCopper_River_75003900029086                    487.820103   \nCopper_River_75003900044552                    514.894985   \nCopper_River_75003900054944                    493.379799   \nCopper_River_75003900029096                    537.522651   \nCopper_River_75003900047600                    541.536563   \n\n                             NhdAwcH12_wtd_lcld_mn_2014_y  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                             NaN   \nCook_Inlet_75004200001724                             NaN   \nCook_Inlet_75004200001726                             NaN   \nCook_Inlet_75004200001493                             NaN   \nCook_Inlet_75004200004105                             NaN   \n...                                                   ...   \nCopper_River_75003900029086                    423.330805   \nCopper_River_75003900044552                    479.218491   \nCopper_River_75003900054944                    470.743883   \nCopper_River_75003900029096                    490.562411   \nCopper_River_75003900047600                    508.126298   \n\n                             NhdAwcH12_wtd_lcld_mn_2015_y  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                             NaN   \nCook_Inlet_75004200001724                             NaN   \nCook_Inlet_75004200001726                             NaN   \nCook_Inlet_75004200001493                             NaN   \nCook_Inlet_75004200004105                             NaN   \n...                                                   ...   \nCopper_River_75003900029086                    414.759543   \nCopper_River_75003900044552                    414.404403   \nCopper_River_75003900054944                    423.433889   \nCopper_River_75003900029096                    485.051815   \nCopper_River_75003900047600                    510.832255   \n\n                             NhdAwcH12_wtd_lcld_mn_2016_y  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                             NaN   \nCook_Inlet_75004200001724                             NaN   \nCook_Inlet_75004200001726                             NaN   \nCook_Inlet_75004200001493                             NaN   \nCook_Inlet_75004200004105                             NaN   \n...                                                   ...   \nCopper_River_75003900029086                    410.158446   \nCopper_River_75003900044552                    425.842085   \nCopper_River_75003900054944                    413.119983   \nCopper_River_75003900029096                    503.140669   \nCopper_River_75003900047600                    509.149879   \n\n                             NhdAwcH12_wtd_lcld_mn_2017_y  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                             NaN   \nCook_Inlet_75004200001724                             NaN   \nCook_Inlet_75004200001726                             NaN   \nCook_Inlet_75004200001493                             NaN   \nCook_Inlet_75004200004105                             NaN   \n...                                                   ...   \nCopper_River_75003900029086                    470.742159   \nCopper_River_75003900044552                    496.148778   \nCopper_River_75003900054944                    478.510037   \nCopper_River_75003900029096                    508.203663   \nCopper_River_75003900047600                    517.992697   \n\n                             NhdAwcH12_wtd_lcld_mn_2018_y  \\\ncat_ID_con                                                  \nCook_Inlet_75004200000901                             NaN   \nCook_Inlet_75004200001724                             NaN   \nCook_Inlet_75004200001726                             NaN   \nCook_Inlet_75004200001493                             NaN   \nCook_Inlet_75004200004105                             NaN   \n...                                                   ...   \nCopper_River_75003900029086                    451.558074   \nCopper_River_75003900044552                    491.614218   \nCopper_River_75003900054944                    472.765282   \nCopper_River_75003900029096                    500.313407   \nCopper_River_75003900047600                    528.987450   \n\n                             NhdAwcH12_wtd_lcld_mn_2019_y  \ncat_ID_con                                                 \nCook_Inlet_75004200000901                             NaN  \nCook_Inlet_75004200001724                             NaN  \nCook_Inlet_75004200001726                             NaN  \nCook_Inlet_75004200001493                             NaN  \nCook_Inlet_75004200004105                             NaN  \n...                                                   ...  \nCopper_River_75003900029086                    439.999561  \nCopper_River_75003900044552                    470.632806  \nCopper_River_75003900054944                    434.095321  \nCopper_River_75003900029096                    485.832311  \nCopper_River_75003900047600                    506.978774  \n\n[906 rows x 38 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NhdAwcH12_wtd_lcld_mn_2001_x</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2002_x</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2003_x</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2004_x</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2005_x</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2006_x</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2007_x</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2008_x</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2009_x</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2010_x</th>\n      <th>...</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2010_y</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2011_y</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2012_y</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2013_y</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2014_y</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2015_y</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2016_y</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2017_y</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2018_y</th>\n      <th>NhdAwcH12_wtd_lcld_mn_2019_y</th>\n    </tr>\n    <tr>\n      <th>cat_ID_con</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Cook_Inlet_75004200000901</th>\n      <td>529.096139</td>\n      <td>522.720965</td>\n      <td>522.882768</td>\n      <td>514.113400</td>\n      <td>520.218164</td>\n      <td>533.944318</td>\n      <td>510.872831</td>\n      <td>535.508999</td>\n      <td>513.403234</td>\n      <td>529.247769</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Cook_Inlet_75004200001724</th>\n      <td>532.838425</td>\n      <td>526.509814</td>\n      <td>516.782917</td>\n      <td>518.399955</td>\n      <td>516.323943</td>\n      <td>528.420731</td>\n      <td>519.110808</td>\n      <td>537.231471</td>\n      <td>514.988760</td>\n      <td>528.820847</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Cook_Inlet_75004200001726</th>\n      <td>532.696767</td>\n      <td>528.730766</td>\n      <td>518.955074</td>\n      <td>522.615145</td>\n      <td>519.107225</td>\n      <td>529.118583</td>\n      <td>524.365134</td>\n      <td>542.061418</td>\n      <td>518.244161</td>\n      <td>530.271466</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Cook_Inlet_75004200001493</th>\n      <td>537.966327</td>\n      <td>532.807992</td>\n      <td>506.473526</td>\n      <td>522.566349</td>\n      <td>504.502532</td>\n      <td>518.689099</td>\n      <td>526.816053</td>\n      <td>553.797604</td>\n      <td>519.326562</td>\n      <td>542.378073</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Cook_Inlet_75004200004105</th>\n      <td>552.605999</td>\n      <td>547.518107</td>\n      <td>520.284118</td>\n      <td>527.237473</td>\n      <td>515.436355</td>\n      <td>531.036268</td>\n      <td>534.917731</td>\n      <td>561.502107</td>\n      <td>536.340433</td>\n      <td>550.691235</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>Copper_River_75003900029086</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>472.782352</td>\n      <td>458.794501</td>\n      <td>485.641902</td>\n      <td>487.820103</td>\n      <td>423.330805</td>\n      <td>414.759543</td>\n      <td>410.158446</td>\n      <td>470.742159</td>\n      <td>451.558074</td>\n      <td>439.999561</td>\n    </tr>\n    <tr>\n      <th>Copper_River_75003900044552</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>505.846055</td>\n      <td>494.667803</td>\n      <td>529.897337</td>\n      <td>514.894985</td>\n      <td>479.218491</td>\n      <td>414.404403</td>\n      <td>425.842085</td>\n      <td>496.148778</td>\n      <td>491.614218</td>\n      <td>470.632806</td>\n    </tr>\n    <tr>\n      <th>Copper_River_75003900054944</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>475.444300</td>\n      <td>476.375077</td>\n      <td>505.542194</td>\n      <td>493.379799</td>\n      <td>470.743883</td>\n      <td>423.433889</td>\n      <td>413.119983</td>\n      <td>478.510037</td>\n      <td>472.765282</td>\n      <td>434.095321</td>\n    </tr>\n    <tr>\n      <th>Copper_River_75003900029096</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>511.948393</td>\n      <td>515.507792</td>\n      <td>543.378319</td>\n      <td>537.522651</td>\n      <td>490.562411</td>\n      <td>485.051815</td>\n      <td>503.140669</td>\n      <td>508.203663</td>\n      <td>500.313407</td>\n      <td>485.832311</td>\n    </tr>\n    <tr>\n      <th>Copper_River_75003900047600</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>526.071806</td>\n      <td>530.774082</td>\n      <td>561.578317</td>\n      <td>541.536563</td>\n      <td>508.126298</td>\n      <td>510.832255</td>\n      <td>509.149879</td>\n      <td>517.992697</td>\n      <td>528.987450</td>\n      <td>506.978774</td>\n    </tr>\n  </tbody>\n</table>\n<p>906 rows  38 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for table in lcld_Ztables:\n",
    "    tblname = table[-16:]\n",
    "    print(tblname)\n",
    "    dfname = tblname + '_arr'\n",
    "    # Make df\n",
    "    dfname = pd.DataFrame()\n",
    "    lcld_field_list = []\n",
    "    for field in arcpy.ListFields(table):\n",
    "        lcld_field_list.append(field.name)\n",
    "        #print(f'{field.name}')\n",
    "    lcld_arr = arcpy.da.TableToNumPyArray(table, lcld_field_list)\n",
    "    dfname = pd.DataFrame(lcld_arr)\n",
    "    dfname = dfname.drop(['OBJECTID','ZONE_CODE', 'AREA', 'COUNT'],axis=1)\n",
    "    dfname = dfname.set_index('cat_ID_con')\n",
    "    dfs.append(dfname)\n",
    "\n",
    "# Merge all data frames together\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "lcld_df = reduce(lambda left,right: pd.merge(left,right,on='cat_ID_con',how=\"outer\"), dfs)\n",
    "lcld_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Export merged dataframe to csv\n",
    "lcld_csv_out = os.path.join(outdir,'AKSSF_NHDPlus_AWC_HUC12_wtd_lcld_mn.csv')\n",
    "lcld_df.to_csv(lcld_csv_out, encoding = 'utf-8')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Drop unnecessary fields and rename as needed from merged tables.\n",
    "- Create Key value dictionary and use update cursor to rename fields."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Table names/paths\n",
    "wtd_per_north_table_out = os.path.join(outgdb, 'AKSSF_NHDPlus_awc_huc12_wtd_north_per')\n",
    "cat_elev_table_out = os.path.join(outgdb,'AKSSF_NHDPlus_awc_huc12_cat_elev')\n",
    "cat_slope_table_out = os.path.join(outgdb,'AKSSF_NHDPlus_awc_huc12_cat_slope')\n",
    "wtd_elev_table_out = os.path.join(outgdb, 'AKSSF_NHDPlus_awc_huc12_wtd_elev')\n",
    "wtd_per_glac_table_out = os.path.join(outgdb, 'AKSSF_NHDPlus_awc_huc12_wtd_glacier_per')\n",
    "wtd_per_lp_table_out = os.path.join(outgdb, 'AKSSF_NHDPlus_awc_huc12_wtd_lakepond_per')\n",
    "wtd_slope_table_out = os.path.join(outgdb, 'AKSSF_NHDPlus_awc_huc12_wtd_slope')\n",
    "wtd_wet_table_out = os.path.join(outgdb, 'AKSSF_NHDPlus_awc_huc12_wtd_wetland_per')\n",
    "\n",
    "# Merge all regional tables together\n",
    "outtables = []\n",
    "wtd_per_north = arcpy.Merge_management(wtd_pernorth_taba_tables, wtd_per_north_table_out)\n",
    "arcpy.AlterField_management(wtd_per_north,\"VALUE_0\",\"NhdAwcH12_non_north_area\",\"NhdAwcH12_non_north_area\")\n",
    "arcpy.AlterField_management(wtd_per_north,\"VALUE_1\",\"NhdAwcH12_north_area\",\"NhdAwcH12_north_area\")\n",
    "outtables.append(wtd_per_north)\n",
    "cat_elev = arcpy.Merge_management(cat_elev_ztables, cat_elev_table_out)\n",
    "outtables.append(cat_elev)\n",
    "wtd_elev = arcpy.Merge_management(wtd_elev_ztables, wtd_elev_table_out)\n",
    "outtables.append(wtd_elev)\n",
    "wtd_slope = arcpy.Merge_management(wtd_slope_ztables, wtd_slope_table_out)\n",
    "outtables.append(wtd_slope)\n",
    "cat_slope = arcpy.Merge_management(cat_slope_ztables, cat_slope_table_out)\n",
    "outtables.append(cat_slope)\n",
    "wtd_wet = arcpy.Merge_management(wtd_wet_taba_tables, wtd_wet_table_out)\n",
    "arcpy.AlterField_management(wtd_wet,\"VALUE_0\",\"NhdAwcH12_non_wetland_area\",\"NhdAwcH12_non_wetland_area\")\n",
    "arcpy.AlterField_management(wtd_wet,\"VALUE_1\",\"NhdAwcH12_wetland_area\",\"NhdAwcH12_wetland_area\")\n",
    "outtables.append(wtd_wet)\n",
    "wtd_glac = arcpy.Merge_management(wtd_glac_tabint_tables, wtd_per_glac_table_out)\n",
    "outtables.append(wtd_glac)\n",
    "wtd_lp = arcpy.Merge_management(wtd_lp_tabint_tables, wtd_per_lp_table_out)\n",
    "outtables.append(wtd_lp)\n",
    "print ('Tables merged')\n",
    "print('----------')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Set up field dictionary\n",
    "elevDict = { 'ZONE_CODE': ('NhdAwcH12_cat_elev_ZONE_CODE', 'NhdAwcH12_wtd_elev_ZONE_CODE'),\n",
    "         'COUNT': ('NhdAwcH12_cat_elev_COUNT', 'NhdAwcH12_wtd_elev_COUNT'),\n",
    "          'AREA': ('NhdAwcH12_cat_elev_AREA', 'NhdAwcH12_wtd_elev_AREA'),\n",
    "          'MIN': ('NhdAwcH12_cat_elev_MIN', 'NhdAwcH12_wtd_elev_MIN'),\n",
    "          'MAX': ('NhdAwcH12_cat_elev_MAX', 'NhdAwcH12_wtd_elev_MAX'),\n",
    "          'RANGE': ('NhdAwcH12_cat_elev_RANGE', 'NhdAwcH12_wtd_elev_RANGE'),\n",
    "          'MEAN': ('NhdAwcH12_cat_elev_MEAN', 'NhdAwcH12_wtd_elev_MEAN'),\n",
    "          'STD': ('NhdAwcH12_cat_elev_STD', 'NhdAwcH12_wtd_elev_STD'),\n",
    "          'SUM': ('NhdAwcH12_cat_elev_SUM', 'NhdAwcH12_wtd_elev_SUM'),\n",
    "          'VARIETY': ('NhdAwcH12_cat_elev_VARIETY', 'NhdAwcH12_wtd_elev_VARIETY'),\n",
    "          'MAJORITY': ('NhdAwcH12_cat_elev_MAJORITY', 'NhdAwcH12_wtd_elev_MAJORITY'),\n",
    "          'MINORITY': ('NhdAwcH12_cat_elev_MINORITY', 'NhdAwcH12_wtd_elev_MINORITY'),\n",
    "          'MEDIAN': ('NhdAwcH12_cat_elev_MEDIAN', 'NhdAwcH12_wtd_elev_MEDIAN'),\n",
    "          'PCT90': ('NhdAwcH12_cat_elev_PCT90', 'NhdAwcH12_wtd_elev_PCT90')\n",
    "         }\n",
    "\n",
    "slopeDict = { 'ZONE_CODE': ('NhdAwcH12_cat_slope_ZONE_CODE', 'NhdAwcH12_wtd_slope_ZONE_CODE'),\n",
    "         'COUNT': ('NhdAwcH12_cat_slope_COUNT', 'NhdAwcH12_wtd_slope_COUNT'),\n",
    "          'AREA': ('NhdAwcH12_cat_slope_AREA', 'NhdAwcH12_wtd_slope_AREA'),\n",
    "          'MIN': ('NhdAwcH12_cat_slope_MIN', 'NhdAwcH12_wtd_slope_MIN'),\n",
    "          'MAX': ('NhdAwcH12_cat_slope_MAX', 'NhdAwcH12_wtd_slope_MAX'),\n",
    "          'RANGE': ('NhdAwcH12_cat_slope_RANGE', 'NhdAwcH12_wtd_slope_RANGE'),\n",
    "          'MEAN': ('NhdAwcH12_cat_slope_MEAN', 'NhdAwcH12_wtd_slope_MEAN'),\n",
    "          'STD': ('NhdAwcH12_cat_slope_STD', 'NhdAwcH12_wtd_slope_STD'),\n",
    "          'SUM': ('NhdAwcH12_cat_slope_SUM', 'NhdAwcH12_wtd_slope_SUM'),\n",
    "          'VARIETY': ('NhdAwcH12_cat_slope_VARIETY', 'NhdAwcH12_wtd_slope_VARIETY'),\n",
    "          'MAJORITY': ('NhdAwcH12_cat_slope_MAJORITY', 'NhdAwcH12_wtd_slope_MAJORITY'),\n",
    "          'MINORITY': ('NhdAwcH12_cat_slope_MINORITY', 'NhdAwcH12_wtd_slope_MINORITY'),\n",
    "          'MEDIAN': ('NhdAwcH12_cat_slope_MEDIAN', 'NhdAwcH12_wtd_slope_MEDIAN'),\n",
    "          'PCT90': ('NhdAwcH12_cat_slope_PCT90', 'NhdAwcH12_wtd_slope_PCT90')\n",
    "         }\n",
    "\n",
    "# Rename fields for elevation tables\n",
    "for field in arcpy.ListFields(wtd_elev):\n",
    "    keyval = field.name\n",
    "    if keyval in elevDict:\n",
    "        newname = elevDict[keyval][1]\n",
    "        newalias = elevDict[keyval][1]\n",
    "        print (keyval, newname)\n",
    "        arcpy.AlterField_management(wtd_elev, keyval, newname, newalias)\n",
    "\n",
    "for field in arcpy.ListFields(cat_elev):\n",
    "    keyval = field.name\n",
    "    if keyval in elevDict:\n",
    "        newname = elevDict[keyval][0]\n",
    "        newalias = elevDict[keyval][0]\n",
    "        print (keyval, newname)\n",
    "        arcpy.AlterField_management(cat_elev, keyval, newname, newalias)\n",
    "\n",
    "# Rename fields for slope tables\n",
    "for field in arcpy.ListFields(wtd_slope):\n",
    "    keyval = field.name\n",
    "    if keyval in slopeDict:\n",
    "        newname = slopeDict[keyval][1]\n",
    "        newalias = slopeDict[keyval][1]\n",
    "        print (keyval, newname)\n",
    "        arcpy.AlterField_management(wtd_slope, keyval, newname, newalias)\n",
    "\n",
    "for field in arcpy.ListFields(cat_slope):\n",
    "    keyval = field.name\n",
    "    if keyval in slopeDict:\n",
    "        newname = slopeDict[keyval][0]\n",
    "        newalias = slopeDict[keyval][0]\n",
    "        print (keyval, newname)\n",
    "        arcpy.AlterField_management(cat_slope, keyval, newname, newalias)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Export copies of dbf tables as csv\n",
    "# for table in outtables:\n",
    "#     tablename = arcpy.Describe(table).basename + \".csv\"\n",
    "#     tablepath = os.path.join(outdir,tablename)\n",
    "#     print( tablepath)\n",
    "#     arcpy.conversion.TableToTable(table, outdir, tablename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.2f}'.format # only display 2 decimal places\n",
    "# list to store covariate data frames\n",
    "dfs = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make catchment elev df\n",
    "cat_df = pd.DataFrame()\n",
    "cat_field_list = []\n",
    "for field in arcpy.ListFields(cat_elev):\n",
    "    cat_field_list.append(field.name)\n",
    "cat_elev_arr = arcpy.da.TableToNumPyArray(cat_elev,cat_field_list)\n",
    "cat_df = pd.DataFrame(cat_elev_arr)\n",
    "cat_df = cat_df.drop([\"OBJECTID\",\"NhdAwcH12_cat_elev_ZONE_CODE\"],axis=1)\n",
    "cat_df = cat_df.set_index('cat_ID_con')\n",
    "dfs.append(cat_df)\n",
    "cat_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make catchment slope df\n",
    "cat_sl_df = pd.DataFrame()\n",
    "cat_sl_field_list = []\n",
    "for field in arcpy.ListFields(cat_slope):\n",
    "    cat_sl_field_list.append(field.name)\n",
    "cat_sl_arr = arcpy.da.TableToNumPyArray(cat_slope, cat_sl_field_list)\n",
    "cat_sl_df = pd.DataFrame(cat_sl_arr)\n",
    "cat_sl_df = cat_sl_df.drop([\"OBJECTID\", \"NhdAwcH12_cat_slope_ZONE_CODE\"],axis=1)\n",
    "cat_sl_df = cat_sl_df.set_index('cat_ID_con')\n",
    "dfs.append(cat_sl_df)\n",
    "cat_sl_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed elev df\n",
    "wtd_df = pd.DataFrame()\n",
    "wtd_field_list = []\n",
    "for field in arcpy.ListFields(wtd_elev):\n",
    "    wtd_field_list.append(field.name)\n",
    "wtd_elev_arr = arcpy.da.TableToNumPyArray(wtd_elev,wtd_field_list)\n",
    "wtd_df = pd.DataFrame(wtd_elev_arr)\n",
    "wtd_df = wtd_df.drop([\"OBJECTID\",\"NhdAwcH12_wtd_elev_ZONE_CODE\"],axis=1)\n",
    "wtd_df = wtd_df.set_index('cat_ID_con')\n",
    "dfs.append(wtd_df)\n",
    "wtd_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed slope df\n",
    "wtd_sl_df = pd.DataFrame()\n",
    "wtd_sl_field_list = []\n",
    "for field in arcpy.ListFields(wtd_slope):\n",
    "    wtd_sl_field_list.append(field.name)\n",
    "wtd_sl_arr = arcpy.da.TableToNumPyArray(wtd_slope, wtd_sl_field_list)\n",
    "wtd_sl_df = pd.DataFrame(wtd_sl_arr)\n",
    "wtd_sl_df = wtd_sl_df.drop([\"OBJECTID\", \"NhdAwcH12_wtd_slope_ZONE_CODE\"],axis=1)\n",
    "wtd_sl_df = wtd_sl_df.set_index('cat_ID_con')\n",
    "dfs.append(wtd_sl_df)\n",
    "wtd_sl_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed north df\n",
    "wtd_n_df = pd.DataFrame()\n",
    "wtd_n_field_list = []\n",
    "for field in arcpy.ListFields(wtd_per_north):\n",
    "    wtd_n_field_list.append(field.name)\n",
    "wtd_n_arr = arcpy.da.TableToNumPyArray(wtd_per_north,wtd_n_field_list)\n",
    "wtd_n_df = pd.DataFrame(wtd_n_arr)\n",
    "wtd_n_df = wtd_n_df.drop(\"OBJECTID\",axis=1)\n",
    "wtd_n_df = wtd_n_df.set_index('cat_ID_con')\n",
    "dfs.append(wtd_n_df)\n",
    "wtd_n_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed wetland df\n",
    "wtd_wet_df = pd.DataFrame()\n",
    "wtd_wet_field_list = []\n",
    "for field in arcpy.ListFields(wtd_wet):\n",
    "    wtd_wet_field_list.append(field.name)\n",
    "wtd_wet_arr = arcpy.da.TableToNumPyArray(wtd_wet,wtd_wet_field_list)\n",
    "wtd_wet_df = pd.DataFrame(wtd_wet_arr)\n",
    "wtd_wet_df = wtd_wet_df.drop(\"OBJECTID\",axis=1)\n",
    "wtd_wet_df = wtd_wet_df.set_index('cat_ID_con')\n",
    "dfs.append(wtd_wet_df)\n",
    "wtd_wet_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed lakes df\n",
    "wtd_lp_df = pd.DataFrame()\n",
    "wtd_lp_field_list = []\n",
    "for field in arcpy.ListFields(wtd_lp):\n",
    "    wtd_lp_field_list.append(field.name)\n",
    "wtd_lp_arr = arcpy.da.TableToNumPyArray(wtd_lp, wtd_lp_field_list)\n",
    "wtd_lp_df = pd.DataFrame(wtd_lp_arr)\n",
    "wtd_lp_df = wtd_lp_df.drop(\"OBJECTID\",axis=1)\n",
    "wtd_lp_df = wtd_lp_df.set_index('cat_ID_con')\n",
    "dfs.append(wtd_lp_df)\n",
    "wtd_lp_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed glacier df\n",
    "wtd_glac_df = pd.DataFrame()\n",
    "wtd_glac_field_list = []\n",
    "for field in arcpy.ListFields(wtd_glac):\n",
    "    wtd_glac_field_list.append(field.name)\n",
    "wtd_glac_arr = arcpy.da.TableToNumPyArray(wtd_glac, wtd_glac_field_list)\n",
    "wtd_glac_df = pd.DataFrame(wtd_glac_arr)\n",
    "wtd_glac_df = wtd_glac_df.drop(\"OBJECTID\",axis=1)\n",
    "wtd_glac_df = wtd_glac_df.set_index('cat_ID_con')\n",
    "dfs.append(wtd_glac_df)\n",
    "wtd_glac_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merge all covariate dataframes together and drop unnecessary columns\n",
    " * Recalculate cat_ID as float64 type\n",
    " * Reorder columns\n",
    " * Export final csv\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merge all data frames together\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "df_final = reduce(lambda left,right: pd.merge(left,right,on='cat_ID_con',how=\"outer\"), dfs)\n",
    "#Generate unique column names\n",
    "def uniquify(df_final):\n",
    "    seen = set()\n",
    "    for item in df_final:\n",
    "        fudge = 1\n",
    "        newitem = item\n",
    "        while newitem in seen:\n",
    "            fudge += 1\n",
    "            newitem = \"{}_{}\".format(item, fudge)\n",
    "        yield newitem\n",
    "        seen.add(newitem)\n",
    "df_final.columns = list(uniquify(df_final))\n",
    "#List of final columns in the order to output\n",
    "# final_cols_old = ['cat_ID_txt','cat_ID','region', 'NhdAwcH12_cat_slope_COUNT', 'NhdAwcH12_cat_slope_AREA', 'NhdAwcH12_cat_slope_MIN', 'NhdAwcH12_cat_slope_MAX',\n",
    "#               'NhdAwcH12_cat_slope_RANGE','NhdAwcH12_cat_slope_MEAN', 'NhdAwcH12_cat_slope_STD', 'NhdAwcH12_cat_slope_SUM', 'NhdAwcH12_cat_slope_MEDIAN', 'NhdAwcH12_cat_slope_PCT90',\n",
    "#               'NhdAwcH12_cat_elev_COUNT', 'NhdAwcH12_cat_elev_AREA', 'NhdAwcH12_cat_elev_MIN', 'NhdAwcH12_cat_elev_MAX', 'NhdAwcH12_cat_elev_RANGE', 'NhdAwcH12_cat_elev_MEAN', 'NhdAwcH12_cat_elev_STD',\n",
    "#               'NhdAwcH12_cat_elev_SUM', 'NhdAwcH12_cat_elev_VARIETY', 'NhdAwcH12_cat_elev_MAJORITY', 'NhdAwcH12_cat_elev_MINORITY', 'NhdAwcH12_cat_elev_MEDIAN', 'NhdAwcH12_cat_elev_PCT90',\n",
    "#               'NhdAwcH12_wtd_elev_COUNT', 'NhdAwcH12_wtd_elev_AREA', 'NhdAwcH12_wtd_elev_MIN', 'NhdAwcH12_wtd_elev_MAX', 'NhdAwcH12_wtd_elev_RANGE', 'NhdAwcH12_wtd_elev_MEAN',\n",
    "#               'NhdAwcH12_wtd_elev_STD', 'NhdAwcH12_wtd_elev_SUM', 'NhdAwcH12_wtd_elev_VARIETY', 'NhdAwcH12_wtd_elev_MAJORITY', 'NhdAwcH12_wtd_elev_MINORITY',\n",
    "#               'NhdAwcH12_wtd_elev_MEDIAN', 'NhdAwcH12_wtd_elev_PCT90', 'NhdAwcH12_wtd_slope_COUNT', 'NhdAwcH12_wtd_slope_AREA', 'NhdAwcH12_wtd_slope_MIN', 'NhdAwcH12_wtd_slope_MAX',\n",
    "#               'NhdAwcH12_wtd_slope_RANGE', 'NhdAwcH12_wtd_slope_MEAN', 'NhdAwcH12_wtd_slope_STD', 'NhdAwcH12_wtd_slope_SUM', 'NhdAwcH12_wtd_slope_MEDIAN', 'NhdAwcH12_wtd_slope_PCT90',\n",
    "#               'NhdAwcH12_non_north_area', 'NhdAwcH12_north_area', 'NhdAwcH12_wtd_north_per', 'non_wetland_area', 'NhdAwcH12_wetland_area', 'NhdAwcH12_wtd_wet_per',\n",
    "#               'NhdAwcH12_wtd_lake_area_sqm', 'NhdAwcH12_wtd_lake_per', 'NhdAwcH12_wtd_glacier_area_sqm', 'NhdAwcH12_wtd_glacier_per' ]\n",
    "final_cols = ['cat_ID_txt','cat_ID','region', 'NhdAwcH12_cat_slope_COUNT', 'NhdAwcH12_cat_slope_AREA', 'NhdAwcH12_cat_slope_MIN', 'NhdAwcH12_cat_slope_MAX',\n",
    "              'NhdAwcH12_cat_slope_RANGE','NhdAwcH12_cat_slope_MEAN', 'NhdAwcH12_cat_slope_STD', 'NhdAwcH12_cat_slope_SUM', 'NhdAwcH12_cat_slope_MEDIAN', 'NhdAwcH12_cat_slope_PCT90',\n",
    "              'NhdAwcH12_cat_elev_COUNT', 'NhdAwcH12_cat_elev_AREA', 'NhdAwcH12_cat_elev_MIN', 'NhdAwcH12_cat_elev_MAX', 'NhdAwcH12_cat_elev_RANGE', 'NhdAwcH12_cat_elev_MEAN', 'NhdAwcH12_cat_elev_STD',\n",
    "              'NhdAwcH12_cat_elev_SUM', 'NhdAwcH12_cat_elev_VARIETY', 'NhdAwcH12_cat_elev_MAJORITY', 'NhdAwcH12_cat_elev_MINORITY', 'NhdAwcH12_cat_elev_MEDIAN', 'NhdAwcH12_cat_elev_PCT90',\n",
    "              'NhdAwcH12_wtd_elev_MIN', 'NhdAwcH12_wtd_elev_MAX','NhdAwcH12_wtd_elev_MEAN','NhdAwcH12_wtd_elev_STD','NhdAwcH12_wtd_slope_MIN', 'NhdAwcH12_wtd_slope_MAX','NhdAwcH12_wtd_slope_MEAN',\n",
    "              'NhdAwcH12_wtd_slope_STD','NhdAwcH12_non_north_area', 'NhdAwcH12_north_area', 'NhdAwcH12_wtd_north_per', 'non_wetland_area', 'NhdAwcH12_wetland_area', 'NhdAwcH12_wtd_wet_per',\n",
    "              'NhdAwcH12_wtd_lake_area_sqm', 'NhdAwcH12_wtd_lake_per', 'NhdAwcH12_wtd_glacier_area_sqm', 'NhdAwcH12_wtd_glacier_per' ]\n",
    "\n",
    "#Create list of duplicate column names and drop\n",
    "drop_cols = ['cat_ID_txt_y', 'region_y', 'cat_ID_txt_x_2', 'region_x_2', 'region_y_2', 'cat_ID_txt_y_2', 'region_x_3',\n",
    "             'cat_ID_txt_x_3', 'cat_ID_txt_y_3', 'FType', 'region_y_3','cat_ID_txt_x_4', 'O1Region', 'region_x_4',\n",
    "             'cat_ID_y', 'cat_ID_txt_y_4', 'region_y_4']\n",
    "df_final.drop(columns=drop_cols, axis = 1, inplace=True)\n",
    "#rename columns\n",
    "df_final.rename({'cat_ID_txt_x':'cat_ID_txt','cat_ID_x':'cat_ID','region_x':'region'},axis=1, inplace=True)\n",
    "#Recalculate cat_ID\n",
    "df_final['cat_ID'] = df_final['cat_ID_txt'].astype(np.float64)\n",
    "# reorder cols\n",
    "df_final = df_final.reindex(columns=final_cols)\n",
    "df_final"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export merged dataframe to csv\n",
    "cov_csv_out = os.path.join(outdir,'AKSSF_AWC_HUC12s_Covariates.csv')\n",
    "df_final.to_csv(cov_csv_out, encoding = 'utf-8')\n",
    "print('Export all covariates dataframe to csv complete')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# bbay_df = df_final.filter(like='Bristol_Bay', axis = 0)\n",
    "# bbay_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # kod_df = df_final.filter(like='Kodiak', axis = 0)\n",
    "# kod_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pws_df = df_final.filter(like='Prince', axis = 0)\n",
    "# pws_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ci_df = df_final.filter(like='Cook', axis = 0)\n",
    "# ci_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cop_df = df_final.filter(like='Copper', axis = 0)\n",
    "cop_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ci_df = df_final.filter(like='Cook', axis = 0)\n",
    "ci_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8caab41e",
   "language": "python",
   "display_name": "PyCharm (AKSSF)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}