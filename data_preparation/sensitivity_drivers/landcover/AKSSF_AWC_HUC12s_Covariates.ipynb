{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a id='top'></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Watershed metrics for all Huc12 subwatersheds that intersect AWC recorded streams\n",
    "Iterate over AKSSF regions and identify all HUC12 sub-watersheds that intersect an AWC recorded stream. Identify the downstream-most/outlet catchment for each Huc12 from this pool and convert the polygon to INSIDE centroid point.  Calculate the distance to coastline as the straight line distance in Km from centroid point to NHD recorded coastline and export this as a feature class/table.  Next use the outlet catchments unique identifier to query the appropriate dataset and build watersheds for each outlet catchment.  Calculate watershed metrics listed in the covariate section and export final merged csv using the catchment unique identifier field \"cat_ID_con\" to link the metric back to the source catchment/HUC12.  Merge watersheds together and use to calculate covariates.\n",
    "## Covariates\n",
    "Covariates needed for prediction on AWC-HUC12 outlets are as follows:\n",
    "### Summer Precipitation\n",
    "To be calculated in R using the outlet catchment centroid point feature class exported during outlet identification process.\n",
    "### Watershed Slope Metrics\n",
    "Regional Slope grids created in AKSSF_merge_grids.ipynb script.\n",
    "Run zonal statistics on slope grid using merged watershed as zone feature.\n",
    "Field names and descriptions:\n",
    "* **awc_huc12s_wtd_slope_mn = mean watershed slope**\n",
    "* **awc_huc12s_wtd_slope_min = min watershed slope**\n",
    "* **awc_huc12s_wtd_slope_max = max watershed slope**\n",
    "* **awc_huc12s_wtd_slope_sd (or cv) = standard deviation of watershed slope**\n",
    "### Watershed Percent North Aspect\n",
    "Regional North grids created in AKSSF_merge_grids.ipynb scripts.\n",
    "North = aspects from 315-45 degrees and calculate the percentage of land area facing north for each watershed. Run tabulate area on north grid using merged watershed as zone feature and calculate percentage from area.\n",
    "Field names and descriptions:\n",
    "* **awc_huc12s_north_wtd = percent watershed with north aspect**\n",
    "### Watershed Percent Lake Cover\n",
    "Lakes feature classes for each network datatype (NHDPlus vs TauDEM) stored in AKSSF hydrography database on the T:\n",
    "Calculate percentage of watershed that is covered by lakes/ponds using tabulate interesection between lake features and watersheds.\n",
    "Field names and descriptions:\n",
    "* **awc_huc12s_wtd_lake_per = percent watershed covered by lakes**\n",
    "### Watershed Percent Glacier Cover\n",
    "Use input glacier fc (from previous covariate calculations) stored in regional gdbs an calculate percent of watershed with glacial coverage using tabulate intersection between lake features and watersheds.\n",
    "Field names and descriptions:\n",
    "* **awc_huc12s_wtd_glac_per = percent watershed covered by glaciers**\n",
    "### Watershed LCLD\n",
    "LCLD rasters created in AKSSF_MODIS_lcld_ipynb.\n",
    "Iterate over LCLD input rasters to produce yearly means for watersheds using zonal statistics.\n",
    "Field names and descriptions:\n",
    "* **awc_huc12s_wtd_lcld_mn_YYYY = mean lcld**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import modules\n",
    "Set initial environments and import modules\n",
    "Print system paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, arcpy, sys,datetime\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "\n",
    "print('imports complete')\n",
    "print(f'{(\"-\"*100)}')\n",
    "print(f'sys paths {sys.path}')\n",
    "print(f'{(\"-\"*100)}')\n",
    "print(f'Python Environment set to - {sys.base_exec_prefix}')\n",
    "print(f'{(\"-\"*100)}')\n",
    "print (datetime.datetime.now())\n",
    "outdir = os.path.dirname(os.getcwd())\n",
    "print(f'CSV table output directory set to {outdir}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions\n",
    "Define any functions that will be used"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to add key, value pairs to dictionary\n",
    "def append_value(dict_obj, key, value):\n",
    "    # Check if key exist in dict or not\n",
    "    if key in dict_obj:\n",
    "        # Key exist in dict.\n",
    "        # Check if type of value of key is list or not\n",
    "        if not isinstance(dict_obj[key], list):\n",
    "            # If type is not list then make it list\n",
    "            dict_obj[key] = [dict_obj[key]]\n",
    "        # Append the value in list\n",
    "        dict_obj[key].append(value)\n",
    "    else:\n",
    "        # As key is not in dict,\n",
    "        # so, add key-value pair\n",
    "        dict_obj[key] = value\n",
    "# Function to remove parenthesis from user inputs\n",
    "def replace_all(userinput, dic):\n",
    "    for i, j in dic.items():\n",
    "        userinput = userinput.replace(i, j)\n",
    "    return userinput\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 1\n",
    "### Set input datasets, output locations, and scratch workspaces\n",
    "User to input paths for necessary input data and output locations\n",
    "Scratch workspaces and output workspaces will be automatically created if they do not already exist."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get user inputs\n",
    "# Used to format user inputs\n",
    "inputDict = {\"'\":\"\",'\"':\"\"}\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        userinput = replace_all((input('Input AKSSF parent directory containing regional sub-folders.\\nLeave blank and hit enter to use the default location.\\nDefault = D:\\\\GIS\\\\AKSSF\\\\') or 'D:\\\\GIS\\\\AKSSF'),inputDict)\n",
    "        if not arcpy.Exists(userinput):\n",
    "            print('Path specified does not exist!\\nPlease re-enter a valid path')\n",
    "            continue\n",
    "        else:\n",
    "            data_dir = userinput\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted!')\n",
    "        sys.exit()\n",
    "\n",
    "print(f'AKSSF parent directory set to {data_dir}\\n {\"-\"*100}')\n",
    "# Specify path to AWC events fc\n",
    "while True:\n",
    "    try:\n",
    "        userinput2 = replace_all((input('Input path to awc events feature class or shapefile.\\nLeave blank and hit enter to use the default location.\\nDefault = J:\\\\GIS_data\\\\biota\\\\Aquatic\\\\Fauna\\\\AWC\\\\2021_Species_LifeStage.gdb\\\\AWC_2021_SpeciesEvents.gdb\\\\awcEventArcs') or \"J:\\\\GIS_data\\\\biota\\\\Aquatic\\\\Fauna\\\\AWC\\\\2021_Species_LifeStage.gdb\\\\AWC_2021_SpeciesEvents.gdb\\\\awcEventArcs\\\\\"), inputDict)\n",
    "        if not arcpy.Exists(userinput2):\n",
    "            print('Path specified does not exist!\\nPlease re-enter a valid path')\n",
    "            continue\n",
    "        else:\n",
    "            awc_events = userinput2\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted!')\n",
    "        sys.exit()\n",
    "print(f'AWC events feature class set to {awc_events}\\n {\"-\"*100}')\n",
    "\n",
    "# Enter output destination  - to create working folders and gdbs\n",
    "while True:\n",
    "    try:\n",
    "        userinput3 = replace_all((input('Input path to create working folders.\\nLeave blank and hit enter to use the default location.\\nDefault = W:\\\\GIS\\\\') or 'W:\\\\GIS\\\\'),inputDict)\n",
    "        if not arcpy.Exists(userinput):\n",
    "            print('Path specified does not exist!\\nPlease re-enter a valid path')\n",
    "            continue\n",
    "        else:\n",
    "            temp_path = userinput3\n",
    "            print(f'Output locations will be created at {temp_path}\\n {\"-\"*100}')\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted!')\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "# Path to lcld rasters\n",
    "lcld_folder = r'D:\\\\Basedata\\\\LCLD_rasters_archive'\n",
    "# Enter output destination  - to create working folders and gdbs\n",
    "while True:\n",
    "    try:\n",
    "        userinput4 = replace_all((input('Input path to LCLD raster parent folder.\\nLeave blank and hit enter to use the default location.\\nDefault = D:\\\\Basedata\\\\LCLD_rasters_archive\\\\') or 'D:\\\\Basedata\\\\LCLD_rasters_archive'),inputDict)\n",
    "        if not arcpy.Exists(userinput):\n",
    "            print('Path specified does not exist!\\nPlease re-enter a valid path')\n",
    "            continue\n",
    "        else:\n",
    "            lcld_folder = userinput4\n",
    "            print(f'LCLD subfolders located at {lcld_folder}\\n {\"-\"*100}')\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted!')\n",
    "        sys.exit()\n",
    "## Create working output location to store intermediate data\n",
    "dirname = 'AKSSF_awcHuc12_cv'\n",
    "tempgdbname = 'AKSSF_awcHuc12_cv.gdb'\n",
    "temp_dir = os.path.join(temp_path, dirname)\n",
    "\n",
    "# Create temporary working gdb\n",
    "if not arcpy.Exists(temp_dir):\n",
    "    os.makedirs(temp_dir)\n",
    "else:\n",
    "    print(f'Working Folder already created {temp_dir}\\n {\"-\"*100}')\n",
    "\n",
    "outcheck = os.path.join(temp_dir, tempgdbname)\n",
    "\n",
    "if arcpy.Exists(outcheck):\n",
    "    print (f'Output location already exists{outcheck}\\n {\"-\"*100}')\n",
    "    outgdb = outcheck\n",
    "if not arcpy.Exists(outcheck):\n",
    "    print(f'Creating output GDB\\n {\"-\"*100}')\n",
    "    tempgdb = arcpy.CreateFileGDB_management(temp_dir,tempgdbname)\n",
    "    print (f'Output geodatabase created at {outcheck}\\n {\"-\"*100}')\n",
    "    outgdb = tempgdb.getOutput(0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 1.1\n",
    "### Set and create local copies of additional input data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import arcpy\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "\n",
    "nhdplusfol = []\n",
    "tahuc12=[]\n",
    "\n",
    "# Create and set HUC12 data if it does not already exist\n",
    "nhdplushucs = os.path.join(outgdb, 'NHDPlusHUC12')\n",
    "tauhucs = os.path.join(outgdb, 'NHD_H_HUC12')\n",
    "\n",
    "if not arcpy.Exists(tauhucs):\n",
    "    print(f'Huc12 data for Tau Regions not yet created')\n",
    "    #Enter path to WBDHU12 from NHD_H gdb\n",
    "    while True:\n",
    "        try:\n",
    "            userinput6 = replace_all((input('Input path to source WBDHU12 for state of Alaska.\\nLeave blank and hit enter to use the default location.\\nDefault = J:\\\\GIS_data\\\\inlandWaters\\\\Hydrography\\\\NHD\\\\NHD_H_Alaska_State_GDB.gdb\\\\WBD\\\\WBDHU12') or 'J:\\\\GIS_data\\\\inlandWaters\\\\Hydrography\\\\NHD\\\\NHD_H_Alaska_State_GDB.gdb\\\\WBD\\\\WBDHU12'),inputDict)\n",
    "            if not arcpy.Exists(userinput6):\n",
    "                print('Path specified does not exist!\\nPlease re-enter a valid path')\n",
    "                continue\n",
    "            else:\n",
    "                tauhuc12 = userinput6\n",
    "                arcpy.CopyFeatures_management(tauhuc12,tauhucs)\n",
    "                print(f'WBD Huc12  copied to {tauhucs}\\n {\"-\"*100}')\n",
    "                break\n",
    "        except KeyboardInterrupt:\n",
    "            print('interrupted!')\n",
    "            sys.exit()\n",
    "\n",
    "else:\n",
    "    print(f'Tau Region Hucs {tauhucs} located and exists = {arcpy.Exists(tauhucs)}')\n",
    "\n",
    "if not arcpy.Exists(nhdplushucs):\n",
    "    print(f'Huc12 data for NHDPlus Regions not yet created')\n",
    "    #Enter NHDplus data folder\n",
    "    while True:\n",
    "        try:\n",
    "            userinput5 = replace_all((input('Input path to source NHDPlus parent folder.\\nLeave blank and hit enter to use the default location.\\nDefault = J:\\\\GIS_data\\\\inlandWaters\\\\Hydrography\\\\NHD\\\\NHDPlus') or 'J:\\\\GIS_data\\\\inlandWaters\\\\Hydrography\\\\NHD\\\\NHDPlus'),inputDict)\n",
    "            if not arcpy.Exists(userinput5):\n",
    "                print('Path specified does not exist!\\nPlease re-enter a valid path')\n",
    "                continue\n",
    "            else:\n",
    "                nhdplusfol = userinput5\n",
    "                print(f'NHD HUC12 will be copied to {nhdplushucs}\\n {\"-\"*100}')\n",
    "                hucs = []\n",
    "                walk = arcpy.da.Walk(nhdplusfol, datatype=\"FeatureClass\", type=\"Polygon\")\n",
    "\n",
    "                for dirpath, dirnames, filenames in walk:\n",
    "                    for filename in filenames:\n",
    "                        if filename == 'WBDHU12':\n",
    "                            hucs.append(os.path.join(dirpath, filename))\n",
    "                arcpy.Merge_management(hucs,nhdplushucs,'','ADD_SOURCE_INFO')\n",
    "                break\n",
    "        except KeyboardInterrupt:\n",
    "            print('interrupted!')\n",
    "            sys.exit()\n",
    "else:\n",
    "    print(f'NHDPlus Hucs {nhdplushucs} located and exists = {arcpy.Exists(nhdplushucs)}')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 2\n",
    "### By Region\n",
    "Identify downstream-most catchment for each Huc 12\n",
    " * Select by location and select catchment with most us contributing area\n",
    "    * NHDPlus\n",
    "        * Use update cursor to join TotalDrainageAreaSqKm from vaa table to catchment\n",
    "        * Find max value from selection and save as outlet catchment for that HUC12\n",
    "    * TauDEM\n",
    "        * DSContArea - Drainage area at the downstream end of the link. Generally this is one grid cell upstream of the downstream end because the drainage area at the downstream end grid cell includes the area of the stream being joined.\n",
    " * Generate Centroid point and append to centroid dataset\n",
    "    * Retain cat_id and Huc12-id\n",
    " * Append to HUC12 catchment dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import arcpy, time, os, datetime, operator\n",
    "\n",
    "arcpy.env.workspace = data_dir\n",
    "regions = arcpy.ListWorkspaces()\n",
    "\n",
    "# Dictionaries and lists\n",
    "nhdplusoutlets = []\n",
    "nhdplusawccatouts = []\n",
    "vaaDict = {}\n",
    "catsDict = {}\n",
    "huc12Dict = {}\n",
    "nhdidDict = {}\n",
    "\n",
    "# Separate data by source type\n",
    "nhdplus_dat = ['Cook_Inlet','Copper_River']\n",
    "tauDem_dat = ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound']\n",
    "\n",
    "# Loop through all processing areas\n",
    "# rois = nhdplus_dat + tauDem_dat\n",
    "\n",
    "# Or comment above and specify below specific subset\n",
    "regions = ['D:\\\\GIS\\\\AKSSF\\\\Cook_Inlet', 'D:\\\\GIS\\\\AKSSF\\\\Copper_River' ]\n",
    "\n",
    "# Start timing function\n",
    "processStart = time.time()\n",
    "processStartdt = datetime.datetime.now()\n",
    "\n",
    "for region in regions:\n",
    "    roi = os.path.basename(region)\n",
    "    print(roi)\n",
    "    if roi in nhdplus_dat:\n",
    "        # Start roi time\n",
    "        roi_start = time.time()\n",
    "        hucs = nhdplushucs\n",
    "        catsList = []\n",
    "        outletList = []\n",
    "        print(f'{roi} using data from {region} folder')\n",
    "        # Set workspace to region folder\n",
    "        arcpy.env.workspace = region\n",
    "        gdb = arcpy.ListWorkspaces(workspace_type='FileGDB')\n",
    "        sourcegdb = gdb[0]\n",
    "        walk = arcpy.da.Walk(sourcegdb, datatype = ['FeatureClass','Table'])\n",
    "        for dirpath, dirnames, filenames in walk:\n",
    "            for filename in filenames:\n",
    "                if filename == 'cats_merge':\n",
    "                    cats  = os.path.join(dirpath, filename)\n",
    "                    append_value(catsDict,roi,cats)\n",
    "                elif filename == 'vaa_merge':\n",
    "                    vaas = os.path.join(dirpath, filename)\n",
    "                    append_value(vaaDict, roi, vaas)\n",
    "        #Output names and paths\n",
    "        outletcatsname = roi + 'awc_huc12_catchment_outlets'\n",
    "        outcatspath = os.path.join(outgdb,outletcatsname)\n",
    "        outcatspath2 = os.path.join(sourcegdb,'awc_huc12_catchment_outlets')\n",
    "        outletcatptsname = roi + 'awc_huc12_catchment_outlets_pts'\n",
    "        outcatptspath = os.path.join(outgdb,outletcatptsname)\n",
    "        outcatptspath2 = os.path.join(sourcegdb,'awc_huc12_catchment_outlets_pts')\n",
    "\n",
    "        # Build Value dictionary to relate NHDPlus id to contributing area\n",
    "        fields = ['NHDPlusID','TotDASqKm']\n",
    "        fields2 = fields + ['cat_ID_con']\n",
    "        valueDict = {int(r[0]):(r[1]) for r in arcpy.da.SearchCursor(vaas, fields)}\n",
    "        where_clause=f'\"MERGE_SRC\" LIKE \\'%{roi}%\\''\n",
    "        print(f'where_clause = {where_clause}')\n",
    "        huclayer = arcpy.MakeFeatureLayer_management(hucs,'huclayer',where_clause = where_clause)\n",
    "        print(f'{arcpy.GetCount_management(huclayer)} huc12s in {roi}')\n",
    "        print(('*'*100))\n",
    "        hucselect = arcpy.SelectLayerByLocation_management(huclayer,'INTERSECT',awc_events,'','SUBSET_SELECTION')\n",
    "        print(('*'*100))\n",
    "        print(f'{arcpy.GetCount_management(hucselect)} Huc12s in {roi} intersect awc events input')\n",
    "        print(('*'*100))\n",
    "        hucFields = [f for f in arcpy.ListFields(hucselect)]\n",
    "        vcount =1\n",
    "        with arcpy.da.SearchCursor(hucselect,['HUC12','SHAPE@']) as cur:\n",
    "            for row in cur:\n",
    "                print(f'Processing HUC {row[0]}')\n",
    "                inhuc = row[1]\n",
    "                cat_layer = arcpy.MakeFeatureLayer_management(cats,'cat_layer')\n",
    "                # Select by location using awc and huc 12\n",
    "                arcpy.SelectLayerByLocation_management(cat_layer,'HAVE_THEIR_CENTER_IN',inhuc,'','NEW_SELECTION')\n",
    "                print(f'{vcount}. Finding outlet for HUC {row[0]} out of {arcpy.GetCount_management(cat_layer)} catchments.\\n{(\"*\" * 60)}')\n",
    "                catList = [r[0] for r in arcpy.da.SearchCursor(cat_layer, 'NHDPlusID')]\n",
    "                intersect = list(set(catList).intersection(valueDict))\n",
    "                catDict = {int(i):(valueDict[i]) for i in intersect}\n",
    "                # Find Catchment with max drainage area\n",
    "                outcatch = max(catDict.items(), key = operator.itemgetter(1))[0]\n",
    "                append_value(huc12Dict, row[0], [int(outcatch),roi,valueDict[int(outcatch)]])\n",
    "                append_value(nhdidDict,int(outcatch),[roi,row[0], valueDict[int(outcatch)]])\n",
    "                outletList.append(int(outcatch))\n",
    "                vcount+=1\n",
    "            del(row)\n",
    "        del(cur)\n",
    "\n",
    "        outlet_cats = arcpy.MakeFeatureLayer_management(cats,'outlet_cats')\n",
    "        out_expression ='\"NHDPlusID\" IN ' + str(tuple(outletList))\n",
    "        #print(out_expression)\n",
    "        outlet_cats_select = arcpy.SelectLayerByAttribute_management(outlet_cats,'NEW_SELECTION', out_expression)\n",
    "        print(f'Creating copy of {arcpy.GetCount_management(outlet_cats)} outlet catchments for Region {roi} at {outcatspath}')\n",
    "        print(('*'*100))\n",
    "\n",
    "        # Copy outputs\n",
    "        arcpy.FeatureClassToFeatureClass_conversion(outlet_cats_select,outgdb,outletcatsname)\n",
    "        arcpy.FeatureToPoint_management(outcatspath, outcatptspath, 'INSIDE')\n",
    "        # Create Copies to akssf data_dir regional gdbs also\n",
    "        arcpy.FeatureClassToFeatureClass_conversion(outlet_cats_select,sourcegdb,'awc_huc12_catchment_outlets')\n",
    "        arcpy.FeatureToPoint_management(outcatspath2, outcatptspath2, 'INSIDE')\n",
    "        nhdplusoutlets.append(outcatptspath)\n",
    "        nhdplusawccatouts.append(outcatspath)\n",
    "        # Add total drainage km from value dict to feature classes and cat_ID_con from regDict\n",
    "        upfcs = [outcatspath, outcatptspath,outcatptspath2,outcatptspath2]\n",
    "        for upfc in upfcs:\n",
    "            arcpy.AddField_management(upfc,fields[1],'TEXT')\n",
    "            arcpy.AddField_management(upfc,fields2[2],'TEXT')\n",
    "            with arcpy.da.UpdateCursor(upfc,fields2) as cur:\n",
    "                for row in cur:\n",
    "                    row[1] = valueDict[row[0]]\n",
    "                    row[2] = roi + '_' + str(int(row[0]))\n",
    "                    cur.updateRow(row)\n",
    "                del(row)\n",
    "            del(cur)\n",
    "\n",
    "        # End roi time\n",
    "        roi_stop = time.time()\n",
    "        roi_time = int (roi_stop - roi_start)\n",
    "        print(f'{roi} Elapsed time: ({datetime.timedelta(seconds=roi_time)})')\n",
    "        print(f'{\"*\"*60}')\n",
    "\n",
    "    else:\n",
    "        #Section of code for taudem to follow\n",
    "        print(('-'*100))\n",
    "        print(f'Region {roi} will not be processed')\n",
    "        print(('-'*100),'\\n')\n",
    "\n",
    "# End timing\n",
    "processEnd = time.time()\n",
    "processElapsed = int(processEnd - processStart)\n",
    "processSuccess_time = datetime.datetime.now()\n",
    "print(f'Process complete')\n",
    "\n",
    "# Report success\n",
    "print(f'Process completed at {processSuccess_time.strftime(\"%Y-%m-%d %H:%M\")} '\n",
    "      f'(Elapsed time: {datetime.timedelta(seconds=processElapsed)})')\n",
    "print(f'{\"*\"*100}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 2.1\n",
    "### Merge all outlet points together and calculate distance to coastline\n",
    "Calculate Distance to Coast from outlet catchment point to the nearest coastline as a straight line distance\n",
    " * Generate near table and export as seperate csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import arcpy, datetime\n",
    "import numpy as pd\n",
    "\n",
    "# Input path to coastline\n",
    "coast = r\"D:\\\\Basedata\\\\AKSSF_Basedata\\\\AKSSF_Basedata.gdb\\\\NHD_H_Alaska_Coastline_alb\"\n",
    "\n",
    "# Merge all catchment outlet centroids together\n",
    "outletsname = 'AKSSF_NHDPlus_awcHuc12_outlet_cats_points'\n",
    "outletspath = os.path.join(outgdb, outletsname)\n",
    "all_nhd_outlet_pts = arcpy.Merge_management(nhdplusoutlets,outletspath)\n",
    "\n",
    "if arcpy.Exists(coast):\n",
    "    # Start timing function\n",
    "    start = datetime.datetime.now()\n",
    "    print(f'Getting distance to coast {datetime.datetime.now()}...')\n",
    "    arcpy.analysis.Near(all_nhd_outlet_pts, coast, None, \"NO_LOCATION\", \"NO_ANGLE\", \"GEODESIC\", \"NEAR_DIST NEAR_DIST\")\n",
    "    arcpy.AlterField_management(all_nhd_outlet_pts,'NEAR_DIST','dist_catch_coast_km','dist_catch_coast_km' )\n",
    "    arcpy.AddField_management(all_nhd_outlet_pts,'HUC12','TEXT')\n",
    "\n",
    "    # Convert distance in meters to km\n",
    "    with arcpy.da.UpdateCursor(all_nhd_outlet_pts,['dist_catch_coast_km','NHDPlusID','HUC12']) as cur:\n",
    "        for row in cur:\n",
    "            row[0] = row[0] * 0.001\n",
    "            row[2] = nhdidDict[row[1]][1]\n",
    "            cur.updateRow(row)\n",
    "        del(row)\n",
    "    del(cur)\n",
    "else:\n",
    "    print('Check path to coastal feature class')\n",
    "\n",
    "stop = datetime.datetime.now()\n",
    "elapsed  = stop - start\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert to df and examine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "# Make catchment points df\n",
    "cat_df = pd.DataFrame()\n",
    "cat_field_list = []\n",
    "\n",
    "for field in arcpy.ListFields(all_nhd_outlet_pts):\n",
    "    cat_field_list.append(field.name)\n",
    "cat_arr = arcpy.da.TableToNumPyArray(all_nhd_outlet_pts, ['cat_ID_con', 'NHDPlusID','dist_catch_coast_km','TotDASqKm','HUC12'])\n",
    "cat_df = pd.DataFrame(cat_arr)\n",
    "cat_df = cat_df.set_index('cat_ID_con')\n",
    "cat_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export csv of outlet points for NHDPlus regions\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "#Export CSV to read into R\n",
    "nhd_catpts_outname = 'AKSSF_NHDPlus_awcHuc12_dist_catch_coast_km.csv'\n",
    "arcpy.da.NumPyArrayToTable(cat_arr,os.path.join(outdir,nhd_catpts_outname))\n",
    "print('CSV export complete')\n",
    "print('----------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 3\n",
    "Generate Watersheds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# NHDPLUS Watersheds\n",
    "\n",
    "import arcpy, time, datetime, os\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import arcpy, time, os, datetime, operator\n",
    "\n",
    "arcpy.env.workspace = data_dir\n",
    "regions = arcpy.ListWorkspaces()\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.qualifiedFieldNames = False\n",
    "sr = arcpy.SpatialReference(3338)  #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "\n",
    "wtdList = []\n",
    "wtdDict = {}\n",
    "\n",
    "# Separate data by source type\n",
    "nhdplus_dat = ['Cook_Inlet','Copper_River']\n",
    "#Limit to Cook inlet for testing\n",
    "regions = ['D:\\\\GIS\\\\AKSSF\\\\Copper_River','D:\\\\GIS\\\\AKSSF\\\\Cook_Inlet']\n",
    "\n",
    "# Start timing function\n",
    "processStart = time.time()\n",
    "processStartdt = datetime.datetime.now()\n",
    "\n",
    "for region in regions:\n",
    "    reg_start = time.time()\n",
    "    roi = os.path.basename(region)\n",
    "    print(roi)\n",
    "    if roi in nhdplus_dat:\n",
    "        try:\n",
    "            arcpy.env.workspace = region\n",
    "            gdb = arcpy.ListWorkspaces(workspace_type='FileGDB')\n",
    "            ingdb = gdb[0]\n",
    "            # set inputs\n",
    "            vaa = os.path.join(ingdb, \"vaa_merge\")\n",
    "            cats = os.path.join(ingdb, \"cats_merge\")\n",
    "            streams = os.path.join(ingdb, \"NHDFlowline_merge\")\n",
    "            outcats = os.path.join(ingdb, \"awc_huc12_catchment_outlets\")\n",
    "            # Create list of nhdplus ids for outlet catchments\n",
    "            idList = [int(row[0]) for row in arcpy.da.SearchCursor(outcats,'NHDPlusID')]\n",
    "            #Make test list of few small catchments\n",
    "            #idList = [75004400004166,75004400004344, 75004400010328]\n",
    "            # Get list of index names for cats merge and add index if not already created\n",
    "            index_names = [i.name for i in arcpy.ListIndexes(cats)]\n",
    "            print(index_names)\n",
    "            if 'NHDPlusID_index' not in index_names:\n",
    "                print (f'Creating index for {cats}')\n",
    "                arcpy.AddIndex_management(cats,'NHDPlusID','NHDPlusID_index')\n",
    "            else:\n",
    "                print(f'{cats} Indexed')\n",
    "\n",
    "            #watersheds feature dataset for storing fcs\n",
    "            fdatname = roi + '_Watersheds'\n",
    "            fdat = os.path.join(outgdb,fdatname)\n",
    "\n",
    "            if not arcpy.Exists(fdat):\n",
    "                arcpy.management.CreateFeatureDataset(outgdb, fdatname, sr)\n",
    "            else:\n",
    "                print(f'{fdat} exists for {roi}')\n",
    "\n",
    "            vaa_df1 = pd.DataFrame(arcpy.da.TableToNumPyArray(vaa, (\"NHDPlusID\", \"FromNode\", \"ToNode\", \"StartFlag\")))\n",
    "            stream_df = pd.DataFrame(arcpy.da.TableToNumPyArray(streams, (\"NHDPlusID\", \"FType\")))\n",
    "            dfs = [vaa_df1, stream_df]\n",
    "            vaa_df = reduce(lambda left,right: pd.merge(left,right,on='NHDPlusID',how=\"outer\"), dfs)\n",
    "            # remove pipelines\n",
    "            vaa_df = vaa_df[(vaa_df['FType'] != 428 )]\n",
    "            vaa_df\n",
    "\n",
    "            c=1\n",
    "            for id in idList:\n",
    "                iteration_start = time.time()\n",
    "                print(f'{c}. Starting watershed for HUC {str(id)} ({(len(idList) - c)} remaining)')\n",
    "                rec = [id]\n",
    "                up_ids = []\n",
    "                up_ids.append(rec)\n",
    "                rec_len = len(rec)\n",
    "                hws_sum = 0\n",
    "\n",
    "                while rec_len != hws_sum:\n",
    "                    fromnode = vaa_df.loc[vaa_df[\"NHDPlusID\"].isin(rec), \"FromNode\"]\n",
    "                    rec = vaa_df.loc[vaa_df[\"ToNode\"].isin(fromnode), \"NHDPlusID\"]\n",
    "                    rec_len = len(rec)\n",
    "                    rec_hws = vaa_df.loc[vaa_df[\"ToNode\"].isin(fromnode), \"StartFlag\"]\n",
    "                    hws_sum = sum(rec_hws)\n",
    "                    up_ids.append(rec)\n",
    "                #up_ids is a list with more than numbers, use extend to only keep numeric nhdplusids\n",
    "                newup_ids = []\n",
    "                for x in up_ids:\n",
    "                    newup_ids.extend(x)\n",
    "\n",
    "                tempLayer = \"catsLyr\"\n",
    "                expression = '\"NHDPlusID\" IN ({0})'.format(', '.join(map(str, newup_ids)) or 'NULL')\n",
    "                arcpy.MakeFeatureLayer_management(cats, tempLayer, where_clause=expression)\n",
    "                outdis = \"memory/wtd_\" + str(round(id))\n",
    "                outwtd = os.path.join(fdat,f'{roi}_wtd_{str(int(id))}')\n",
    "\n",
    "                dis = arcpy.Dissolve_management(tempLayer, outdis)\n",
    "                watershed = arcpy.EliminatePolygonPart_management(dis, outwtd,\"PERCENT\", \"0 SquareKilometers\", 90, \"CONTAINED_ONLY\")\n",
    "                wtdList.append(outwtd)\n",
    "                append_value(wtdDict,roi,outwtd)\n",
    "\n",
    "\n",
    "                # Stop iteration timer\n",
    "                iteration_stop = time.time()\n",
    "                iter_time = int (iteration_stop - iteration_start)\n",
    "                print(f'Elapsed time: ({datetime.timedelta(seconds=iter_time)})')\n",
    "                print(f'{\"*\"*60}')\n",
    "\n",
    "                c=c+1\n",
    "            wtd_merge = arcpy.Merge_management(wtdList, os.path.join(ingdb,f'awc_huc12_wtds_merge'),'','ADD_SOURCE_INFO')\n",
    "            arcpy.AddField_management(wtd_merge,'cat_ID_con','TEXT')\n",
    "            arcpy.AddField_management(wtd_merge,'cat_ID','DOUBLE')\n",
    "            arcpy.AddField_management(wtd_merge,'cat_ID_txt','TEXT')\n",
    "            arcpy.AddField_management(wtd_merge,'NHDPlusID','DOUBLE')\n",
    "            with arcpy.da.UpdateCursor(wtd_merge,['MERGE_SRC','NHDPlusID','cat_ID_con','cat_ID','cat_ID_txt']) as cur:\n",
    "                for row in cur:\n",
    "                    # Pull nhdplus id from merge source and calculate fields\n",
    "                    nhdplusid= int(row[0].split('_')[-1])\n",
    "                    row[1] = nhdplusid\n",
    "                    row[2] = roi + '_' + str(nhdplusid)\n",
    "                    row[3] = nhdplusid\n",
    "                    row[4] = str(nhdplusid)\n",
    "                    cur.updateRow(row)\n",
    "                del(row)\n",
    "            del(cur)\n",
    "            arcpy.CopyFeatures_management(wtd_merge,os.path.join(outgdb,f'{roi}_NhdAwcH12_wtds_merge' ))\n",
    "\n",
    "            # Stop iteration timer\n",
    "            reg_stop = time.time()\n",
    "            reg_time = int (reg_stop - reg_start)\n",
    "            print(f'{roi} Elapsed time: ({datetime.timedelta(seconds=reg_time)})')\n",
    "            print(f'{\"*\"*100}')\n",
    "\n",
    "        except:\n",
    "            e = sys.exc_info()[1]\n",
    "            print(e.args[0])\n",
    "            arcpy.AddError(e.args[0])\n",
    "\n",
    "# End timing\n",
    "processEnd = time.time()\n",
    "processElapsed = int(processEnd - processStart)\n",
    "processSuccess_time = datetime.datetime.now()\n",
    "\n",
    "# Report success\n",
    "print(f'Process completed at {processSuccess_time.strftime(\"%Y-%m-%d %H:%M\")} '\n",
    "      f'(Elapsed time: {datetime.timedelta(seconds=processElapsed)})')\n",
    "print(f'{\"*\"*100}')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from arcpy.sa import *\n",
    "arcpy.env.workspace = data_dir\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338) #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "regions  = arcpy.ListWorkspaces(workspace_type=\"Folder\")\n",
    "\n",
    "# Lists for variables not needed at present time\n",
    "#cat_asp_ztables = []\n",
    "#wtd_asp_ztables = []\n",
    "#cat_pernorth_taba_tables=[]\n",
    "\n",
    "# # Lists to store output tables\n",
    "# wtd_pernorth_taba_tables=[]\n",
    "# wtd_lp_tabint_tables = []\n",
    "# wtd_glac_tabint_tables = []\n",
    "# wtd_wet_taba_tables = []\n",
    "# cat_elev_ztables = []\n",
    "# wtd_elev_ztables = []\n",
    "# cat_slope_ztables = []\n",
    "wtd_slope_ztables = []\n",
    "# lcld_Ztables = []\n",
    "\n",
    "# Clear lists\n",
    "cat_cur_fields = []\n",
    "wtd_cur_fields = []\n",
    "\n",
    "# Start timing function\n",
    "processStart = time.time()\n",
    "processStartdt = datetime.datetime.now()\n",
    "\n",
    "# Seperate data by\n",
    "nhdplus_dat = ['Cook_Inlet','Copper_River']\n",
    "tauDem_dat = ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound']\n",
    "\n",
    "# Loop through all processing areas\n",
    "rois = nhdplus_dat + tauDem_dat\n",
    "#Limit to ci for testing\n",
    "regions = ['D:\\\\GIS\\\\AKSSF\\\\Copper_River']\n",
    "\n",
    "for region in regions:\n",
    "    roi = os.path.basename(region)\n",
    "    if roi in str(region):\n",
    "        print(f'{roi} using data from {region} folder')\n",
    "        # Set data and variables unique to regions with NHDPlus Data\n",
    "        if roi in nhdplus_dat:\n",
    "            lakes_fc = r\"D:\\\\Basedata\\\\AKSSF_Basedata\\\\AKSSF_Basedata.gdb\\\\AKSSF_NHDPlus_LakePond_alb\"\n",
    "            # Fields for update cursor\n",
    "            cat_cur_fields = ['cat_ID_txt', 'NHDPlusID',\"cat_ID_con\"]\n",
    "            wtd_cur_fields = ['cat_ID_txt', 'cat_ID',\"cat_ID_con\"]\n",
    "            print (f'{roi} in {nhdplus_dat} AKSSF list, using cat_fields {cat_cur_fields} and watershed fields {wtd_cur_fields}')\n",
    "            print(f'{\"*\"*100}')\n",
    "        # Set data and variables unique to regions with TauDEM Data\n",
    "        elif roi in tauDem_dat:\n",
    "            lakes_fc = r\"D:\\\\Basedata\\\\AKSSF_Basedata\\\\AKSSF_Basedata.gdb\\\\AKSSF_NHD_LakesPonds_alb\"\n",
    "            # Fields for update cursor\n",
    "            if roi == 'Bristol_Bay':\n",
    "                cat_cur_fields = ['cat_ID_txt', 'catID',\"cat_ID_con\"]\n",
    "                wtd_cur_fields = ['cat_ID_txt', 'cat_ID',\"cat_ID_con\"]\n",
    "            else:\n",
    "                cat_cur_fields = ['cat_ID_txt', 'gridcode',\"cat_ID_con\"]\n",
    "                wtd_cur_fields = ['cat_ID_txt', 'cat_ID',\"cat_ID_con\"]\n",
    "            print (f'{roi} in {tauDem_dat} TauDEM list, using cat_fields {cat_cur_fields} and watershed fields {wtd_cur_fields}')\n",
    "            print(f'{\"*\"*100}')\n",
    "        # Start iter timing function\n",
    "        iteration_start = time.time()\n",
    "        # Set workspace to region folder\n",
    "        arcpy.env.workspace = region\n",
    "        walk = arcpy.da.Walk(region, datatype = ['FeatureClass','RasterDataset'])\n",
    "        for dirpath, dirnames, filenames in walk:\n",
    "            for filename in filenames:\n",
    "                # Set merged watersheds dataset\n",
    "                if 'awc_huc12_wtds_merge'== filename:\n",
    "                    wtdpath = os.path.join(dirpath,filename)\n",
    "                    wtdname = roi +'_'+ filename\n",
    "                    # Make local copy projected in AKAlbers\n",
    "                    wtd_merge = arcpy.FeatureClassToFeatureClass_conversion(wtdpath,outgdb,wtdname)\n",
    "                    print(f'Merged watershed dataset {filename} found')\n",
    "                    print(f'{\"*\"*100}')\n",
    "                    wtdfieldnames = []\n",
    "                    wtdlstFields = arcpy.ListFields(wtd_merge)\n",
    "                    for field in wtdlstFields:\n",
    "                        wtdfieldnames.append(field.name)\n",
    "                    if str(wtd_cur_fields[0]) in wtdfieldnames:\n",
    "                        print (f'{wtd_cur_fields[0]} field already in dataset')\n",
    "                        print(f'{\"*\"*100}')\n",
    "                    else:\n",
    "                        print (f'Adding {wtd_cur_fields[0]} field to watershed dataset {wtd_merge}')\n",
    "                        print(f'{\"*\"*100}')\n",
    "                        # add cat_ID_txt field and concat cat_ID + region\n",
    "                        arcpy.AddField_management(wtd_merge, str(wtd_cur_fields[0]),field_type='TEXT')\n",
    "                        # populate cat_ID_txt\n",
    "                        with arcpy.da.UpdateCursor(wtd_merge, wtd_cur_fields[0:2]) as cur:\n",
    "                            for row in cur:\n",
    "                                strval = str(row[1])\n",
    "                                row[0] = strval.replace('.0',\"\")\n",
    "                                # Update rows\n",
    "                                cur.updateRow(row)\n",
    "                            del(row)\n",
    "                        del(cur)\n",
    "                    if str(wtd_cur_fields[2]) in wtdfieldnames:\n",
    "                        print (f'{wtd_cur_fields[2]} field already in dataset {wtd_merge}')\n",
    "                        print(f'{\"*\"*100}')\n",
    "                    else:\n",
    "                        print (f'Adding {wtd_cur_fields[2]} field to watershed dataset {wtd_merge}')\n",
    "                        print(f'{\"*\"*100}')\n",
    "                        # add cat_ID_con field and concat cat_ID + region\n",
    "                        arcpy.AddField_management(wtd_merge, str(wtd_cur_fields[2]),field_type='TEXT')\n",
    "                        # populate cat_ID_txt\n",
    "                        with arcpy.da.UpdateCursor(wtd_merge, wtd_cur_fields) as cur:\n",
    "                            for row in cur:\n",
    "                                strval = str(row[1])\n",
    "                                row[2] = str(roi) +'_'+ strval.replace(\".0\",\"\")\n",
    "                                # Update rows\n",
    "                                cur.updateRow(row)\n",
    "                            del(row)\n",
    "                        del(cur)\n",
    "\n",
    "                # Select glaciers fc\n",
    "                elif 'glaciers' == filename:\n",
    "                    # Make local copy projected in AKAlbers\n",
    "                    glacpath = os.path.join(dirpath, filename)\n",
    "                    glacname = roi+'_'+filename\n",
    "                    glac_fc = arcpy.FeatureClassToFeatureClass_conversion(glacpath,outgdb,glacname)\n",
    "\n",
    "                # Select elevation raster\n",
    "                elif 'elev.tif' == filename:\n",
    "                    elev_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "                # # Select aspect raster\n",
    "                # elif 'aspect' in filename:\n",
    "                #     asp_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "                # Select north raster\n",
    "                elif 'north.tif' == filename:\n",
    "                    nor_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "                # Select slope raster\n",
    "                elif 'slope.tif' == filename:\n",
    "                    slope_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "                # Select wetland raster\n",
    "                elif 'wetlands.tif' == filename:\n",
    "                    wet_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "                # Select catch_int fc (catchments of interest for region) and make a copy\n",
    "                elif 'awc_huc12_catchment_outlets' == filename:\n",
    "                    # Make local copy projected in AKAlbers\n",
    "                    catspath = os.path.join(dirpath,filename)\n",
    "                    catsname = roi +\"_\"+filename\n",
    "                    cats = arcpy.FeatureClassToFeatureClass_conversion(catspath, outgdb,catsname)\n",
    "                    catlstfields = arcpy.ListFields(cats)\n",
    "                    catfieldnames = []\n",
    "                    for field in catlstfields:\n",
    "                        catfieldnames.append(field.name)\n",
    "                    if str(cat_cur_fields[0]) in catfieldnames:\n",
    "                        print (f'{cat_cur_fields[0]} field already in dataset {cats}')\n",
    "                        print(f'{\"*\"*100}')\n",
    "                    else:\n",
    "                        print (f'Adding {cat_cur_fields[0]} field to catchment dataset {cats}')\n",
    "                        print(f'{\"*\"*100}')\n",
    "                        # add cat_ID_txt field\n",
    "                        arcpy.AddField_management(cats, str(cat_cur_fields[0]), field_type='TEXT')\n",
    "                        # populate cat_ID_txt\n",
    "                        with arcpy.da.UpdateCursor(cats, cat_cur_fields[0:2]) as cur:\n",
    "                            for row in cur:\n",
    "                                strval = str(row[1])\n",
    "                                row[0] = strval.replace('.0',\"\")\n",
    "                                # Update rows\n",
    "                                cur.updateRow(row)\n",
    "                            del(row)\n",
    "                        del(cur)\n",
    "                    if str(cat_cur_fields[2]) in catfieldnames:\n",
    "                        print (f'{cat_cur_fields[2]} field already in dataset {cats}')\n",
    "                        print(f'{\"*\"*100}')\n",
    "                    else:\n",
    "                        print (f'Adding {cat_cur_fields[2]} field to catchment dataset {cats}')\n",
    "                        print(f'{\"*\"*100}')\n",
    "                        # add cat_ID_txt field & cat_ID + region concat field\n",
    "                        arcpy.AddField_management(cats,str(cat_cur_fields[2]),field_type='TEXT')\n",
    "                        # populate cat_ID_con\n",
    "                        with arcpy.da.UpdateCursor(cats, cat_cur_fields) as cur:\n",
    "                            for row in cur:\n",
    "                                strval = str(row[1])\n",
    "                                row[2] = str(roi) +'_'+ strval.replace('.0',\"\")\n",
    "                                # Update rows\n",
    "                                cur.updateRow(row)\n",
    "                            del(row)\n",
    "                        del(cur)\n",
    "\n",
    "        print (f'Calculating topographic metrics for catchments & watersheds of interest in {roi} region')\n",
    "        print ('----------')\n",
    "        print(f'Geodatabase: {outgdb}')\n",
    "        print ('----------')\n",
    "        print (f'Watershed Merge: {wtd_merge}')\n",
    "        print (f'  Projection {arcpy.Describe(wtd_merge).spatialReference.name}')\n",
    "        print ('----------')\n",
    "        print (f'HUC12 Catchment Outlets: {cats}')\n",
    "        print (f'  Projection {arcpy.Describe(cats).spatialReference.name}')\n",
    "        print ('----------')\n",
    "        print (f'Elevation Raster: {elev_rast}')\n",
    "        print (f'  Projection: {arcpy.Describe(elev_rast).spatialReference.name}')\n",
    "        print ('----------')\n",
    "        print (f'North Aspect Raster: {nor_rast}')\n",
    "        print (f'  Projection: {arcpy.Describe(nor_rast).spatialReference.name}')\n",
    "        print ('----------')\n",
    "        print (f'Wetlands Raster: {wet_rast}')\n",
    "        print (f'  Projection {arcpy.Describe(wet_rast).spatialReference.name}')\n",
    "        print ('----------')\n",
    "        print (f'Slope Raster: {slope_rast}')\n",
    "        print (f'  Projection {arcpy.Describe(slope_rast).spatialReference.name}')\n",
    "        print ('----------')\n",
    "        print (f'Lakes Ponds fc: {lakes_fc}')\n",
    "        print (f'  Projection {arcpy.Describe(lakes_fc).spatialReference.name}')\n",
    "        print ('----------')\n",
    "        print (f'Glaciers fc: {glac_fc} ')\n",
    "        print (f'  Projection {arcpy.Describe(glac_fc).spatialReference.name}')\n",
    "        print ('----------')\n",
    "        print (f'{arcpy.GetCount_management(wtd_merge)} Watersheds to process')\n",
    "        print ('----------')\n",
    "        print (f'Catchment intersect {cats} selected')\n",
    "        print ('----------')\n",
    "\n",
    "        # # Aspect variables\n",
    "        # wtd_merge_asp_table_name = roi + \"awc_huc12_Watersheds_Merge_AspectZstats\"\n",
    "        # wtd_merge_asp_table_path = os.path.join(outgdb, wtd_merge_asp_table_name)\n",
    "        # cat_asp_table_name = roi + \"awc_huc12_Catchments_AspectZstats\"\n",
    "        # cat_asp_table_path = os.path.join(outgdb, cat_asp_table_name)\n",
    "\n",
    "        # Percent North variables\n",
    "        wtd_merge_pernorth_table_name = roi + \"_NhdAwcH12_Watersheds_Merge_PercentNorth\"\n",
    "        wtd_merge_pernorth_table_path = os.path.join(outgdb, wtd_merge_pernorth_table_name)\n",
    "        # cat_pernorth_table_name = roi + \"awc_huc12_Catchments_PercentNorth\"\n",
    "        # cat_pernorth_table_path = os.path.join(outgdb, cat_pernorth_table_name)\n",
    "\n",
    "        # Elevation variables\n",
    "        wtd_merge_elev_table_name = roi + \"_NhdAwcH12_Watersheds_Merge_ElevZstats\"\n",
    "        wtd_merge_elev_table_path = os.path.join(outgdb, wtd_merge_elev_table_name)\n",
    "        cat_elev_table_name = roi + \"_NhdAwcH12_Catchments_ElevZstats\"\n",
    "        cat_elev_table_path = os.path.join(outgdb, cat_elev_table_name)\n",
    "\n",
    "        # Slope variables\n",
    "        wtd_merge_slope_table_name = roi + \"_NhdAwcH12_Watershed_Merge_SlopeZstats\"\n",
    "        wtd_merge_slope_table_path = os.path.join(outgdb, wtd_merge_slope_table_name)\n",
    "        cat_slope_table_name = roi + \"_NhdAwcH12_Catchments_SlopeZstats\"\n",
    "        cat_slope_table_path = os.path.join(outgdb, cat_slope_table_name)\n",
    "\n",
    "        # Lakes Ponds variables\n",
    "        wtd_merge_lp_table_name = roi + \"_NhdAwcH12_Watershed_Merge_LakesPonds\"\n",
    "        wtd_merge_lp_table_path = os.path.join(outgdb, wtd_merge_lp_table_name)\n",
    "        cat_lp_table_name = roi + \"_NhdAwcH12_Catchments_LakesPonds\"\n",
    "        cat_lp_path = os.path.join(outgdb, cat_lp_table_name)\n",
    "\n",
    "        # Wetlands variables\n",
    "        wtd_merge_wetlands_table_name = roi + \"_NhdAwcH12_Watershed_Merge_Wetlands\"\n",
    "        wtd_merge_wetlands_table_path = os.path.join(outgdb, wtd_merge_wetlands_table_name)\n",
    "        cat_wetlands_table_name = roi + \"NhdAwcH12_Catchments_Wetlands\"\n",
    "        cat_wetlands_table_path = os.path.join(outgdb, cat_wetlands_table_name)\n",
    "\n",
    "        # Glaciers\n",
    "        wtd_merge_glac_table_name = roi + \"NhdAwcH12_Watershed_Merge_Glaciers\"\n",
    "        wtd_merge_glac_table_path = os.path.join(outgdb, wtd_merge_glac_table_name)\n",
    "        cat_glac_table_name = roi + \"_NhdAwcH12_Catchments_Glaciers\"\n",
    "        cat_glac_table_path = os.path.join(outgdb, cat_glac_table_name)\n",
    "\n",
    "        try: # Zonal Stats section\n",
    "\n",
    "            # Begin Zonal Stats\n",
    "            zstat_start = time.time()\n",
    "            print(f'Begin Elevation zonal statistics min/mean/max std dev for watersheds and catchments in {roi}'\n",
    "                  f' region')\n",
    "\n",
    "            # Elevation Zonal statistics  for watersheds\n",
    "            print(f'Calculating {roi} watershed elevation zonal stats...')\n",
    "            arcpy.env.snapRaster = elev_rast\n",
    "            arcpy.env.cellSize = elev_rast\n",
    "            wtd_elev_metrics_table = ZonalStatisticsAsTable(in_zone_data = wtd_merge,\n",
    "                                                            zone_field = wtd_cur_fields[0],\n",
    "                                                            in_value_raster = elev_rast,\n",
    "                                                            out_table = wtd_merge_elev_table_path,\n",
    "                                                            statistics_type='ALL'\n",
    "                                                            )\n",
    "            # Add region identifier field for watershed tables                                                )\n",
    "            arcpy.AddField_management(wtd_elev_metrics_table,'region',field_type='TEXT')\n",
    "            # Add cat_ID_Con field\n",
    "            arcpy.AddField_management(wtd_elev_metrics_table,'cat_ID_con',field_type='TEXT')\n",
    "            # Update region field\n",
    "            with arcpy.da.UpdateCursor(wtd_elev_metrics_table,['region','cat_ID_txt','cat_ID_con']) as cur:\n",
    "                for row in cur:\n",
    "                    row[0] = roi\n",
    "                    strval = str(row[1])\n",
    "                    row[2] = roi+\"_\"+strval.replace(\".0\",\"\")\n",
    "                    # Update\n",
    "                    cur.updateRow(row)\n",
    "                del(row)\n",
    "            del(cur)\n",
    "            # Append watershed elev table to list\n",
    "            wtd_elev_ztables.append(wtd_elev_metrics_table)\n",
    "\n",
    "            # Elevation zonal statistics for catchments\n",
    "            print(f'Calculating {roi} catchment elevation zonal stats...')\n",
    "            arcpy.env.snapRaster = elev_rast\n",
    "            arcpy.env.cellSize = elev_rast\n",
    "            cat_elev_metrics_table = ZonalStatisticsAsTable(in_zone_data = cats ,\n",
    "                                                            zone_field = cat_cur_fields[0],\n",
    "                                                            in_value_raster = elev_rast,\n",
    "                                                            out_table = cat_elev_table_path,\n",
    "                                                            statistics_type='ALL'\n",
    "                                                            )\n",
    "            # Add region identifier field for catchment table\n",
    "            arcpy.AddField_management(cat_elev_metrics_table,'region',field_type='TEXT')\n",
    "            # Add cat_ID_Con field\n",
    "            arcpy.AddField_management(cat_elev_metrics_table,'cat_ID_con',field_type='TEXT')\n",
    "\n",
    "            # Update fields\n",
    "            with arcpy.da.UpdateCursor(cat_elev_metrics_table,['region','cat_ID_txt','cat_ID_con']) as cur:\n",
    "                for row in cur:\n",
    "                    row[0] = roi\n",
    "                    strval = str(row[1])\n",
    "                    row[2] = roi+\"_\"+strval.replace(\".0\",\"\")\n",
    "                    # Update\n",
    "                    cur.updateRow(row)\n",
    "                del(row)\n",
    "            del(cur)\n",
    "            # Append catchment elev table to list\n",
    "            cat_elev_ztables.append(cat_elev_metrics_table)\n",
    "\n",
    "            # Slope zonal statistics for catchments\n",
    "            print(f'Calculating {roi} catchment slope zonal stats...')\n",
    "            arcpy.env.snapRaster = slope_rast\n",
    "            arcpy.env.cellSize = slope_rast\n",
    "            cat_slope_metrics_table = ZonalStatisticsAsTable(in_zone_data = cats ,\n",
    "                                                            zone_field = cat_cur_fields[0],\n",
    "                                                            in_value_raster = slope_rast,\n",
    "                                                            out_table = cat_slope_table_path,\n",
    "                                                            statistics_type='ALL'\n",
    "                                                            )\n",
    "            # Add region identifier field for catchment table\n",
    "            arcpy.AddField_management(cat_slope_metrics_table,'region',field_type='TEXT')\n",
    "            # Add cat_ID_Con field\n",
    "            arcpy.AddField_management(cat_slope_metrics_table,'cat_ID_con',field_type='TEXT')\n",
    "\n",
    "            # Update region field\n",
    "            with arcpy.da.UpdateCursor(cat_slope_metrics_table,['region','cat_ID_txt','cat_ID_con']) as cur:\n",
    "                for row in cur:\n",
    "                    row[0] = roi\n",
    "                    strval =str(row[1])\n",
    "                    row[2] = roi+\"_\"+strval.replace(\".0\",\"\")\n",
    "                    # Update\n",
    "                    cur.updateRow(row)\n",
    "                del(row)\n",
    "            del(cur)\n",
    "            # Append catchment slope table to list\n",
    "            cat_slope_ztables.append(cat_slope_metrics_table)\n",
    "\n",
    "            # # Watershed slope Zonal Statistics\n",
    "            # print(f'Begin Slope zonal statistics min/mean/max std dev for watersheds and catchments in {roi}'\n",
    "            #       f' region')\n",
    "            #\n",
    "            # # Slope Zonal statistics  for watersheds\n",
    "            # print(f'Calculating {roi} watershed slope zonal stats...')\n",
    "            # arcpy.env.snapRaster = slope_rast\n",
    "            # arcpy.env.cellSize = slope_rast\n",
    "            # wtd_slope_metrics_table = ZonalStatisticsAsTable(in_zone_data = wtd_merge,\n",
    "            #                                                 zone_field = wtd_cur_fields[0],\n",
    "            #                                                 in_value_raster = slope_rast,\n",
    "            #                                                 out_table = wtd_merge_slope_table_path,\n",
    "            #                                                 statistics_type='ALL'\n",
    "            #                                                 )\n",
    "            # # Add region identifier field for watershed tables                                                )\n",
    "            # arcpy.AddField_management(wtd_slope_metrics_table,'region',field_type='TEXT')\n",
    "            # # Add cat_ID_Con field\n",
    "            # arcpy.AddField_management(wtd_slope_metrics_table,'cat_ID_con',field_type='TEXT')\n",
    "            #\n",
    "            # # Update region field\n",
    "            # with arcpy.da.UpdateCursor(wtd_slope_metrics_table,['region','cat_ID_txt','cat_ID_con']) as cur:\n",
    "            #     for row in cur:\n",
    "            #         row[0] = roi\n",
    "            #         strval = str(row[1])\n",
    "            #         row[2] = roi+\"_\"+strval.replace(\".0\",\"\")\n",
    "            #         # Update\n",
    "            #         cur.updateRow(row)\n",
    "            #     del(row)\n",
    "            # del(cur)\n",
    "            # # Append watershed slope table to list\n",
    "            # wtd_slope_ztables.append(wtd_slope_metrics_table)\n",
    "\n",
    "            # # Aspect Zonal statistics  for watersheds\n",
    "            # print(f'Calculating {roi} watershed aspect zonal stats...')\n",
    "            # wtd_asp_metrics_table = ZonalStatisticsAsTable(in_zone_data = wtd_merge, zone_field =\"cat_ID_txt\",\n",
    "            #                                                in_value_raster = asp_rast, out_table = wtd_merge_asp_table_path,\n",
    "            #                                                statistics_type='ALL')\n",
    "            # arcpy.AddField_management(wtd_asp_metrics_table, 'region', field_type='TEXT')\n",
    "            # Add cat_ID_Con field\n",
    "            # arcpy.AddField_management(wtd_asp_metrics_table,'cat_ID_con',field_type='TEXT')\n",
    "            # arcpy.CalculateField_management(wtd_asp_metrics_table, 'region', 'roi')\n",
    "            # Update region field\n",
    "            # with arcpy.da.UpdateCursor(wtd_asp_metrics_table,['region','cat_ID_txt','cat_ID_con']) as cur:\n",
    "            #     for row in cur:\n",
    "            #         row[0] = roi\n",
    "            #         strval = str(row[1])\n",
    "            #         row[2] = roi+\"_\"+strval.replace(\".0\",\"\")\n",
    "            #         # Update\n",
    "            #         cur.updateRow(row)\n",
    "            #     del(row)\n",
    "            # del(cur)\n",
    "            # wtd_asp_ztables.append(wtd_asp_metrics_table)\n",
    "\n",
    "            # # Aspect Zonal statistics for catchments\n",
    "            # print(f'Calculating {roi} catchment aspect zonal stats...')\n",
    "            # cat_asp_metrics_table = ZonalStatisticsAsTable(in_zone_data = cats, zone_field =\"cat_ID_txt\",\n",
    "            #                                                in_value_raster = asp_rast, out_table = cat_asp_table_path,\n",
    "            #                                                statistics_type='ALL')\n",
    "            # arcpy.AddField_management(cat_asp_metrics_table, 'region', field_type='TEXT')\n",
    "            # Add cat_ID_Con field\n",
    "            # arcpy.AddField_management(cat_asp_metrics_table,'cat_ID_con',field_type='TEXT')\n",
    "            # arcpy.CalculateField_management(cat_asp_metrics_table, 'region', 'roi')\n",
    "            # Update region field\n",
    "            # with arcpy.da.UpdateCursor(cat_asp_metrics_table,['region','cat_ID_txt','cat_ID_con']) as cur:\n",
    "            #     for row in cur:\n",
    "            #         strval = str(row[1])\n",
    "            #         row[2] = roi+\"_\"+strval.replace(\".0\",\"\")\n",
    "            #         # Update\n",
    "            #         cur.updateRow(row)\n",
    "            #     del(row)\n",
    "            # del(cur)\n",
    "            # cat_asp_ztables.append(cat_asp_metrics_table)\n",
    "\n",
    "            zstat_stop = time.time()\n",
    "            zstat_time = int (zstat_stop - zstat_start)\n",
    "            print(f'Slope Zonal Stats for {roi} Elapsed time: ({datetime.timedelta(seconds=zstat_time)})')\n",
    "            print(f'{\"*\"*100}')\n",
    "\n",
    "            # Tabulate Area with north grid and watersheds\n",
    "            tabarea_start = time.time()\n",
    "            print(f'Begin tabulate area of north facing cells for watersheds and catchments in {roi} region')\n",
    "            print(f'{\"*\"*100}')\n",
    "            # Percent North Tabulate area for watersheds\n",
    "            wtd_per_north_tabarea = arcpy.sa.TabulateArea(in_zone_data= wtd_merge,\n",
    "                                                          zone_field= wtd_cur_fields[0],\n",
    "                                                          in_class_data=nor_rast,\n",
    "                                                          class_field=\"Value\",\n",
    "                                                          out_table = wtd_merge_pernorth_table_path\n",
    "                                                          )\n",
    "            # Add region and percent north fields\n",
    "            arcpy.AlterField_management(wtd_per_north_tabarea,'CAT_ID_TXT','CAT_ID_TXT_DEL','CAT_ID_TXT_DEL')\n",
    "            arcpy.AddField_management(wtd_per_north_tabarea, 'region', field_type='TEXT')\n",
    "            arcpy.AddField_management(wtd_per_north_tabarea, 'NhdAwcH12_wtd_north_per', field_type='Float')\n",
    "            arcpy.AddField_management(wtd_per_north_tabarea, wtd_cur_fields[0], field_type='TEXT')\n",
    "            arcpy.AddField_management(wtd_per_north_tabarea, wtd_cur_fields[2], field_type='TEXT')\n",
    "            wtdnorfields = [f.name for f in arcpy.ListFields(wtd_per_north_tabarea)]\n",
    "            #print (wtdnorfields)\n",
    "            with arcpy.da.UpdateCursor(wtd_per_north_tabarea, wtdnorfields) as cur:\n",
    "                for row in cur:\n",
    "                    strval = str(row[1])\n",
    "                    row[4] = roi\n",
    "                    row[5] = row[3]/(row[3]+row[2])*100\n",
    "                    row[6] = strval.replace('.0','')\n",
    "                    row[7] = roi +'_'+ strval.replace(\".0\",\"\")\n",
    "                    # Update\n",
    "                    cur.updateRow(row)\n",
    "                del(row)\n",
    "            del(cur)\n",
    "            # Drop UPPERCASE field form tab area\n",
    "            arcpy.DeleteField_management(wtd_per_north_tabarea,'CAT_ID_TXT_DEL')\n",
    "            # Append watershed percent north table to list\n",
    "            wtd_pernorth_taba_tables.append(wtd_per_north_tabarea)\n",
    "\n",
    "            # Percent Lakes Ponds using Tabulate Intersection for watersheds\n",
    "            print(f'Begin tabulate intersection between {lakes_fc} and watersheds in {roi} region')\n",
    "            print(f'{\"*\"*100}')\n",
    "            wtd_lp_tabint = arcpy.TabulateIntersection_analysis(wtd_merge,\n",
    "                                                                zone_fields=wtd_cur_fields[0],\n",
    "                                                                in_class_features=lakes_fc,\n",
    "                                                                out_table=wtd_merge_lp_table_path,\n",
    "                                                                class_fields='Ftype',\n",
    "                                                                out_units=\"SQUARE_METERS\"\n",
    "                                                                )\n",
    "            # Add region and cat id fields\n",
    "            arcpy.AlterField_management(wtd_lp_tabint,'PERCENTAGE','NhdAwcH12_wtd_lake_per','NhdAwcH12_wtd_lake_per')\n",
    "            arcpy.AlterField_management(wtd_lp_tabint,'AREA','NhdAwcH12_wtd_lake_area_sqm','NhdAwcH12_wtd_lake_area_sqm')\n",
    "            arcpy.AddField_management(wtd_lp_tabint, 'region', field_type='TEXT')\n",
    "            arcpy.AddField_management(wtd_lp_tabint, wtd_cur_fields[1], field_type='TEXT')\n",
    "            arcpy.AddField_management(wtd_lp_tabint, wtd_cur_fields[2], field_type='TEXT')\n",
    "            wtdlpfields = [f.name for f in arcpy.ListFields(wtd_lp_tabint)]\n",
    "            #print (wtdlpfields)\n",
    "            with arcpy.da.UpdateCursor(wtd_lp_tabint, wtdlpfields) as cur:\n",
    "                for row in cur:\n",
    "                    strval = str(row[1])\n",
    "                    row[5] = roi\n",
    "                    row[6] = strval.replace('.0','')\n",
    "                    row[7] = roi +'_'+ strval.replace(\".0\",\"\")\n",
    "                    # Update\n",
    "                    cur.updateRow(row)\n",
    "                del(row)\n",
    "            del(cur)\n",
    "\n",
    "            # Append watershed lakes ponds table to list\n",
    "            wtd_lp_tabint_tables.append(wtd_lp_tabint)\n",
    "\n",
    "            # Percent glaciers using Tabulate Intersection for watersheds\n",
    "            print(f'Begin tabulate intersection between {glac_fc} and watersheds in {roi} region')\n",
    "            print(f'{\"*\"*100}')\n",
    "            wtd_glac_tabint = arcpy.TabulateIntersection_analysis(wtd_merge,\n",
    "                                                                zone_fields=wtd_cur_fields[0],\n",
    "                                                                in_class_features=glac_fc,\n",
    "                                                                out_table=wtd_merge_glac_table_path,\n",
    "                                                                class_fields='O1Region',\n",
    "                                                                out_units=\"SQUARE_METERS\"\n",
    "                                                                )\n",
    "            # Add region and cat id fields\n",
    "            arcpy.AlterField_management(wtd_glac_tabint,'PERCENTAGE','NhdAwcH12_wtd_glacier_per','NhdAwcH12_wtd_glacier_per')\n",
    "            arcpy.AlterField_management(wtd_glac_tabint,'AREA','NhdAwcH12_wtd_glacier_area_sqm','NhdAwcH12_wtd_glacier_area_sqm')\n",
    "            arcpy.AddField_management(wtd_glac_tabint, 'region', field_type='TEXT')\n",
    "            arcpy.AddField_management(wtd_glac_tabint, wtd_cur_fields[1], field_type='TEXT')\n",
    "            arcpy.AddField_management(wtd_glac_tabint, wtd_cur_fields[2], field_type='TEXT')\n",
    "            wtdglacfields = [f.name for f in arcpy.ListFields(wtd_glac_tabint)]\n",
    "            #print (wtdglacfields)\n",
    "            with arcpy.da.UpdateCursor(wtd_glac_tabint, wtdglacfields) as cur:\n",
    "                for row in cur:\n",
    "                    strval = str(row[1])\n",
    "                    row[5] = roi\n",
    "                    row[6] = strval.replace('.0','')\n",
    "                    row[7] = roi +'_'+ strval.replace(\".0\",\"\")\n",
    "                    # Update\n",
    "                    cur.updateRow(row)\n",
    "                del(row)\n",
    "            del(cur)\n",
    "            # Append watershed percent glacier table to list\n",
    "            wtd_glac_tabint_tables.append(wtd_glac_tabint)\n",
    "\n",
    "            # Tabulate Area with wetlands grid and watersheds\n",
    "            print(f'Begin tabulate intersection between {wet_rast} and watersheds in {roi} region')\n",
    "            print(f'{\"*\"*100}')\n",
    "            # Wetlands tabulate area for watersheds\n",
    "            wtd_per_wet_tabarea = arcpy.sa.TabulateArea(in_zone_data= wtd_merge,\n",
    "                                                          zone_field= wtd_cur_fields[0],\n",
    "                                                          in_class_data=wet_rast,\n",
    "                                                          class_field=\"Value\",\n",
    "                                                          out_table=wtd_merge_wetlands_table_path\n",
    "                                                          )\n",
    "            # Add region and percent wet fields\n",
    "            arcpy.AlterField_management(wtd_per_wet_tabarea,'CAT_ID_TXT','CAT_ID_TXT_DEL','CAT_ID_TXT_DEL')\n",
    "            arcpy.AddField_management(wtd_per_wet_tabarea, 'region', field_type='TEXT')\n",
    "            arcpy.AddField_management(wtd_per_wet_tabarea, 'NhdAwcH12_wtd_wet_per', field_type='Float')\n",
    "            arcpy.AddField_management(wtd_per_wet_tabarea, wtd_cur_fields[0], field_type='TEXT')\n",
    "            arcpy.AddField_management(wtd_per_wet_tabarea, wtd_cur_fields[2], field_type='TEXT')\n",
    "            wtdwetfields = [f.name for f in arcpy.ListFields(wtd_per_wet_tabarea)]\n",
    "            #print (wtdwetfields)\n",
    "            with arcpy.da.UpdateCursor(wtd_per_wet_tabarea, wtdwetfields) as cur:\n",
    "                for row in cur:\n",
    "                    strval = str(row[1])\n",
    "                    row[4] = roi\n",
    "                    row[5] = row[3]/(row[3]+row[2])*100\n",
    "                    row[6] = strval.replace('.0','')\n",
    "                    row[7] = roi +'_'+ strval.replace(\".0\",\"\")\n",
    "                    # Update\n",
    "                    cur.updateRow(row)\n",
    "                del(row)\n",
    "            del(cur)\n",
    "            # Drop UPPERCASE field form tab area\n",
    "            arcpy.DeleteField_management(wtd_per_wet_tabarea,'CAT_ID_TXT_DEL')\n",
    "            # Append watershed percent wetlands table to list\n",
    "            wtd_wet_taba_tables.append(wtd_per_wet_tabarea)\n",
    "\n",
    "            # # Percent North Tabulate Area for catchments\n",
    "            # cat_per_north_tabarea = arcpy.sa.TabulateArea(in_zone_data= cats, zone_field='cat_ID_con',\n",
    "            #                                             in_class_data=nor_rast,\"Value\",\n",
    "            #                                             out_table=cat_pernorth_table_path)\n",
    "\n",
    "            # # Add and calculate region identifier field for catchment table\n",
    "            # arcpy.AlterField_management(cat_per_north_tabarea,'CAT_ID_TXT','CAT_ID_TXT_DEL','CAT_ID_TXT_DEL')\n",
    "            # arcpy.AddField_management(cat_per_north_tabarea, 'region', field_type='TEXT')\n",
    "            # arcpy.AddField_management(cat_per_north_tabarea, 'cat_north_per', field_type='Float')\n",
    "            # arcpy.AddField_management(cat_per_north_tabarea, cat_cur_fields[0], field_type='TEXT')\n",
    "            # arcpy.AddField_management(cat_per_north_tabarea, cat_cur_fields[2], field_type='TEXT')\n",
    "            # catnorfields = [f.name for f in arcpy.ListFields(cat_per_north_tabarea)]\n",
    "            # print (catnorfields)\n",
    "            # with arcpy.da.UpdateCursor(cat_per_north_tabarea,catnorfields) as cur:\n",
    "            #     for row in cur:\n",
    "            #         strval = str(row[1])\n",
    "            #         row[4] = roi\n",
    "            #         row[5] = row[3]/(row[3]+row[2])*100\n",
    "            #         row[6] = strval.replace('.0','')\n",
    "            #         row[7] = roi +'_'+ strval.replace(\".0\",\"\")\n",
    "            #         # Update\n",
    "            #         cur.updateRow(row)\n",
    "            #     del(row)\n",
    "            # del(cur)\n",
    "            # Drop UPPERCASE field form tab area\n",
    "            # arcpy.DeleteField_management(cat_per_north_tabarea,'CAT_ID_TXT_DEL')\n",
    "            # # Append catchment percent north table to list\n",
    "            # cat_pernorth_taba_tables.append(cat_per_north_tabarea)\n",
    "\n",
    "            tabarea_stop = time.time()\n",
    "            tabarea_time = int (tabarea_stop - tabarea_start)\n",
    "            print(f'Tabulate area/intersections for {roi} Elapsed time: ({datetime.timedelta(seconds=tabarea_time)})')\n",
    "            print(f'{\"*\"*100}')\n",
    "\n",
    "            # Begin LCLD calculations\n",
    "            walk = arcpy.da.Walk(lcld_folder, datatype='RasterDataset')\n",
    "            for dirpath, dirnames, filenames in walk:\n",
    "                for filename in filenames:\n",
    "                    raspath = os.path.join(dirpath, filename)\n",
    "                    year = filename[0:4]\n",
    "                    lcld_outname = roi+'NhdAwcH12_lcld_'+str(year)+'_zStats'\n",
    "                    lcld_outpath = os.path.join(outgdb, lcld_outname)\n",
    "                    print(f'Year: {year} - raster path {raspath}')\n",
    "                    colname = 'NhdAwcH12_wtd_lcld_mn_' + str(year)\n",
    "                    # lcld zonal statistics as table for all akssf watersheds\n",
    "                    print(f'Calculating {filename} zonal stats for all {roi} watersheds...')\n",
    "                    #arcpy.env.snapRaster = raspath\n",
    "                    #arcpy.env.cellSize = raspath\n",
    "\n",
    "                    # Begin Zonal Stats\n",
    "                    zstat_start = time.time()\n",
    "                    print(f'Begin zonal stats for {filename}')\n",
    "                    lcld_table = ZonalStatisticsAsTable(in_zone_data = wtd_merge,\n",
    "                                                                    zone_field = 'cat_ID_con',\n",
    "                                                                    in_value_raster = raspath,\n",
    "                                                                    out_table = lcld_outpath,\n",
    "                                                                    statistics_type='MEAN'\n",
    "                                                                    )\n",
    "                    # Append zTable to table list\n",
    "                    lcld_Ztables.append(lcld_outpath)\n",
    "                    arcpy.AlterField_management(lcld_table,'MEAN', colname,colname)\n",
    "                    proc_list = [row[0] for row in arcpy.da.SearchCursor(lcld_table,'cat_ID_con')]\n",
    "                    zstat_stop = time.time()\n",
    "                    zstat_time = int (zstat_stop - zstat_start)\n",
    "                    print(f'Zonal Stats for {filename} Elapsed time: ({datetime.timedelta(seconds=zstat_time)})')\n",
    "\n",
    "        except:\n",
    "            e = sys.exc_info()[1]\n",
    "            print(f'ERRFLAG!!! = {e.args[0]}')\n",
    "            arcpy.AddError(e.args[0])\n",
    "\n",
    "            iter_stop = time.time()\n",
    "            iter_time = int(iter_stop - iteration_start)\n",
    "            print(f'All Covariates for {roi} completed. Elapsed time: ({datetime.timedelta(seconds=iter_time)})')\n",
    "            print(f'{\"*\"*100}')\n",
    "\n",
    "    else:\n",
    "        print(f'Region {str(roi)} not found in {region}')\n",
    "\n",
    "# End timing\n",
    "processEnd = time.time()\n",
    "processElapsed = int(processEnd - processStart)\n",
    "processSuccess_time = datetime.datetime.now()\n",
    "\n",
    "# Report success\n",
    "print(f'Process completed at {processSuccess_time.strftime(\"%Y-%m-%d %H:%M\")} '\n",
    "      f'(Elapsed time: {datetime.timedelta(seconds=processElapsed)})')\n",
    "print(f'{\"*\"*100}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lists to store output tables\n",
    "print(wtd_pernorth_taba_tables[0])\n",
    "print(wtd_lp_tabint_tables[0])\n",
    "print(wtd_glac_tabint_tables[0])\n",
    "print(wtd_wet_taba_tables[0])\n",
    "print(cat_elev_ztables[0])\n",
    "print(wtd_elev_ztables[0])\n",
    "print(cat_slope_ztables[0])\n",
    "print(wtd_slope_ztables[0])\n",
    "print(lcld_Ztables)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Examine LCLD tables and merge/export\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for table in lcld_Ztables:\n",
    "    tblname = table[-16:]\n",
    "    print(tblname)\n",
    "    dfname = tblname + '_arr'\n",
    "    # Make df\n",
    "    dfname = pd.DataFrame()\n",
    "    lcld_field_list = []\n",
    "    for field in arcpy.ListFields(table):\n",
    "        lcld_field_list.append(field.name)\n",
    "        #print(f'{field.name}')\n",
    "    lcld_arr = arcpy.da.TableToNumPyArray(table, lcld_field_list)\n",
    "    dfname = pd.DataFrame(lcld_arr)\n",
    "    dfname = dfname.drop(['OBJECTID','ZONE_CODE', 'AREA', 'COUNT'],axis=1)\n",
    "    dfname = dfname.set_index('cat_ID_con')\n",
    "    dfs.append(dfname)\n",
    "\n",
    "# Merge all data frames together\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "lcld_df = reduce(lambda left,right: pd.merge(left,right,on='cat_ID_con',how=\"outer\"), dfs)\n",
    "lcld_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export merged dataframe to csv\n",
    "lcld_csv_out = os.path.join(outdir,'AKSSF_NHDPlus_AWC_HUC12_wtd_lcld_mn.csv')\n",
    "lcld_df.to_csv(lcld_csv_out, encoding = 'utf-8')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Drop unnecessary fields and rename as needed from merged tables.\n",
    "- Create Key value dictionary and use update cursor to rename fields."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Table names/paths\n",
    "wtd_per_north_table_out = os.path.join(outgdb, 'AKSSF_NHDPlus_awc_huc12_wtd_north_per')\n",
    "cat_elev_table_out = os.path.join(outgdb,'AKSSF_NHDPlus_awc_huc12_cat_elev')\n",
    "cat_slope_table_out = os.path.join(outgdb,'AKSSF_NHDPlus_awc_huc12_cat_slope')\n",
    "wtd_elev_table_out = os.path.join(outgdb, 'AKSSF_NHDPlus_awc_huc12_wtd_elev')\n",
    "wtd_per_glac_table_out = os.path.join(outgdb, 'AKSSF_NHDPlus_awc_huc12_wtd_glacier_per')\n",
    "wtd_per_lp_table_out = os.path.join(outgdb, 'AKSSF_NHDPlus_awc_huc12_wtd_lakepond_per')\n",
    "wtd_slope_table_out = os.path.join(outgdb, 'AKSSF_NHDPlus_awc_huc12_wtd_slope')\n",
    "wtd_wet_table_out = os.path.join(outgdb, 'AKSSF_NHDPlus_awc_huc12_wtd_wetland_per')\n",
    "\n",
    "# Merge all regional tables together\n",
    "outtables = []\n",
    "wtd_per_north = arcpy.Merge_management(wtd_pernorth_taba_tables, wtd_per_north_table_out)\n",
    "arcpy.AlterField_management(wtd_per_north,\"VALUE_0\",\"NhdAwcH12__non_north_area\",\"NhdAwcH12__non_north_area\")\n",
    "arcpy.AlterField_management(wtd_per_north,\"VALUE_1\",\"NhdAwcH12__north_area\",\"NhdAwcH12__north_area\")\n",
    "outtables.append(wtd_per_north)\n",
    "cat_elev = arcpy.Merge_management(cat_elev_ztables, cat_elev_table_out)\n",
    "outtables.append(cat_elev)\n",
    "wtd_elev = arcpy.Merge_management(wtd_elev_ztables, wtd_elev_table_out)\n",
    "outtables.append(wtd_elev)\n",
    "wtd_slope = arcpy.Merge_management(wtd_slope_ztables, wtd_slope_table_out)\n",
    "outtables.append(wtd_slope)\n",
    "cat_slope = arcpy.Merge_management(cat_slope_ztables, cat_slope_table_out)\n",
    "outtables.append(cat_slope)\n",
    "wtd_wet = arcpy.Merge_management(wtd_wet_taba_tables, wtd_wet_table_out)\n",
    "arcpy.AlterField_management(wtd_wet,\"VALUE_0\",\"NhdAwcH12__non_wetland_area\",\"NhdAwcH12__non_wetland_area\")\n",
    "arcpy.AlterField_management(wtd_wet,\"VALUE_1\",\"NhdAwcH12__wetland_area\",\"NhdAwcH12__wetland_area\")\n",
    "outtables.append(wtd_wet)\n",
    "wtd_glac = arcpy.Merge_management(wtd_glac_tabint_tables, wtd_per_glac_table_out)\n",
    "outtables.append(wtd_glac)\n",
    "wtd_lp = arcpy.Merge_management(wtd_lp_tabint_tables, wtd_per_lp_table_out)\n",
    "outtables.append(wtd_lp)\n",
    "print ('Tables merged')\n",
    "print('----------')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Set up field dictionary\n",
    "elevDict = { 'ZONE_CODE': ('NhdAwcH12__cat_elev_ZONE_CODE', 'NhdAwcH12__wtd_elev_ZONE_CODE'),\n",
    "         'COUNT': ('NhdAwcH12__cat_elev_COUNT', 'NhdAwcH12__wtd_elev_COUNT'),\n",
    "          'AREA': ('NhdAwcH12__cat_elev_AREA', 'NhdAwcH12__wtd_elev_AREA'),\n",
    "          'MIN': ('NhdAwcH12__cat_elev_MIN', 'NhdAwcH12__wtd_elev_MIN'),\n",
    "          'MAX': ('NhdAwcH12__cat_elev_MAX', 'NhdAwcH12__wtd_elev_MAX'),\n",
    "          'RANGE': ('NhdAwcH12__cat_elev_RANGE', 'NhdAwcH12__wtd_elev_RANGE'),\n",
    "          'MEAN': ('NhdAwcH12__cat_elev_MEAN', 'NhdAwcH12__wtd_elev_MEAN'),\n",
    "          'STD': ('NhdAwcH12__cat_elev_STD', 'NhdAwcH12__wtd_elev_STD'),\n",
    "          'SUM': ('NhdAwcH12__cat_elev_SUM', 'NhdAwcH12__wtd_elev_SUM'),\n",
    "          'VARIETY': ('NhdAwcH12__cat_elev_VARIETY', 'NhdAwcH12__wtd_elev_VARIETY'),\n",
    "          'MAJORITY': ('NhdAwcH12__cat_elev_MAJORITY', 'NhdAwcH12__wtd_elev_MAJORITY'),\n",
    "          'MINORITY': ('NhdAwcH12__cat_elev_MINORITY', 'NhdAwcH12__wtd_elev_MINORITY'),\n",
    "          'MEDIAN': ('NhdAwcH12__cat_elev_MEDIAN', 'NhdAwcH12__wtd_elev_MEDIAN'),\n",
    "          'PCT90': ('NhdAwcH12__cat_elev_PCT90', 'NhdAwcH12__wtd_elev_PCT90')\n",
    "         }\n",
    "\n",
    "slopeDict = { 'ZONE_CODE': ('NhdAwcH12__cat_slope_ZONE_CODE', 'NhdAwcH12__wtd_slope_ZONE_CODE'),\n",
    "         'COUNT': ('NhdAwcH12__cat_slope_COUNT', 'NhdAwcH12__wtd_slope_COUNT'),\n",
    "          'AREA': ('NhdAwcH12__cat_slope_AREA', 'NhdAwcH12__wtd_slope_AREA'),\n",
    "          'MIN': ('NhdAwcH12__cat_slope_MIN', 'NhdAwcH12__wtd_slope_MIN'),\n",
    "          'MAX': ('NhdAwcH12__cat_slope_MAX', 'NhdAwcH12__wtd_slope_MAX'),\n",
    "          'RANGE': ('NhdAwcH12__cat_slope_RANGE', 'NhdAwcH12__wtd_slope_RANGE'),\n",
    "          'MEAN': ('NhdAwcH12__cat_slope_MEAN', 'NhdAwcH12__wtd_slope_MEAN'),\n",
    "          'STD': ('NhdAwcH12__cat_slope_STD', 'NhdAwcH12__wtd_slope_STD'),\n",
    "          'SUM': ('NhdAwcH12__cat_slope_SUM', 'NhdAwcH12__wtd_slope_SUM'),\n",
    "          'VARIETY': ('NhdAwcH12__cat_slope_VARIETY', 'NhdAwcH12__wtd_slope_VARIETY'),\n",
    "          'MAJORITY': ('NhdAwcH12__cat_slope_MAJORITY', 'NhdAwcH12__wtd_slope_MAJORITY'),\n",
    "          'MINORITY': ('NhdAwcH12__cat_slope_MINORITY', 'NhdAwcH12__wtd_slope_MINORITY'),\n",
    "          'MEDIAN': ('NhdAwcH12__cat_slope_MEDIAN', 'NhdAwcH12__wtd_slope_MEDIAN'),\n",
    "          'PCT90': ('NhdAwcH12__cat_slope_PCT90', 'NhdAwcH12__wtd_slope_PCT90')\n",
    "         }\n",
    "\n",
    "# Rename fields for elevation tables\n",
    "for field in arcpy.ListFields(wtd_elev):\n",
    "    keyval = field.name\n",
    "    if keyval in elevDict:\n",
    "        newname = elevDict[keyval][1]\n",
    "        newalias = elevDict[keyval][1]\n",
    "        print (keyval, newname)\n",
    "        arcpy.AlterField_management(wtd_elev, keyval, newname, newalias)\n",
    "\n",
    "for field in arcpy.ListFields(cat_elev):\n",
    "    keyval = field.name\n",
    "    if keyval in elevDict:\n",
    "        newname = elevDict[keyval][0]\n",
    "        newalias = elevDict[keyval][0]\n",
    "        print (keyval, newname)\n",
    "        arcpy.AlterField_management(cat_elev, keyval, newname, newalias)\n",
    "\n",
    "# Rename fields for slope tables\n",
    "for field in arcpy.ListFields(wtd_slope):\n",
    "    keyval = field.name\n",
    "    if keyval in slopeDict:\n",
    "        newname = slopeDict[keyval][1]\n",
    "        newalias = slopeDict[keyval][1]\n",
    "        print (keyval, newname)\n",
    "        arcpy.AlterField_management(wtd_slope, keyval, newname, newalias)\n",
    "\n",
    "for field in arcpy.ListFields(cat_slope):\n",
    "    keyval = field.name\n",
    "    if keyval in slopeDict:\n",
    "        newname = slopeDict[keyval][0]\n",
    "        newalias = slopeDict[keyval][0]\n",
    "        print (keyval, newname)\n",
    "        arcpy.AlterField_management(cat_slope, keyval, newname, newalias)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Export copies of dbf tables as csv\n",
    "# for table in outtables:\n",
    "#     tablename = arcpy.Describe(table).basename + \".csv\"\n",
    "#     tablepath = os.path.join(outdir,tablename)\n",
    "#     print( tablepath)\n",
    "#     arcpy.conversion.TableToTable(table, outdir, tablename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.2f}'.format # only display 2 decimal places\n",
    "# list to store covariate data frames\n",
    "dfs = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make catchment elev df\n",
    "cat_df = pd.DataFrame()\n",
    "cat_field_list = []\n",
    "for field in arcpy.ListFields(cat_elev):\n",
    "    cat_field_list.append(field.name)\n",
    "cat_elev_arr = arcpy.da.TableToNumPyArray(cat_elev,cat_field_list)\n",
    "cat_df = pd.DataFrame(cat_elev_arr)\n",
    "cat_df = cat_df.drop([\"OBJECTID\",\"NhdAwcH12__cat_elev_ZONE_CODE\"],axis=1)\n",
    "cat_df = cat_df.set_index('cat_ID_con')\n",
    "dfs.append(cat_df)\n",
    "cat_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make catchment slope df\n",
    "cat_sl_df = pd.DataFrame()\n",
    "cat_sl_field_list = []\n",
    "for field in arcpy.ListFields(cat_slope):\n",
    "    cat_sl_field_list.append(field.name)\n",
    "cat_sl_arr = arcpy.da.TableToNumPyArray(cat_slope, cat_sl_field_list)\n",
    "cat_sl_df = pd.DataFrame(cat_sl_arr)\n",
    "cat_sl_df = cat_sl_df.drop([\"OBJECTID\", \"NhdAwcH12__cat_slope_ZONE_CODE\"],axis=1)\n",
    "cat_sl_df = cat_sl_df.set_index('cat_ID_con')\n",
    "dfs.append(cat_sl_df)\n",
    "cat_sl_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed elev df\n",
    "wtd_df = pd.DataFrame()\n",
    "wtd_field_list = []\n",
    "for field in arcpy.ListFields(wtd_elev):\n",
    "    wtd_field_list.append(field.name)\n",
    "wtd_elev_arr = arcpy.da.TableToNumPyArray(wtd_elev,wtd_field_list)\n",
    "wtd_df = pd.DataFrame(wtd_elev_arr)\n",
    "wtd_df = wtd_df.drop([\"OBJECTID\",\"NhdAwcH12__wtd_elev_ZONE_CODE\"],axis=1)\n",
    "wtd_df = wtd_df.set_index('cat_ID_con')\n",
    "dfs.append(wtd_df)\n",
    "wtd_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed slope df\n",
    "wtd_sl_df = pd.DataFrame()\n",
    "wtd_sl_field_list = []\n",
    "for field in arcpy.ListFields(wtd_slope):\n",
    "    wtd_sl_field_list.append(field.name)\n",
    "wtd_sl_arr = arcpy.da.TableToNumPyArray(wtd_slope, wtd_sl_field_list)\n",
    "wtd_sl_df = pd.DataFrame(wtd_sl_arr)\n",
    "wtd_sl_df = wtd_sl_df.drop([\"OBJECTID\", \"NhdAwcH12__wtd_slope_ZONE_CODE\"],axis=1)\n",
    "wtd_sl_df = wtd_sl_df.set_index('cat_ID_con')\n",
    "dfs.append(wtd_sl_df)\n",
    "wtd_sl_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed north df\n",
    "wtd_n_df = pd.DataFrame()\n",
    "wtd_n_field_list = []\n",
    "for field in arcpy.ListFields(wtd_per_north):\n",
    "    wtd_n_field_list.append(field.name)\n",
    "wtd_n_arr = arcpy.da.TableToNumPyArray(wtd_per_north,wtd_n_field_list)\n",
    "wtd_n_df = pd.DataFrame(wtd_n_arr)\n",
    "wtd_n_df = wtd_n_df.drop(\"OBJECTID\",axis=1)\n",
    "wtd_n_df = wtd_n_df.set_index('cat_ID_con')\n",
    "dfs.append(wtd_n_df)\n",
    "wtd_n_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed wetland df\n",
    "wtd_wet_df = pd.DataFrame()\n",
    "wtd_wet_field_list = []\n",
    "for field in arcpy.ListFields(wtd_wet):\n",
    "    wtd_wet_field_list.append(field.name)\n",
    "wtd_wet_arr = arcpy.da.TableToNumPyArray(wtd_wet,wtd_wet_field_list)\n",
    "wtd_wet_df = pd.DataFrame(wtd_wet_arr)\n",
    "wtd_wet_df = wtd_wet_df.drop(\"OBJECTID\",axis=1)\n",
    "wtd_wet_df = wtd_wet_df.set_index('cat_ID_con')\n",
    "dfs.append(wtd_wet_df)\n",
    "wtd_wet_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed lakes df\n",
    "wtd_lp_df = pd.DataFrame()\n",
    "wtd_lp_field_list = []\n",
    "for field in arcpy.ListFields(wtd_lp):\n",
    "    wtd_lp_field_list.append(field.name)\n",
    "wtd_lp_arr = arcpy.da.TableToNumPyArray(wtd_lp, wtd_lp_field_list)\n",
    "wtd_lp_df = pd.DataFrame(wtd_lp_arr)\n",
    "wtd_lp_df = wtd_lp_df.drop(\"OBJECTID\",axis=1)\n",
    "wtd_lp_df = wtd_lp_df.set_index('cat_ID_con')\n",
    "dfs.append(wtd_lp_df)\n",
    "wtd_lp_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed glacier df\n",
    "wtd_glac_df = pd.DataFrame()\n",
    "wtd_glac_field_list = []\n",
    "for field in arcpy.ListFields(wtd_glac):\n",
    "    wtd_glac_field_list.append(field.name)\n",
    "wtd_glac_arr = arcpy.da.TableToNumPyArray(wtd_glac, wtd_glac_field_list)\n",
    "wtd_glac_df = pd.DataFrame(wtd_glac_arr)\n",
    "wtd_glac_df = wtd_glac_df.drop(\"OBJECTID\",axis=1)\n",
    "wtd_glac_df = wtd_glac_df.set_index('cat_ID_con')\n",
    "dfs.append(wtd_glac_df)\n",
    "wtd_glac_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merge all covariate dataframes together and drop unnecessary columns\n",
    " * Recalculate cat_ID as float64 type\n",
    " * Reorder columns\n",
    " * Export final csv\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merge all data frames together\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "df_final = reduce(lambda left,right: pd.merge(left,right,on='cat_ID_con',how=\"outer\"), dfs)\n",
    "#Generate unique column names\n",
    "def uniquify(df_final):\n",
    "    seen = set()\n",
    "    for item in df_final:\n",
    "        fudge = 1\n",
    "        newitem = item\n",
    "        while newitem in seen:\n",
    "            fudge += 1\n",
    "            newitem = \"{}_{}\".format(item, fudge)\n",
    "        yield newitem\n",
    "        seen.add(newitem)\n",
    "df_final.columns = list(uniquify(df_final))\n",
    "#List of final columns in the order to output\n",
    "final_cols = ['cat_ID_txt','cat_ID','region', 'NhdAwcH12__cat_slope_COUNT', 'NhdAwcH12__cat_slope_AREA', 'NhdAwcH12__cat_slope_MIN', 'NhdAwcH12__cat_slope_MAX',\n",
    "              'NhdAwcH12__cat_slope_RANGE','NhdAwcH12__cat_slope_MEAN', 'NhdAwcH12__cat_slope_STD', 'NhdAwcH12__cat_slope_SUM', 'NhdAwcH12__cat_slope_MEDIAN', 'NhdAwcH12__cat_slope_PCT90',\n",
    "              'NhdAwcH12__cat_elev_COUNT', 'NhdAwcH12__cat_elev_AREA', 'NhdAwcH12__cat_elev_MIN', 'NhdAwcH12__cat_elev_MAX', 'NhdAwcH12__cat_elev_RANGE', 'NhdAwcH12__cat_elev_MEAN', 'NhdAwcH12__cat_elev_STD',\n",
    "              'NhdAwcH12__cat_elev_SUM', 'NhdAwcH12__cat_elev_VARIETY', 'NhdAwcH12__cat_elev_MAJORITY', 'NhdAwcH12__cat_elev_MINORITY', 'NhdAwcH12__cat_elev_MEDIAN', 'NhdAwcH12__cat_elev_PCT90',\n",
    "              'NhdAwcH12__wtd_elev_COUNT', 'NhdAwcH12__wtd_elev_AREA', 'NhdAwcH12__wtd_elev_MIN', 'NhdAwcH12__wtd_elev_MAX', 'NhdAwcH12__wtd_elev_RANGE', 'NhdAwcH12__wtd_elev_MEAN',\n",
    "              'NhdAwcH12__wtd_elev_STD', 'NhdAwcH12__wtd_elev_SUM', 'NhdAwcH12__wtd_elev_VARIETY', 'NhdAwcH12__wtd_elev_MAJORITY', 'NhdAwcH12__wtd_elev_MINORITY',\n",
    "              'NhdAwcH12__wtd_elev_MEDIAN', 'NhdAwcH12__wtd_elev_PCT90', 'NhdAwcH12__wtd_slope_COUNT', 'NhdAwcH12__wtd_slope_AREA', 'NhdAwcH12__wtd_slope_MIN', 'NhdAwcH12__wtd_slope_MAX',\n",
    "              'NhdAwcH12__wtd_slope_RANGE', 'NhdAwcH12__wtd_slope_MEAN', 'NhdAwcH12__wtd_slope_STD', 'NhdAwcH12__wtd_slope_SUM', 'NhdAwcH12__wtd_slope_MEDIAN', 'NhdAwcH12__wtd_slope_PCT90',\n",
    "              'NhdAwcH12__non_north_area', 'NhdAwcH12__north_area', 'NhdAwcH12__wtd_north_per', 'non_wetland_area', 'NhdAwcH12__wetland_area', 'NhdAwcH12__wtd_wet_per',\n",
    "              'NhdAwcH12__wtd_lake_area_sqm', 'NhdAwcH12__wtd_lake_per', 'NhdAwcH12__wtd_glacier_area_sqm', 'NhdAwcH12__wtd_glacier_per' ]\n",
    "#Create list of duplicate column names and drop\n",
    "drop_cols = ['cat_ID_txt_y', 'region_y', 'cat_ID_txt_x_2', 'region_x_2', 'region_y_2', 'cat_ID_txt_y_2', 'region_x_3',\n",
    "             'cat_ID_txt_x_3', 'cat_ID_txt_y_3', 'FType', 'region_y_3','cat_ID_txt_x_4', 'O1Region', 'region_x_4',\n",
    "             'cat_ID_y', 'cat_ID_txt_y_4', 'region_y_4']\n",
    "df_final.drop(columns=drop_cols, axis = 1, inplace=True)\n",
    "#rename columns\n",
    "df_final.rename({'cat_ID_txt_x':'cat_ID_txt','cat_ID_x':'cat_ID','region_x':'region'},axis=1, inplace=True)\n",
    "#Recalculate cat_ID\n",
    "df_final['cat_ID'] = df_final['cat_ID_txt'].astype(np.float64)\n",
    "# reorder cols\n",
    "df_final = df_final.reindex(columns=final_cols)\n",
    "df_final"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "invalid_cols = []\n",
    "final_cols = ['cat_ID_txt','cat_ID','region', 'NhdAwcH12__cat_slope_COUNT', 'NhdAwcH12__cat_slope_AREA', 'NhdAwcH12__cat_slope_MIN', 'NhdAwcH12__cat_slope_MAX',\n",
    "              'NhdAwcH12__cat_slope_RANGE','NhdAwcH12__cat_slope_MEAN', 'NhdAwcH12__cat_slope_STD', 'NhdAwcH12__cat_slope_SUM', 'NhdAwcH12__cat_slope_MEDIAN', 'NhdAwcH12__cat_slope_PCT90',\n",
    "              'NhdAwcH12__cat_elev_COUNT', 'NhdAwcH12__cat_elev_AREA', 'NhdAwcH12__cat_elev_MIN', 'NhdAwcH12__cat_elev_MAX', 'NhdAwcH12__cat_elev_RANGE', 'NhdAwcH12__cat_elev_MEAN', 'NhdAwcH12__cat_elev_STD',       'NhdAwcH12__cat_elev_SUM', 'NhdAwcH12__cat_elev_VARIETY', 'NhdAwcH12__cat_elev_MAJORITY', 'NhdAwcH12__cat_elev_MINORITY', 'NhdAwcH12__cat_elev_MEDIAN', 'NhdAwcH12__cat_elev_PCT90',\n",
    "              'NhdAwcH12__wtd_elev_COUNT', 'NhdAwcH12__wtd_elev_AREA', 'NhdAwcH12__wtd_elev_MIN', 'NhdAwcH12__wtd_elev_MAX', 'NhdAwcH12__wtd_elev_RANGE', 'NhdAwcH12__wtd_elev_MEAN',\n",
    "              'NhdAwcH12__wtd_elev_STD', 'NhdAwcH12__wtd_elev_SUM', 'NhdAwcH12__wtd_elev_VARIETY', 'NhdAwcH12__wtd_elev_MAJORITY', 'NhdAwcH12__wtd_elev_MINORITY',\n",
    "              'NhdAwcH12__wtd_elev_MEDIAN', 'NhdAwcH12__wtd_elev_PCT90', 'NhdAwcH12__wtd_slope_COUNT', 'NhdAwcH12__wtd_slope_AREA', 'NhdAwcH12__wtd_slope_MIN', 'NhdAwcH12__wtd_slope_MAX',\n",
    "              'NhdAwcH12__wtd_slope_RANGE', 'NhdAwcH12__wtd_slope_MEAN', 'NhdAwcH12__wtd_slope_STD', 'NhdAwcH12__wtd_slope_SUM', 'NhdAwcH12__wtd_slope_MEDIAN', 'NhdAwcH12__wtd_slope_PCT90',\n",
    "              'NhdAwcH12__non_north_area', 'NhdAwcH12__north_area', 'NhdAwcH12__wtd_north_per', 'non_wetland_area', 'NhdAwcH12__wetland_area', 'NhdAwcH12__wtd_wet_per',\n",
    "              'NhdAwcH12__wtd_lake_area_sqm', 'NhdAwcH12__wtd_lake_per', 'NhdAwcH12__wtd_glacier_area_sqm', 'NhdAwcH12__wtd_glacier_per' ]\n",
    "for col in final_cols:\n",
    "    if len(col) >= 36:\n",
    "        print (col, len(col))\n",
    "        invalid_cols.append(col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export merged dataframe to csv\n",
    "cov_csv_out = os.path.join(outdir,'AKSSF_AWC_HUC12s_Covariates.csv')\n",
    "df_final.to_csv(cov_csv_out, encoding = 'utf-8')\n",
    "print('Export all covariates dataframe to csv complete')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# bbay_df = df_final.filter(like='Bristol_Bay', axis = 0)\n",
    "# bbay_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # kod_df = df_final.filter(like='Kodiak', axis = 0)\n",
    "# kod_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pws_df = df_final.filter(like='Prince', axis = 0)\n",
    "# pws_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ci_df = df_final.filter(like='Cook', axis = 0)\n",
    "# ci_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cop_df = df_final.filter(like='Copper', axis = 0)\n",
    "cop_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ci_df = df_final.filter(like='Cook', axis = 0)\n",
    "ci_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8caab41e",
   "language": "python",
   "display_name": "PyCharm (AKSSF)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}