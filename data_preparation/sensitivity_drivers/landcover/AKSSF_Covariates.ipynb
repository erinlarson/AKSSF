{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a id='top'></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Watershed and Catchment Landcover Metrics\n",
    "\n",
    "## Aspect and Percent North Covariates\n",
    "Calculate aspect grid with the DEM used to create the flow network for each processing area (e.g. HUC8 in NHDPlus or\n",
    " region for BB, PWS, or Kodiak). We need the DEMs used for each flow direction grid, or we can't calculate covariates at\n",
    " the watershed scale.\n",
    " * **~~aspect_rch = calculate mean aspect for stream reach (zonal statistics)~~**\n",
    " * **~~aspect_cat = calculate mean aspect over catchments (zonal statistics)~~**\n",
    " * **north_wtd = create 1/0 grid of north facing cells from aspect grid (north = aspects from 315-45 degrees), ~~~use\n",
    "  weighted flow accumulation to sum north facing cells, divide by flow accumulation grid to get % of north facing\n",
    "  cells in each watershed~~~ Use tabulate area to quantify area of North cells (VALUE_1) and non-North (VALUE_0) cells\n",
    "  for each watershed and calculate percent north as north_wtd = row[VALUE_1]/(row[VALUE_1]+row[VALUE_0])*100 .**\n",
    "## Elevation Metrics\n",
    "Calculate elevation metrics for catchment/watershed with temperature data using zonal statistics as table.\n",
    "* <b> Running \"ALL\" zonal statistics as it does not allow you to choose two types of statistics\n",
    "(ex statistics_type = [\"MIN\",\"MAX\"]) instead set statistics_type=\"ALL\"</b>\n",
    "### Catchment Elevation Metrics\n",
    "* **cat_elev_mn = mean elevation for catchment**\n",
    "* **cat_elev_min = minimum elevation for catchment**\n",
    "* **cat_elev_max = max elevation for catchment**\n",
    "* **cat_elev_std = standard deviation of elevation for catchment**\n",
    "### Watershed Elevation Metrics\n",
    "* **wtd_elev_mn = mean watershed elevation**\n",
    "* **wtd_elev_min = min watershed elevation**\n",
    "* **wtd_elev_max = max watershed elevation**\n",
    "* **wtd_elev_sd (or cv) = standard deviation of watershed elevation**\n",
    "## Slope Metrics\n",
    "Calculate slope metrics for catchment/watershed with temperature data using zonal statistics as table and\n",
    "statistics_type = \"ALL\".\n",
    "### Catchment Slope Metrics\n",
    "* **cat_slope_mn = mean slope for catchment**\n",
    "* **cat_slope_min = minimum slope for catchment**\n",
    "* **cat_slope_max = max slope for catchment**\n",
    "* **cat_slope_std = standard deviation of slope for catchment**\n",
    "### Watershed Elevation Metrics\n",
    "* **wtd_slope_mn = mean watershed slope**\n",
    "* **wtd_slope_min = min watershed slope**\n",
    "* **wtd_slope_max = max watershed slope**\n",
    "* **wtd_slope_sd (or cv) = standard deviation of watershed slope**\n",
    "## Lake, Wetland and Glacier Cover\n",
    "Lake/Pond type waterbodies from NHDPLus hydrography for those regions with NHDPlus derived datasets and NHD waterbodies\n",
    "for those regions with TauDEM derived waterbodies.  Merge all waterbodies together (FTYPE = 390) and use tabulate area\n",
    "calculate percent cover of lakes in catchments/watersheds.\n",
    " * **Created local copies of NHDPlus and NHD waterbodies and exported to T driver here <>  **\n",
    "Use NLCD grid (wetlands.tif) and tabulate area on wetlands grid with watersheds (sum) / divide by number of cells in\n",
    "watershed from fac grid, wetlands from NLCD\n",
    "## Distance from coast\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import modules"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20102021\n",
      "C:\\Users\\dwmerrigan\\Documents\\GitHub\\AKSSF\\data_preparation\\sensitivity_drivers\\landcover\n",
      "C:\\Users\\dwmerrigan\\AppData\\Local\\Programs\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\n"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "arcpy.env.overwriteOutput = True\n",
    "today = datetime.datetime.now()\n",
    "# Make the time stamp.\n",
    "time_stamp = '{:%d%m%Y}'.format(today)\n",
    "print(time_stamp)\n",
    "\n",
    "path = os.getcwd()\n",
    "print (path)\n",
    "print (sys.base_exec_prefix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collect data and set working output locations\n",
    "Set data_dir to folder containing all AKSSF regional subfolders/geoadatabases\n",
    "* RS data folder: data_dir = r\"W:\\GIS\\AKSSF\"\n",
    "* DM data folder: data_dir = r\"D:\\GIS_Temp\\AKSSF_BeckyCopy\\AKSSF\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\GIS_temp\\\\AKSSF\\\\Bristol_Bay', 'D:\\\\GIS_temp\\\\AKSSF\\\\Cook_Inlet', 'D:\\\\GIS_temp\\\\AKSSF\\\\Copper_River', 'D:\\\\GIS_temp\\\\AKSSF\\\\Kodiak', 'D:\\\\GIS_temp\\\\AKSSF\\\\Prince_William_Sound']\n",
      "Working Folder already created... D:\\GIS_temp\\AKSSF_land_met\n",
      "Output location already exists D:\\GIS_temp\\AKSSF_land_met\\AKSSF_land_met.gdb\n"
     ]
    }
   ],
   "source": [
    "# Set AKSSF Data directory\n",
    "# data_dir = r\"W:\\GIS\\AKSSF\"\n",
    "\n",
    "# dm local\n",
    "data_dir = r\"D:\\GIS_Temp\\AKSSF\"\n",
    "\n",
    "arcpy.env.workspace = data_dir\n",
    "regions = arcpy.ListWorkspaces(workspace_type=\"Folder\")\n",
    "rois= []\n",
    "print (regions)\n",
    "\n",
    "# Path to create output folder/gdb\n",
    "temppath = r\"D:\\GIS_temp\" # Output folder\n",
    "dirname = 'AKSSF_land_met'\n",
    "tempgdbname = 'AKSSF_land_met.gdb'\n",
    "temp_dir = os.path.join(temppath, dirname)\n",
    "\n",
    "# Create temporary working gdb\n",
    "if not arcpy.Exists(temp_dir):\n",
    "    os.makedirs(temp_dir)\n",
    "else:\n",
    "    print('Working Folder already created...', temp_dir)\n",
    "\n",
    "outcheck = os.path.join(temp_dir, tempgdbname)\n",
    "\n",
    "if arcpy.Exists(outcheck):\n",
    "    print ('Output location already exists', outcheck)\n",
    "    outgdb = outcheck\n",
    "if not arcpy.Exists(outcheck):\n",
    "    print('Creating output GDB')\n",
    "    tempgdb = arcpy.CreateFileGDB_management(temp_dir,tempgdbname)\n",
    "    print ('Output geodatabase created at', outcheck)\n",
    "    outgdb = tempgdb.getOutput(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "data": {
      "text/plain": "['D:\\\\GIS_temp\\\\AKSSF\\\\Bristol_Bay',\n 'D:\\\\GIS_temp\\\\AKSSF\\\\Cook_Inlet',\n 'D:\\\\GIS_temp\\\\AKSSF\\\\Copper_River',\n 'D:\\\\GIS_temp\\\\AKSSF\\\\Kodiak',\n 'D:\\\\GIS_temp\\\\AKSSF\\\\Prince_William_Sound']"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set regional workspaces from AKSSF data folder and store in list\n",
    "arcpy.env.workspace = data_dir\n",
    "regions  = arcpy.ListWorkspaces(workspace_type=\"Folder\")\n",
    "regions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create aspect and north grids if they do not already exists\n",
    "### NHDPlus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "# Code to create merged elevation grid, aspect and north aspect grids for entire AKSSF region with NHDPlus elev data\n",
    "\n",
    "# elrasters = []\n",
    "# nhd_dat = r\"C:\\\\Users\\\\dwmerrigan\\\\Documents\\\\GitHub\\\\AKSSF\\\\hydrography\\\\AKSSF_NHDPlus\\\\extracts\"\n",
    "# for dirpath, dirname, filenames in arcpy.da.Walk(nhd_dat, 'RasterDataset'):\n",
    "#     for filename in filenames:\n",
    "#         if filename == \"elev_cm.tif\":\n",
    "#             elras = os.path.join(dirpath, filename)\n",
    "#             elrasters.append(elras)\n",
    "#             print (f'Appending {elras} to list')\n",
    "#\n",
    "# spatial_ref = arcpy.Describe(elrasters[0]).spatialReference\n",
    "# print (spatial_ref.name)\n",
    "# # Merge all nhdplus elevation rasters together\n",
    "# elevname = 'AKSSF_NHDPlus_elev_cm.tif'\n",
    "# AKSSF_elev_cm = arcpy.MosaicToNewRaster_management(elrasters,temp_dir,elevname, spatial_ref, \"32_BIT_SIGNED\", \"5\", \"1\", \"LAST\",\"FIRST\")\n",
    "#\n",
    "# #make aspect raster for all of akssf from merged nhdplus\n",
    "# from arcpy.sa import *\n",
    "# aspname = 'AKSSF_NHDPlus_aspect.tif'\n",
    "# asp_rast = Aspect(in_raster= AKSSF_elev_cm, method='Planar')\n",
    "# asp_rast.save(os.path.join(temp_dir, aspname))\n",
    "#\n",
    "# # make north raster from aspect raster\n",
    "# norname = 'AKSSF_NHDPlus_north_aspect.tif'\n",
    "# aspect_in = Raster(asp_rast)\n",
    "# north_raster = Con((aspect_in>=0)&(aspect_in<=45),1,Con((aspect_in<=360)&(aspect_in>=315),1,0))\n",
    "# north_raster.save(os.path.join(temp_dir, norname))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TauDEM - Create grids using Timms Composite DEM for entire TauDEM processed areas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "# Code to create merged elevation grid, aspect and north aspect grids for entire AKSSF region with TauDEM elev data\n",
    "# compDEM = r\"C:\\Users\\dwmerrigan\\Documents\\GitHub\\AKSSF\\hydrography\\data\\topography\\AKSSF_Elevation_Composite_10m.tif\"\n",
    "# clip to study area\n",
    "# AKSSF_elev_10m = r\"D:\\\\GIS_temp\\\\AKSSF_Composite_10m_extract.tif\"\n",
    "#\n",
    "# #make aspect raster for all of akssf from merged nhdplus\n",
    "# from arcpy.sa import *\n",
    "# aspname = 'AKSSF_Composite_10m_aspect.tif'\n",
    "# asp_rast = Aspect(in_raster= AKSSF_elev_10m, method='Planar')\n",
    "# asp_rast.save(os.path.join(temp_dir, aspname))\n",
    "#\n",
    "# # make north raster from aspect raster\n",
    "# norname = 'AKSSF_Composite_10m_north_aspect.tif'\n",
    "# aspect_in = Raster(asp_rast)\n",
    "# north_raster = Con((aspect_in>=0)&(aspect_in<=45),1,Con((aspect_in<=360)&(aspect_in>=315),1,0))\n",
    "# north_raster.save(os.path.join(temp_dir, norname))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Begin GIS portion\n",
    "- May be faster and save more space to create aspect rasters here using extract by mask?\n",
    "- May be able to use aspect raster with tabulate intersection to calculate percent North (aspects from 315-45 degrees)\n",
    "\n",
    "### <b>UPDATE - 2021-10-12 Do not run aspect metrics at this time. Only need to run percent North for watersheds</b>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kodiak using data from D:\\GIS_temp\\AKSSF\\Kodiak folder\n",
      "Kodiak in ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound'] TauDEM list, using cat_fields ['cat_ID_txt', 'gridcode', 'cat_ID_con'] and watershed fields ['cat_ID_txt', 'cat_ID', 'cat_ID_con']\n",
      "TauDem data: north raster - D:\\Basedata\\AKSSF_Basedata\\AKSSF_Composite_10m_north_aspect.tif\n",
      "         aspect raster - D:\\Basedata\\AKSSF_Basedata\\AKSSF_Composite_10m_aspect.tif\n",
      "      elevation raster - D:\\Basedata\\AKSSF_Basedata\\AKSSF_Composite_10m_extract.tif\n",
      "----------\n",
      "cat_ID_txt field already in dataset D:\\GIS_temp\\AKSSF\\Kodiak\\Kodiak.gdb\\cats_intersect\n",
      "----------\n",
      "cat_ID_con field already in dataset D:\\GIS_temp\\AKSSF\\Kodiak\\Kodiak.gdb\\cats_intersect\n",
      "----------\n",
      "Merged watershed dataset wtds_merge found\n",
      "----------\n",
      "cat_ID_txt field already in dataset\n",
      "----------\n",
      "cat_ID_con field already in dataset D:\\GIS_temp\\AKSSF\\Kodiak\\Kodiak.gdb\\wtds_merge\n",
      "----------\n",
      " wtd_48267 watershed identified\n",
      " wtd_49617 watershed identified\n",
      " wtd_50197 watershed identified\n",
      " wtd_64593 watershed identified\n",
      " wtd_72144 watershed identified\n",
      " wtd_76954 watershed identified\n",
      " wtd_77794 watershed identified\n",
      " wtd_90346 watershed identified\n",
      " wtd_93176 watershed identified\n",
      " wtd_94216 watershed identified\n",
      " wtd_97276 watershed identified\n",
      " wtd_99516 watershed identified\n",
      " wtd_100826 watershed identified\n",
      " wtd_101556 watershed identified\n",
      " wtd_103096 watershed identified\n",
      " wtd_103196 watershed identified\n",
      " wtd_103296 watershed identified\n",
      " wtd_103456 watershed identified\n",
      " wtd_103496 watershed identified\n",
      " wtd_106626 watershed identified\n",
      " wtd_106676 watershed identified\n",
      " wtd_107796 watershed identified\n",
      " wtd_107816 watershed identified\n",
      " wtd_108356 watershed identified\n",
      " wtd_128685 watershed identified\n",
      " wtd_129955 watershed identified\n",
      " wtd_130295 watershed identified\n",
      " wtd_130735 watershed identified\n",
      "Calculating aspect metrics and percent north for catchments & watersheds of interest in Kodiak region\n",
      "----------\n",
      "Geodatabase: ['D:\\\\GIS_temp\\\\AKSSF\\\\Kodiak\\\\Kodiak.gdb']\n",
      "----------\n",
      "Elevation Raster: D:\\GIS_temp\\AKSSF\\Kodiak\\elev.tif\n",
      "----------\n",
      "North Aspect Raster: D:\\GIS_temp\\AKSSF\\Kodiak\\north.tif\n",
      "----------\n",
      "28 Watersheds to process\n",
      "----------\n",
      "Catchment intersect D:\\GIS_temp\\AKSSF\\Kodiak\\Kodiak.gdb\\cats_intersect selected\n",
      "----------\n",
      "Begin zonal statistics min/mean/max std dev for watersheds and catchments in Kodiak region\n",
      "Zonal Stats for Kodiak Elapsed time: (0:00:10)\n",
      "----------\n",
      "Begin tabulate area of north facing cells for watersheds and catchments in Kodiak region\n",
      "['OBJECTID', 'CAT_ID_TXT_DEL', 'VALUE_0', 'VALUE_1', 'region', 'wtd_north_per', 'cat_ID_txt', 'cat_ID_con']\n",
      "Tabulate area for Kodiak Elapsed time: (0:00:09)\n",
      "----------\n",
      "Region Kodiak not found in D:\\GIS_temp\\AKSSF\\Prince_William_Sound\n",
      "Process completed at 2021-10-19 14:18 (Elapsed time: 0:00:20)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from arcpy.sa import *\n",
    "import re\n",
    "\n",
    "# Set data_dir equal to folder containing AKSSF regional subfolders containing GDBs and raster datasets\n",
    "arcpy.env.workspace = data_dir\n",
    "arcpy.env.overwriteOutput = True\n",
    "regions  = arcpy.ListWorkspaces(workspace_type=\"Folder\")\n",
    "\n",
    "# Lists for variables not needed at present time\n",
    "#cat_asp_ztables = []\n",
    "#wtd_asp_ztables = []\n",
    "#cat_pernorth_taba_tables=[]\n",
    "\n",
    "# Lists to store output tables\n",
    "wtd_pernorth_taba_tables=[]\n",
    "cat_elev_ztables = []\n",
    "wtd_elev_ztables = []\n",
    "cat_slope_ztables = []\n",
    "wtd_slope_ztables = []\n",
    "\n",
    "\n",
    "# Clear lists\n",
    "cat_cur_fields = []\n",
    "wtd_cur_fields = []\n",
    "\n",
    "# Start timing function\n",
    "processStart = time.time()\n",
    "processStartdt = datetime.datetime.now()\n",
    "\n",
    "# Seperate data by\n",
    "nhdplus_dat = ['Cook_Inlet','Copper_River']\n",
    "tauDem_dat = ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound']\n",
    "\n",
    "# Loop through all processing areas\n",
    "rois = nhdplus_dat + tauDem_dat\n",
    "# Test on Kodiak\n",
    "rois = ['Kodiak']\n",
    "for roi in rois:\n",
    "    # Loop through regional folders\n",
    "    for region in regions:\n",
    "        if roi in str(region):\n",
    "            print(f'{roi} using data from {region} folder')\n",
    "            # Set data and variables unique to regions with NHDPlus Data\n",
    "            if roi in nhdplus_dat:\n",
    "                # # NHDPLus raster data for DM pc, variables will be overwritten below if they exist in source folder.\n",
    "                # asp_rast = r\"D:\\Basedata\\AKSSF_Basedata\\AKSSF_NHDPlus_aspect.tif\"\n",
    "                # elev_rast = r\"D:\\Basedata\\AKSSF_Basedata\\AKSSF_NHDPlus_elev_cm.tif\"\n",
    "                # nor_rast = r\"D:\\Basedata\\AKSSF_Basedata\\AKSSF_NHDPlus_north_aspect.tif\"\n",
    "                lakes_fc = r\"\"\n",
    "                # Fields for update cursor\n",
    "                cat_cur_fields = ['cat_ID_txt', 'NHDPlusID',\"cat_ID_con\"]\n",
    "                wtd_cur_fields = ['cat_ID_txt', 'cat_ID',\"cat_ID_con\"]\n",
    "                print (f'{roi} in {nhdplus_dat} AKSSF list, using cat_fields {cat_cur_fields} and watershed fields {wtd_cur_fields}')\n",
    "                print(f'NHD data: north raster - {nor_rast}')\n",
    "                print(f'         aspect raster - {asp_rast}')\n",
    "                print(f'      elevation raster - {elev_rast}')\n",
    "                print('----------')\n",
    "            # Set data and variables unique to regions with TauDEM Data\n",
    "            elif roi in tauDem_dat:\n",
    "                # # TauDEM raster data for DM pc, variables will be overwritten below if they exist in source folder.\n",
    "                # nor_rast = r\"D:\\Basedata\\AKSSF_Basedata\\AKSSF_Composite_10m_north_aspect.tif\"\n",
    "                # asp_rast = r\"D:\\Basedata\\AKSSF_Basedata\\AKSSF_Composite_10m_aspect.tif\"\n",
    "                # elev_rast = r\"D:\\Basedata\\AKSSF_Basedata\\AKSSF_Composite_10m_extract.tif\"\n",
    "                \n",
    "                lakes_fc = r\"\"\n",
    "                # Fields for update cursor\n",
    "                cat_cur_fields = ['cat_ID_txt', 'gridcode',\"cat_ID_con\"]\n",
    "                wtd_cur_fields = ['cat_ID_txt', 'cat_ID',\"cat_ID_con\"]\n",
    "\n",
    "                print (f'{roi} in {tauDem_dat} TauDEM list, using cat_fields {cat_cur_fields} and watershed fields {wtd_cur_fields}')\n",
    "                print(f'TauDem data: north raster - {nor_rast}')\n",
    "                print(f'         aspect raster - {asp_rast}')\n",
    "                print(f'      elevation raster - {elev_rast}')\n",
    "                print('----------')\n",
    "\n",
    "            wtds = []\n",
    "            cats = []\n",
    "            # Start iter timing function\n",
    "            iteration_start = time.time()\n",
    "            # Set workspace to region folder\n",
    "            arcpy.env.workspace = region\n",
    "            gdb = arcpy.ListWorkspaces(workspace_type='FileGDB')\n",
    "            walk = arcpy.da.Walk(region, datatype = ['FeatureClass','RasterDataset'])\n",
    "            for dirpath, dirnames, filenames in walk:\n",
    "                # Create list of watersheds\n",
    "                for filename in filenames:\n",
    "                    if ((\"wtd\" in filename) and (\"merge\" not in filename)):\n",
    "                        print(f\" {filename} watershed identified\")\n",
    "                        wtds.append(os.path.join(dirpath, filename))\n",
    "                    # Set watershed merged watersheds dataset\n",
    "                    if 'wtds_merge'in filename:\n",
    "                        wtd_merge = os.path.join(dirpath, filename)\n",
    "                        print(f'Merged watershed dataset {filename} found')\n",
    "                        print('----------')\n",
    "                        wtdfieldnames = []\n",
    "                        wtdlstFields = arcpy.ListFields(wtd_merge)\n",
    "                        for field in wtdlstFields:\n",
    "                            wtdfieldnames.append(field.name)\n",
    "                        if str(wtd_cur_fields[0]) in wtdfieldnames:\n",
    "                            print (f'{wtd_cur_fields[0]} field already in dataset')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print (f'Adding {wtd_cur_fields[0]} field to watershed dataset {wtd_merge}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_txt field and concat cat_ID + region\n",
    "                            arcpy.AddField_management(wtd_merge, str(wtd_cur_fields[0]),field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(wtd_merge, wtd_cur_fields[0:2]) as cur:\n",
    "                                for row in cur:\n",
    "                                    int_val = int(row[1])\n",
    "                                    row[0] = str(int_val)\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del(row)\n",
    "                            del(cur)\n",
    "                        if str(wtd_cur_fields[2]) in wtdfieldnames:\n",
    "                            print (f'{wtd_cur_fields[2]} field already in dataset {wtd_merge}')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print (f'Adding {wtd_cur_fields[2]} field to watershed dataset {wtd_merge}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_con field and concat cat_ID + region\n",
    "                            arcpy.AddField_management(wtd_merge, str(wtd_cur_fields[2]),field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(wtd_merge, wtd_cur_fields) as cur:\n",
    "                                for row in cur:\n",
    "                                    int_val = int(row[1])\n",
    "                                    row[2] = str(roi) +'_'+ str(int_val)\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del(row)\n",
    "                            del(cur)\n",
    "\n",
    "                    # Select elevation raster\n",
    "                    elif 'elev.tif' == filename:\n",
    "                        elev_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "                    # # Select aspect raster\n",
    "                    # elif 'aspect' in filename:\n",
    "                    #     asp_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "                    # Select north raster\n",
    "                    elif 'north.tif' == filename:\n",
    "                        nor_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "                    # Select slope\n",
    "                    elif 'slope.tif' == filename:\n",
    "                        slope_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "                    # Select catch_int fc (catchments of interest for region) and make a copy\n",
    "                    elif 'cats_intersect' in filename:\n",
    "                        cats = os.path.join(dirpath,filename)\n",
    "                        catlstfields = arcpy.ListFields(cats)\n",
    "                        catfieldnames = []\n",
    "                        for field in catlstfields:\n",
    "                            catfieldnames.append(field.name)\n",
    "                        if str(cat_cur_fields[0]) in catfieldnames:\n",
    "                            print (f'{cat_cur_fields[0]} field already in dataset {cats}')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print (f'Adding {cat_cur_fields[0]} field to catchment dataset {cats}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_txt field\n",
    "                            arcpy.AddField_management(cats, str(cat_cur_fields[0]), field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(cats, cat_cur_fields[0:2]) as cur:\n",
    "                                for row in cur:\n",
    "                                    int_val = int(row[1])\n",
    "                                    row[0] = str(int_val)\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del(row)\n",
    "                            del(cur)\n",
    "                        if str(cat_cur_fields[2]) in catfieldnames:\n",
    "                            print (f'{cat_cur_fields[2]} field already in dataset {cats}')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print (f'Adding {cat_cur_fields[2]} field to catchment dataset {cats}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_txt field & cat_ID + region concat field\n",
    "                            arcpy.AddField_management(cats,str(cat_cur_fields[2]),field_type='TEXT')\n",
    "                            # populate cat_ID_con\n",
    "                            with arcpy.da.UpdateCursor(cats, cat_cur_fields) as cur:\n",
    "                                for row in cur:\n",
    "                                    int_val = int(row[1])\n",
    "                                    row[2] = str(roi) +'_'+ str(int_val)\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del(row)\n",
    "                            del(cur)\n",
    "\n",
    "            print (f'Calculating topographic metrics for catchments & watersheds of interest in {roi}'\n",
    "                   f' region')\n",
    "            print ('----------')\n",
    "            print(f'Geodatabase: {gdb}')\n",
    "            print ('----------')\n",
    "            print (f'Elevation Raster: {elev_rast}')\n",
    "            print ('----------')\n",
    "            print (f'North Aspect Raster: {nor_rast}')\n",
    "            print ('----------')\n",
    "            print (f'Slope Raster: {slope_rast}')\n",
    "            print ('----------')        \n",
    "            print (f'{len(wtds)} Watersheds to process')\n",
    "            print ('----------')\n",
    "            print (f'Catchment intersect {cats} selected')\n",
    "            print ('----------')\n",
    "\n",
    "\n",
    "            try:\n",
    "                # Set output names/paths for watershed and catchment tables\n",
    "                wtd_merge_name = roi + \"_Watersheds_Merge\"\n",
    "                wtd_merge_path = os.path.join(outgdb,wtd_merge_name)\n",
    "\n",
    "                # # Aspect variables\n",
    "                # wtd_merge_asp_table_name = roi + \"_Watersheds_Merge_AspectZstats\"\n",
    "                # wtd_merge_asp_table_path = os.path.join(outgdb, wtd_merge_asp_table_name)\n",
    "                # cat_asp_table_name = roi + \"_Catchments_AspectZstats\"\n",
    "                # cat_asp_table_path = os.path.join(outgdb, cat_asp_table_name)\n",
    "\n",
    "                # Percent North variables\n",
    "                wtd_merge_pernorth_table_name = roi + \"_Watersheds_Merge_PercentNorth\"\n",
    "                wtd_merge_pernorth_table_path = os.path.join(outgdb, wtd_merge_pernorth_table_name)\n",
    "                # cat_pernorth_table_name = roi + \"_Catchments_PercentNorth\"\n",
    "                # cat_pernorth_table_path = os.path.join(outgdb, cat_pernorth_table_name)\n",
    "\n",
    "                # Elevation variables\n",
    "                wtd_merge_elev_table_name = roi + \"_Watersheds_Merge_ElevZstats\"\n",
    "                wtd_merge_elev_table_path = os.path.join(outgdb, wtd_merge_elev_table_name)\n",
    "                cat_elev_table_name = roi + \"_Catchments_ElevZstats\"\n",
    "                cat_elev_table_path = os.path.join(outgdb, cat_elev_table_name)\n",
    "                \n",
    "                # Slope variables\n",
    "                wtd_merge_slope_table_name = roi + \"_Watershed_Merge_SlopeZstats\"\n",
    "                wtd_merge_slope_table_path = os.path.join(outgdb, wtd_merge_slope_table_name)\n",
    "                cat_slope_table_name = roi + \"_Catchments_SlopeZstats\"\n",
    "                cat_slope_table_path = os.path.join(outgdb, cat_slope_table_name)\n",
    "                \n",
    "                \n",
    "\n",
    "                # # Watersheds already merged so no need to run at this time\n",
    "                # if not arcpy.Exists(wtd_merge):\n",
    "                #     mergestart = time.time()\n",
    "                #     # Merge watersheds\n",
    "                #     wtd_merge = arcpy.Merge_management( wtds, wtd_merge_path ,add_source='ADD_SOURCE_INFO')\n",
    "                #     # Add wtd_id field\n",
    "                #     arcpy.AddField_management(wtd_merge,'cat_ID_txt',field_type='TEXT')\n",
    "                #     arcpy.AddField_management(wtd_merge,\"cat_ID_con\",field_type='TEXT')\n",
    "                #     # Add region field\n",
    "                #     arcpy.AddField_management(wtd_merge,'region',field_type='TEXT')\n",
    "                #     # Populate watershed id and region information using update cursor - faster than field calc\n",
    "                #     with arcpy.da.UpdateCursor(wtd_merge,['MERGE_SRC','cat_ID_txt','region','cat_ID_con']) as cur:\n",
    "                #         for row in cur:\n",
    "                #             row[1] = re.findall('\\d+', row[0])[0]\n",
    "                #             row[2] = roi\n",
    "                #             row[3] = + str(roi) + '_'+ str(re.findall('\\d+', row[0])[0])\n",
    "                #             # Update\n",
    "                #             cur.updateRow(row)\n",
    "                #         del(row)\n",
    "                #     del(cur)\n",
    "                #     mergestop = time.time()\n",
    "                #     mergetime = int (mergestop - mergestart)\n",
    "                #     print(f'Watershed Merge for {roi} Elapsed time: ({datetime.timedelta(seconds=mergetime)})')\n",
    "                #     print('----------')\n",
    "                # else:\n",
    "                #     print (f'Merged watershed dataset {wtd_merge} already created')\n",
    "                #     print('----------')\n",
    "\n",
    "                # Begin Zonal Stats\n",
    "                zstat_start = time.time()\n",
    "                print(f'Begin zonal statistics min/mean/max std dev for watersheds and catchments in {roi} region')\n",
    "\n",
    "                # Elevation Zonal statistics  for watersheds\n",
    "                arcpy.env.snapRaster = elev_rast\n",
    "                arcpy.env.cellSize = elev_rast\n",
    "                wtd_elev_metrics_table = ZonalStatisticsAsTable(in_zone_data = wtd_merge,\n",
    "                                                                zone_field = wtd_cur_fields[0],\n",
    "                                                                in_value_raster = elev_rast,\n",
    "                                                                out_table = wtd_merge_elev_table_path,\n",
    "                                                                statistics_type='ALL'\n",
    "                                                                )\n",
    "                # Add region identifier field for watershed tables                                                )\n",
    "                arcpy.AddField_management(wtd_elev_metrics_table,'region',field_type='TEXT')\n",
    "                # Update region field\n",
    "                with arcpy.da.UpdateCursor(wtd_elev_metrics_table,'region') as cur:\n",
    "                    for row in cur:\n",
    "                        row[0] = roi\n",
    "                        # Update\n",
    "                        cur.updateRow(row)\n",
    "                    del(row)\n",
    "                del(cur)\n",
    "                # Append watershed elev table to list\n",
    "                wtd_elev_ztables.append(wtd_elev_metrics_table)\n",
    "\n",
    "                # Elevation zonal statistics for catchments\n",
    "                arcpy.env.snapRaster = elev_rast\n",
    "                arcpy.env.cellSize = elev_rast\n",
    "                cat_elev_metrics_table = ZonalStatisticsAsTable(in_zone_data = cats ,\n",
    "                                                                zone_field = cat_cur_fields[0],\n",
    "                                                                in_value_raster = elev_rast,\n",
    "                                                                out_table = cat_elev_table_path,\n",
    "                                                                statistics_type='ALL'\n",
    "                                                                )\n",
    "                # Add region identifier field for catchment table\n",
    "                arcpy.AddField_management(cat_elev_metrics_table,'region',field_type='TEXT')\n",
    "\n",
    "                # Update region field\n",
    "                with arcpy.da.UpdateCursor(cat_elev_metrics_table,'region') as cur:\n",
    "                    for row in cur:\n",
    "                        row[0] = roi\n",
    "                        # Update\n",
    "                        cur.updateRow(row)\n",
    "                    del(row)\n",
    "                del(cur)\n",
    "                # Append catchment elev table to list\n",
    "                cat_elev_ztables.append(cat_elev_metrics_table)\n",
    "\n",
    "                # # Aspect Zonal statistics  for watersheds\n",
    "                # wtd_asp_metrics_table = ZonalStatisticsAsTable(in_zone_data = wtd_merge, zone_field =\"cat_ID_txt\",\n",
    "                #                                                in_value_raster = asp_rast, out_table = wtd_merge_asp_table_path,\n",
    "                #                                                statistics_type='ALL')\n",
    "                # arcpy.AddField_management(wtd_asp_metrics_table, 'region', field_type='TEXT')\n",
    "                # arcpy.CalculateField_management(wtd_asp_metrics_table, 'region', 'roi')\n",
    "                # wtd_asp_ztables.append(wtd_asp_metrics_table)\n",
    "\n",
    "                # # Aspect Zonal statistics for catchments\n",
    "                # cat_asp_metrics_table = ZonalStatisticsAsTable(in_zone_data = cats, zone_field =\"cat_ID_txt\",\n",
    "                #                                                in_value_raster = asp_rast, out_table = cat_asp_table_path,\n",
    "                #                                                statistics_type='ALL')\n",
    "                # cat_asp_ztables.append(cat_asp_metrics_table)\n",
    "\n",
    "                zstat_stop = time.time()\n",
    "                zstat_time = int (zstat_stop - zstat_start)\n",
    "                print(f'Zonal Stats for {roi} Elapsed time: ({datetime.timedelta(seconds=zstat_time)})')\n",
    "                print('----------')\n",
    "\n",
    "                # Tabulate Area with north grid and catchments/watersheds\n",
    "                tabarea_start = time.time()\n",
    "                print(f'Begin tabulate area of north facing cells for watersheds and catchments in {roi} region')\n",
    "                # Percent North Tabulate area for watersheds\n",
    "                wtd_per_north_tabarea = arcpy.sa.TabulateArea(in_zone_data= wtd_merge,\n",
    "                                                              zone_field= wtd_cur_fields[0],\n",
    "                                                              in_class_data=nor_rast,\n",
    "                                                              class_field=\"Value\",\n",
    "                                                              out_table=wtd_merge_pernorth_table_path\n",
    "                                                              )\n",
    "                # Add region and percent north fields\n",
    "                arcpy.AlterField_management(wtd_per_north_tabarea,'CAT_ID_TXT','CAT_ID_TXT_DEL','CAT_ID_TXT_DEL')\n",
    "                arcpy.AddField_management(wtd_per_north_tabarea, 'region', field_type='TEXT')\n",
    "                arcpy.AddField_management(wtd_per_north_tabarea, 'wtd_north_per', field_type='Float')\n",
    "                arcpy.AddField_management(wtd_per_north_tabarea, wtd_cur_fields[0], field_type='TEXT')\n",
    "                arcpy.AddField_management(wtd_per_north_tabarea, wtd_cur_fields[2], field_type='TEXT')\n",
    "                wtdnorfields = [f.name for f in arcpy.ListFields(wtd_per_north_tabarea)]\n",
    "                print (wtdnorfields)\n",
    "                with arcpy.da.UpdateCursor(wtd_per_north_tabarea, wtdnorfields) as cur:\n",
    "                    for row in cur:\n",
    "                        row[4] = roi\n",
    "                        row[5] = row[3]/(row[3]+row[2])*100\n",
    "                        row[6] = row[1]\n",
    "                        row[7] = roi +'_'+ row[1]\n",
    "                        # Update\n",
    "                        cur.updateRow(row)\n",
    "                    del(row)\n",
    "                del(cur)\n",
    "                # Drop UPPERCASE field form tab area\n",
    "                arcpy.DeleteField_management(wtd_per_north_tabarea,'CAT_ID_TXT_DEL')\n",
    "                # Append watershed percent north table to list\n",
    "                wtd_pernorth_taba_tables.append(wtd_per_north_tabarea)\n",
    "\n",
    "                # # Percent North Tabulate Area for catchments\n",
    "                # cat_per_north_tabarea = arcpy.sa.TabulateArea(in_zone_data= cats, zone_field='cat_ID_con',\n",
    "                #                                             in_class_data=nor_rast,\"Value\",\n",
    "                #                                             out_table=cat_pernorth_table_path)\n",
    "\n",
    "                # # Add and calculate region identifier field for catchment table\n",
    "                # arcpy.AlterField_management(cat_per_north_tabarea,'CAT_ID_TXT','CAT_ID_TXT_DEL','CAT_ID_TXT_DEL')\n",
    "                # arcpy.AddField_management(cat_per_north_tabarea, 'region', field_type='TEXT')\n",
    "                # arcpy.AddField_management(cat_per_north_tabarea, 'cat_north_per', field_type='Float')\n",
    "                # arcpy.AddField_management(cat_per_north_tabarea, cat_cur_fields[0], field_type='TEXT')\n",
    "                # arcpy.AddField_management(cat_per_north_tabarea, cat_cur_fields[2], field_type='TEXT')\n",
    "                # catnorfields = [f.name for f in arcpy.ListFields(cat_per_north_tabarea)]\n",
    "                # print (catnorfields)\n",
    "                # with arcpy.da.UpdateCursor(cat_per_north_tabarea,catnorfields) as cur:\n",
    "                #     for row in cur:\n",
    "                #         row[4] = roi\n",
    "                #         row[5] = row[3]/(row[3]+row[2])*100\n",
    "                #         row[6] = row[1]\n",
    "                #         row[7] = roi +'_'+ row[1]\n",
    "                #         # Update\n",
    "                #         cur.updateRow(row)\n",
    "                #     del(row)\n",
    "                # del(cur)\n",
    "                # Drop UPPERCASE field form tab area\n",
    "                # arcpy.DeleteField_management(cat_per_north_tabarea,'CAT_ID_TXT_DEL')\n",
    "                # # Append catchment percent north table to list\n",
    "                # cat_pernorth_taba_tables.append(cat_per_north_tabarea)\n",
    "                tabarea_stop = time.time()\n",
    "                tabarea_time = int (tabarea_stop - tabarea_start)\n",
    "                print(f'Tabulate area for {roi} Elapsed time: ({datetime.timedelta(seconds=tabarea_time)})')\n",
    "                print('----------')\n",
    "\n",
    "            except:\n",
    "                e = sys.exc_info()[1]\n",
    "                print(e.args[0])\n",
    "                arcpy.AddError(e.args[0])\n",
    "\n",
    "    else:\n",
    "        print(f'Region {str(roi)} not found in {region}')\n",
    "# End timing\n",
    "processEnd = time.time()\n",
    "processElapsed = int(processEnd - processStart)\n",
    "processSuccess_time = datetime.datetime.now()\n",
    "\n",
    "# Report success\n",
    "print(f'Process completed at {processSuccess_time.strftime(\"%Y-%m-%d %H:%M\")} '\n",
    "      f'(Elapsed time: {datetime.timedelta(seconds=processElapsed)})')\n",
    "print('----------')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Drop unnecessary fields and rename as needed from merged tables.\n",
    "- Create Key value dictionary and use update cursor to rename fields."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "# Table names/paths\n",
    "wtd_per_north_table_out = os.path.join(outgdb, 'AKSSF_wtd_north_per')\n",
    "cat_elev_table_out = os.path.join(outgdb,'AKSSF_cat_elev')\n",
    "wtd_elev_table_out = os.path.join(outgdb, 'AKSSF_wtd_elev')\n",
    "\n",
    "# Merge all regional tables together\n",
    "wtd_per_north = arcpy.Merge_management(wtd_pernorth_taba_tables, wtd_per_north_table_out)\n",
    "cat_elev = arcpy.Merge_management(cat_elev_ztables, cat_elev_table_out)\n",
    "wtd_elev = arcpy.Merge_management(wtd_elev_ztables, wtd_elev_table_out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZONE_CODE wtd_elev_ZONE_CODE\n",
      "COUNT wtd_elev_COUNT\n",
      "AREA wtd_elev_AREA\n",
      "MIN wtd_elev_MIN\n",
      "MAX wtd_elev_MAX\n",
      "RANGE wtd_elev_RANGE\n",
      "MEAN wtd_elev_MEAN\n",
      "STD wtd_elev_STD\n",
      "SUM wtd_elev_SUM\n",
      "VARIETY wtd_elev_VARIETY\n",
      "MAJORITY wtd_elev_MAJORITY\n",
      "MINORITY wtd_elev_MINORITY\n",
      "MEDIAN wtd_elev_MEDIAN\n",
      "PCT90 wtd_elev_PCT90\n",
      "ZONE_CODE cat_elev_ZONE_CODE\n",
      "COUNT cat_elev_COUNT\n",
      "AREA cat_elev_AREA\n",
      "MIN cat_elev_MIN\n",
      "MAX cat_elev_MAX\n",
      "RANGE cat_elev_RANGE\n",
      "MEAN cat_elev_MEAN\n",
      "STD cat_elev_STD\n",
      "SUM cat_elev_SUM\n",
      "VARIETY cat_elev_VARIETY\n",
      "MAJORITY cat_elev_MAJORITY\n",
      "MINORITY cat_elev_MINORITY\n",
      "MEDIAN cat_elev_MEDIAN\n",
      "PCT90 cat_elev_PCT90\n"
     ]
    }
   ],
   "source": [
    "#Set up field dictionary\n",
    "dict = { 'ZONE_CODE': ('cat_elev_ZONE_CODE', 'wtd_elev_ZONE_CODE'),\n",
    "         'COUNT': ('cat_elev_COUNT', 'wtd_elev_COUNT'),\n",
    "          'AREA': ('cat_elev_AREA', 'wtd_elev_AREA'),\n",
    "          'MIN': ('cat_elev_MIN', 'wtd_elev_MIN'),\n",
    "          'MAX': ('cat_elev_MAX', 'wtd_elev_MAX'),\n",
    "          'RANGE': ('cat_elev_RANGE', 'wtd_elev_RANGE'),\n",
    "          'MEAN': ('cat_elev_MEAN', 'wtd_elev_MEAN'),\n",
    "          'STD': ('cat_elev_STD', 'wtd_elev_STD'),\n",
    "          'SUM': ('cat_elev_SUM', 'wtd_elev_SUM'),\n",
    "          'VARIETY': ('cat_elev_VARIETY', 'wtd_elev_VARIETY'),\n",
    "          'MAJORITY': ('cat_elev_MAJORITY', 'wtd_elev_MAJORITY'),\n",
    "          'MINORITY': ('cat_elev_MINORITY', 'wtd_elev_MINORITY'),\n",
    "          'MEDIAN': ('cat_elev_MEDIAN', 'wtd_elev_MEDIAN'),\n",
    "          'PCT90': ('cat_elev_PCT90', 'wtd_elev_PCT90')\n",
    "         }\n",
    "\n",
    "# Rename fields\n",
    "for field in arcpy.ListFields(wtd_elev):\n",
    "    keyval = field.name\n",
    "    if keyval in dict:\n",
    "        newname = dict[keyval][1]\n",
    "        newalias = dict[keyval][1]\n",
    "        print (keyval, newname)\n",
    "        arcpy.AlterField_management(wtd_elev, keyval, newname, newalias)\n",
    "\n",
    "for field in arcpy.ListFields(cat_elev):\n",
    "    keyval = field.name\n",
    "    if keyval in dict:\n",
    "        newname = dict[keyval][0]\n",
    "        newalias = dict[keyval][0]\n",
    "        print (keyval, newname)\n",
    "        arcpy.AlterField_management(cat_elev, keyval, newname, newalias)\n",
    "\n",
    "# for field in arcpy.ListFields(wtd_per_north):\n",
    "#     keyval = field.name\n",
    "#     if keyval in dict:\n",
    "#         newname = dict[keyval][0]\n",
    "#         newalias = dict[keyval][0]\n",
    "#         print (keyval, newname)\n",
    "#         arcpy.AlterField_management(cat_elev, keyval, newname, newalias)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert tables to pandas df and explore results\n",
    "* **Merge tables and export to csv**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.2f}'.format # only display 2 decimal places"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJECTID OBJECTID\n",
      "cat_ID_txt cat_ID_txt\n",
      "cat_elev_ZONE_CODE cat_elev_ZONE_CODE\n",
      "cat_elev_COUNT cat_elev_COUNT\n",
      "cat_elev_AREA cat_elev_AREA\n",
      "cat_elev_MIN cat_elev_MIN\n",
      "cat_elev_MAX cat_elev_MAX\n",
      "cat_elev_RANGE cat_elev_RANGE\n",
      "cat_elev_MEAN cat_elev_MEAN\n",
      "cat_elev_STD cat_elev_STD\n",
      "cat_elev_SUM cat_elev_SUM\n",
      "cat_elev_VARIETY cat_elev_VARIETY\n",
      "cat_elev_MAJORITY cat_elev_MAJORITY\n",
      "cat_elev_MINORITY cat_elev_MINORITY\n",
      "cat_elev_MEDIAN cat_elev_MEDIAN\n",
      "cat_elev_PCT90 cat_elev_PCT90\n",
      "region region\n"
     ]
    },
    {
     "data": {
      "text/plain": "            cat_elev_COUNT  cat_elev_AREA  cat_elev_MIN  cat_elev_MAX  \\\ncat_ID_txt                                                              \n48267              7607.00      760700.00             9            48   \n49617              1470.00      147000.00            12            54   \n50197               819.00       81900.00            13            35   \n64593              2080.00      208000.00            13            75   \n72144               657.00       65700.00            38           121   \n76954              4168.00      416800.00            29            80   \n77794              4689.00      468900.00            15            88   \n90346             19618.00     1961800.00            81           638   \n93176               853.00       85300.00           123           195   \n94216              6507.00      650700.00           120           508   \n97276              7207.00      720700.00            81           378   \n99516              7097.00      709700.00           126           659   \n100826              912.00       91200.00           121           167   \n101556             2371.00      237100.00           121           162   \n103096              736.00       73600.00           124           130   \n103196            16141.00     1614100.00            48           387   \n103296             2391.00      239100.00           131           183   \n103456             6314.00      631400.00           125           171   \n103496              766.00       76600.00            18            57   \n106626             3158.00      315800.00           123           759   \n106676              977.00       97700.00           114           155   \n107796             1874.00      187400.00            46           143   \n107816             2566.00      256600.00            42           347   \n108356             5022.00      502200.00             8           105   \n128685              514.00       51400.00            19            79   \n129955             1777.00      177700.00           114           144   \n130295             2050.00      205000.00           106           144   \n130735             2197.00      219700.00            12           319   \n\n            cat_elev_RANGE  cat_elev_MEAN  cat_elev_STD  cat_elev_SUM  \\\ncat_ID_txt                                                              \n48267                   39          30.06          8.90     228638.00   \n49617                   42          19.59          9.47      28800.00   \n50197                   22          21.07          5.38      17260.00   \n64593                   62          45.99         13.43      95652.00   \n72144                   83          80.80         19.10      53086.00   \n76954                   51          41.13          8.84     171422.00   \n77794                   73          27.75         10.86     130100.00   \n90346                  557         183.12        112.56    3592436.00   \n93176                   72         140.28         16.51     119656.00   \n94216                  388         217.61         84.27    1415989.00   \n97276                  297         132.64         46.22     955940.00   \n99516                  533         263.11        126.91    1867312.00   \n100826                  46         128.89          9.84     117545.00   \n101556                  41         128.70          6.87     305149.00   \n103096                   6         125.48          0.78      92355.00   \n103196                 339          74.64         60.76    1204734.00   \n103296                  52         149.84         16.25     358273.00   \n103456                  46         129.17          7.56     815566.00   \n103496                  39          30.68         10.11      23500.00   \n106626                 636         272.65        176.56     861020.00   \n106676                  41         133.51          9.31     130440.00   \n107796                  97          65.46         17.72     122681.00   \n107816                 305         131.10         82.50     336400.00   \n108356                  97          30.75         17.20     154438.00   \n128685                  60          32.64         14.86      16775.00   \n129955                  30         120.83          6.19     214720.00   \n130295                  38         123.86          9.61     253905.00   \n130735                 307          67.69         54.58     148725.00   \n\n            cat_elev_VARIETY  cat_elev_MAJORITY  cat_elev_MINORITY  \\\ncat_ID_txt                                                           \n48267                     40                 34                  9   \n49617                     43                 15                 34   \n50197                     23                 14                 33   \n64593                     63                 49                 14   \n72144                     84                 74                 38   \n76954                     52                 33                 29   \n77794                     74                 21                 83   \n90346                    546                 81                513   \n93176                     67                131                181   \n94216                    376                121                402   \n97276                    261                118                302   \n99516                    519                129                543   \n100826                    45                123                121   \n101556                    42                127                158   \n103096                     7                125                129   \n103196                   332                 54                192   \n103296                    53                134                183   \n103456                    47                126                169   \n103496                    40                 20                 18   \n106626                   583                150                239   \n106676                    42                125                117   \n107796                    89                 47                113   \n107816                   301                 48                 42   \n108356                    94                 14                  9   \n128685                    60                 22                 60   \n129955                    31                116                144   \n130295                    39                109                106   \n130735                   250                 19                146   \n\n            cat_elev_MEDIAN  cat_elev_PCT90  region  \ncat_ID_txt                                           \n48267                    32              40  Kodiak  \n49617                    16              30  Kodiak  \n50197                    21              28  Kodiak  \n64593                    47              63  Kodiak  \n72144                    79             106  Kodiak  \n76954                    38              54  Kodiak  \n77794                    24              38  Kodiak  \n90346                   140             353  Kodiak  \n93176                   133             168  Kodiak  \n94216                   193             348  Kodiak  \n97276                   120             199  Kodiak  \n99516                   224             464  Kodiak  \n100826                  124             145  Kodiak  \n101556                  127             137  Kodiak  \n103096                  125             126  Kodiak  \n103196                   56             111  Kodiak  \n103296                  143             171  Kodiak  \n103456                  127             133  Kodiak  \n103496                   28              48  Kodiak  \n106626                  169             577  Kodiak  \n106676                  130             148  Kodiak  \n107796                   62              89  Kodiak  \n107816                  111             265  Kodiak  \n108356                   22              54  Kodiak  \n128685                   26              58  Kodiak  \n129955                  119             131  Kodiak  \n130295                  123             139  Kodiak  \n130735                   51             137  Kodiak  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat_elev_COUNT</th>\n      <th>cat_elev_AREA</th>\n      <th>cat_elev_MIN</th>\n      <th>cat_elev_MAX</th>\n      <th>cat_elev_RANGE</th>\n      <th>cat_elev_MEAN</th>\n      <th>cat_elev_STD</th>\n      <th>cat_elev_SUM</th>\n      <th>cat_elev_VARIETY</th>\n      <th>cat_elev_MAJORITY</th>\n      <th>cat_elev_MINORITY</th>\n      <th>cat_elev_MEDIAN</th>\n      <th>cat_elev_PCT90</th>\n      <th>region</th>\n    </tr>\n    <tr>\n      <th>cat_ID_txt</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>48267</th>\n      <td>7607.00</td>\n      <td>760700.00</td>\n      <td>9</td>\n      <td>48</td>\n      <td>39</td>\n      <td>30.06</td>\n      <td>8.90</td>\n      <td>228638.00</td>\n      <td>40</td>\n      <td>34</td>\n      <td>9</td>\n      <td>32</td>\n      <td>40</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>49617</th>\n      <td>1470.00</td>\n      <td>147000.00</td>\n      <td>12</td>\n      <td>54</td>\n      <td>42</td>\n      <td>19.59</td>\n      <td>9.47</td>\n      <td>28800.00</td>\n      <td>43</td>\n      <td>15</td>\n      <td>34</td>\n      <td>16</td>\n      <td>30</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>50197</th>\n      <td>819.00</td>\n      <td>81900.00</td>\n      <td>13</td>\n      <td>35</td>\n      <td>22</td>\n      <td>21.07</td>\n      <td>5.38</td>\n      <td>17260.00</td>\n      <td>23</td>\n      <td>14</td>\n      <td>33</td>\n      <td>21</td>\n      <td>28</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>64593</th>\n      <td>2080.00</td>\n      <td>208000.00</td>\n      <td>13</td>\n      <td>75</td>\n      <td>62</td>\n      <td>45.99</td>\n      <td>13.43</td>\n      <td>95652.00</td>\n      <td>63</td>\n      <td>49</td>\n      <td>14</td>\n      <td>47</td>\n      <td>63</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>72144</th>\n      <td>657.00</td>\n      <td>65700.00</td>\n      <td>38</td>\n      <td>121</td>\n      <td>83</td>\n      <td>80.80</td>\n      <td>19.10</td>\n      <td>53086.00</td>\n      <td>84</td>\n      <td>74</td>\n      <td>38</td>\n      <td>79</td>\n      <td>106</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>76954</th>\n      <td>4168.00</td>\n      <td>416800.00</td>\n      <td>29</td>\n      <td>80</td>\n      <td>51</td>\n      <td>41.13</td>\n      <td>8.84</td>\n      <td>171422.00</td>\n      <td>52</td>\n      <td>33</td>\n      <td>29</td>\n      <td>38</td>\n      <td>54</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>77794</th>\n      <td>4689.00</td>\n      <td>468900.00</td>\n      <td>15</td>\n      <td>88</td>\n      <td>73</td>\n      <td>27.75</td>\n      <td>10.86</td>\n      <td>130100.00</td>\n      <td>74</td>\n      <td>21</td>\n      <td>83</td>\n      <td>24</td>\n      <td>38</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>90346</th>\n      <td>19618.00</td>\n      <td>1961800.00</td>\n      <td>81</td>\n      <td>638</td>\n      <td>557</td>\n      <td>183.12</td>\n      <td>112.56</td>\n      <td>3592436.00</td>\n      <td>546</td>\n      <td>81</td>\n      <td>513</td>\n      <td>140</td>\n      <td>353</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>93176</th>\n      <td>853.00</td>\n      <td>85300.00</td>\n      <td>123</td>\n      <td>195</td>\n      <td>72</td>\n      <td>140.28</td>\n      <td>16.51</td>\n      <td>119656.00</td>\n      <td>67</td>\n      <td>131</td>\n      <td>181</td>\n      <td>133</td>\n      <td>168</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>94216</th>\n      <td>6507.00</td>\n      <td>650700.00</td>\n      <td>120</td>\n      <td>508</td>\n      <td>388</td>\n      <td>217.61</td>\n      <td>84.27</td>\n      <td>1415989.00</td>\n      <td>376</td>\n      <td>121</td>\n      <td>402</td>\n      <td>193</td>\n      <td>348</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>97276</th>\n      <td>7207.00</td>\n      <td>720700.00</td>\n      <td>81</td>\n      <td>378</td>\n      <td>297</td>\n      <td>132.64</td>\n      <td>46.22</td>\n      <td>955940.00</td>\n      <td>261</td>\n      <td>118</td>\n      <td>302</td>\n      <td>120</td>\n      <td>199</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>99516</th>\n      <td>7097.00</td>\n      <td>709700.00</td>\n      <td>126</td>\n      <td>659</td>\n      <td>533</td>\n      <td>263.11</td>\n      <td>126.91</td>\n      <td>1867312.00</td>\n      <td>519</td>\n      <td>129</td>\n      <td>543</td>\n      <td>224</td>\n      <td>464</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>100826</th>\n      <td>912.00</td>\n      <td>91200.00</td>\n      <td>121</td>\n      <td>167</td>\n      <td>46</td>\n      <td>128.89</td>\n      <td>9.84</td>\n      <td>117545.00</td>\n      <td>45</td>\n      <td>123</td>\n      <td>121</td>\n      <td>124</td>\n      <td>145</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>101556</th>\n      <td>2371.00</td>\n      <td>237100.00</td>\n      <td>121</td>\n      <td>162</td>\n      <td>41</td>\n      <td>128.70</td>\n      <td>6.87</td>\n      <td>305149.00</td>\n      <td>42</td>\n      <td>127</td>\n      <td>158</td>\n      <td>127</td>\n      <td>137</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>103096</th>\n      <td>736.00</td>\n      <td>73600.00</td>\n      <td>124</td>\n      <td>130</td>\n      <td>6</td>\n      <td>125.48</td>\n      <td>0.78</td>\n      <td>92355.00</td>\n      <td>7</td>\n      <td>125</td>\n      <td>129</td>\n      <td>125</td>\n      <td>126</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>103196</th>\n      <td>16141.00</td>\n      <td>1614100.00</td>\n      <td>48</td>\n      <td>387</td>\n      <td>339</td>\n      <td>74.64</td>\n      <td>60.76</td>\n      <td>1204734.00</td>\n      <td>332</td>\n      <td>54</td>\n      <td>192</td>\n      <td>56</td>\n      <td>111</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>103296</th>\n      <td>2391.00</td>\n      <td>239100.00</td>\n      <td>131</td>\n      <td>183</td>\n      <td>52</td>\n      <td>149.84</td>\n      <td>16.25</td>\n      <td>358273.00</td>\n      <td>53</td>\n      <td>134</td>\n      <td>183</td>\n      <td>143</td>\n      <td>171</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>103456</th>\n      <td>6314.00</td>\n      <td>631400.00</td>\n      <td>125</td>\n      <td>171</td>\n      <td>46</td>\n      <td>129.17</td>\n      <td>7.56</td>\n      <td>815566.00</td>\n      <td>47</td>\n      <td>126</td>\n      <td>169</td>\n      <td>127</td>\n      <td>133</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>103496</th>\n      <td>766.00</td>\n      <td>76600.00</td>\n      <td>18</td>\n      <td>57</td>\n      <td>39</td>\n      <td>30.68</td>\n      <td>10.11</td>\n      <td>23500.00</td>\n      <td>40</td>\n      <td>20</td>\n      <td>18</td>\n      <td>28</td>\n      <td>48</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>106626</th>\n      <td>3158.00</td>\n      <td>315800.00</td>\n      <td>123</td>\n      <td>759</td>\n      <td>636</td>\n      <td>272.65</td>\n      <td>176.56</td>\n      <td>861020.00</td>\n      <td>583</td>\n      <td>150</td>\n      <td>239</td>\n      <td>169</td>\n      <td>577</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>106676</th>\n      <td>977.00</td>\n      <td>97700.00</td>\n      <td>114</td>\n      <td>155</td>\n      <td>41</td>\n      <td>133.51</td>\n      <td>9.31</td>\n      <td>130440.00</td>\n      <td>42</td>\n      <td>125</td>\n      <td>117</td>\n      <td>130</td>\n      <td>148</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>107796</th>\n      <td>1874.00</td>\n      <td>187400.00</td>\n      <td>46</td>\n      <td>143</td>\n      <td>97</td>\n      <td>65.46</td>\n      <td>17.72</td>\n      <td>122681.00</td>\n      <td>89</td>\n      <td>47</td>\n      <td>113</td>\n      <td>62</td>\n      <td>89</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>107816</th>\n      <td>2566.00</td>\n      <td>256600.00</td>\n      <td>42</td>\n      <td>347</td>\n      <td>305</td>\n      <td>131.10</td>\n      <td>82.50</td>\n      <td>336400.00</td>\n      <td>301</td>\n      <td>48</td>\n      <td>42</td>\n      <td>111</td>\n      <td>265</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>108356</th>\n      <td>5022.00</td>\n      <td>502200.00</td>\n      <td>8</td>\n      <td>105</td>\n      <td>97</td>\n      <td>30.75</td>\n      <td>17.20</td>\n      <td>154438.00</td>\n      <td>94</td>\n      <td>14</td>\n      <td>9</td>\n      <td>22</td>\n      <td>54</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>128685</th>\n      <td>514.00</td>\n      <td>51400.00</td>\n      <td>19</td>\n      <td>79</td>\n      <td>60</td>\n      <td>32.64</td>\n      <td>14.86</td>\n      <td>16775.00</td>\n      <td>60</td>\n      <td>22</td>\n      <td>60</td>\n      <td>26</td>\n      <td>58</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>129955</th>\n      <td>1777.00</td>\n      <td>177700.00</td>\n      <td>114</td>\n      <td>144</td>\n      <td>30</td>\n      <td>120.83</td>\n      <td>6.19</td>\n      <td>214720.00</td>\n      <td>31</td>\n      <td>116</td>\n      <td>144</td>\n      <td>119</td>\n      <td>131</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>130295</th>\n      <td>2050.00</td>\n      <td>205000.00</td>\n      <td>106</td>\n      <td>144</td>\n      <td>38</td>\n      <td>123.86</td>\n      <td>9.61</td>\n      <td>253905.00</td>\n      <td>39</td>\n      <td>109</td>\n      <td>106</td>\n      <td>123</td>\n      <td>139</td>\n      <td>Kodiak</td>\n    </tr>\n    <tr>\n      <th>130735</th>\n      <td>2197.00</td>\n      <td>219700.00</td>\n      <td>12</td>\n      <td>319</td>\n      <td>307</td>\n      <td>67.69</td>\n      <td>54.58</td>\n      <td>148725.00</td>\n      <td>250</td>\n      <td>19</td>\n      <td>146</td>\n      <td>51</td>\n      <td>137</td>\n      <td>Kodiak</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make catchment elev df\n",
    "cat_df = pd.DataFrame()\n",
    "cat_field_list = []\n",
    "for field in arcpy.ListFields(cat_elev):\n",
    "    cat_field_list.append(field.name)\n",
    "    print (field.name, field.aliasName)\n",
    "cat_elev_arr = arcpy.da.TableToNumPyArray(cat_elev,cat_field_list)\n",
    "cat_df = pd.DataFrame(cat_elev_arr)\n",
    "cat_df = cat_df.drop([\"OBJECTID\",\"cat_elev_ZONE_CODE\"],axis=1)\n",
    "cat_df = cat_df.set_index('cat_ID_txt')\n",
    "cat_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed elev df\n",
    "wtd_df = pd.DataFrame()\n",
    "wtd_field_list = []\n",
    "for field in arcpy.ListFields(wtd_elev):\n",
    "    wtd_field_list.append(field.name)\n",
    "wtd_elev_arr = arcpy.da.TableToNumPyArray(wtd_elev,wtd_field_list)\n",
    "wtd_df = pd.DataFrame(wtd_elev_arr)\n",
    "wtd_df = wtd_df.drop([\"OBJECTID\",\"wtd_elev_ZONE_CODE\"],axis=1)\n",
    "wtd_df = wtd_df.set_index('cat_ID_txt')\n",
    "wtd_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed north df\n",
    "wtd_n_df = pd.DataFrame()\n",
    "wtd_n_field_list = []\n",
    "for field in arcpy.ListFields(wtd_per_north):\n",
    "    wtd_n_field_list.append(field.name)\n",
    "wtd_n_arr = arcpy.da.TableToNumPyArray(wtd_per_north,wtd_n_field_list)\n",
    "wtd_n_df = pd.DataFrame(wtd_n_arr)\n",
    "wtd_n_df = wtd_n_df.drop(\"OBJECTID\",axis=1)\n",
    "wtd_n_df = wtd_n_df.set_index('cat_ID_txt')\n",
    "wtd_n_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "landmet_merge_df = pd.merge(wtd_df,cat_df,on='cat_ID_txt', how='outer')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfs = [wtd_df,cat_df]\n",
    "df_final = reduce(lambda left,right: pd.merge(left,right,on='cat_ID_txt'), dfs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_final"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8caab41e",
   "language": "python",
   "display_name": "PyCharm (AKSSF)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}