{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a id='top'></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Watershed and Catchment Landcover Metrics\n",
    "\n",
    "## Aspect and Percent North Covariates\n",
    "Calculate aspect grid with the DEM used to create the flow network for each processing area (e.g. HUC8 in NHDPlus or\n",
    " region for BB, PWS, or Kodiak). We need the DEMs used for each flow direction grid, or we can't calculate covariates at\n",
    " the watershed scale.\n",
    " * **~~aspect_rch = calculate mean aspect for stream reach (zonal statistics)~~**\n",
    " * **~~aspect_cat = calculate mean aspect over catchments (zonal statistics)~~**\n",
    " * **north_wtd = create 1/0 grid of north facing cells from aspect grid (north = aspects from 315-45 degrees), ~~~use\n",
    "  weighted flow accumulation to sum north facing cells, divide by flow accumulation grid to get % of north facing\n",
    "  cells in each watershed~~~ Use tabulate area to quantify area of North cells (VALUE_1) and non-North (VALUE_0) cells\n",
    "  for each watershed and calculate percent north as north_wtd = row[VALUE_1]/(row[VALUE_1]+row[VALUE_0])*100 .**\n",
    "## Elevation Metrics\n",
    "Calculate elevation metrics for catchment/watershed with temperature data using zonal statistics as table.\n",
    "* <b> Running \"ALL\" zonal statistics as it does not allow you to choose two types of statistics\n",
    "(ex statistics_type = [\"MIN\",\"MAX\"]) instead set statistics_type=\"ALL\"</b>\n",
    "### Catchment Elevation Metrics\n",
    "* **cat_elev_mn = mean elevation for catchment**\n",
    "* **cat_elev_min = minimum elevation for catchment**\n",
    "* **cat_elev_max = max elevation for catchment**\n",
    "* **cat_elev_std = standard deviation of elevation for catchment**\n",
    "### Watershed Elevation Metrics\n",
    "* **wtd_elev_mn = mean watershed elevation**\n",
    "* **wtd_elev_min = min watershed elevation**\n",
    "* **wtd_elev_max = max watershed elevation**\n",
    "* **wtd_elev_sd (or cv) = standard deviation of watershed elevation**\n",
    "## Slope Metrics\n",
    "Calculate slope metrics for catchment/watershed with temperature data using zonal statistics as table and\n",
    "statistics_type = \"ALL\".\n",
    "### Catchment Slope Metrics\n",
    "* **cat_slope_mn = mean slope for catchment**\n",
    "* **cat_slope_min = minimum slope for catchment**\n",
    "* **cat_slope_max = max slope for catchment**\n",
    "* **cat_slope_std = standard deviation of slope for catchment**\n",
    "### Watershed Elevation Metrics\n",
    "* **wtd_slope_mn = mean watershed slope**\n",
    "* **wtd_slope_min = min watershed slope**\n",
    "* **wtd_slope_max = max watershed slope**\n",
    "* **wtd_slope_sd (or cv) = standard deviation of watershed slope**\n",
    "## Lake, Wetland and Glacier Cover\n",
    "Lake/Pond type waterbodies from NHDPLus hydrography for those regions with NHDPlus derived datasets and NHD waterbodies\n",
    "for those regions with TauDEM derived waterbodies.  Merge all waterbodies together (FTYPE = 390) and use tabulate area\n",
    "calculate percent cover of lakes in catchments/watersheds.\n",
    " * **Created local copies of NHDPlus and NHD waterbodies and exported to T driver here <>  **\n",
    "Use NLCD grid (wetlands.tif) and tabulate area on wetlands grid with watersheds (sum) / divide by number of cells in\n",
    "watershed from fac grid, wetlands from NLCD\n",
    "## Distance from coast\n",
    "DO NOT WORK ON THIS\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import modules"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29102021\n",
      "C:\\Users\\dwmerrigan\\Documents\\GitHub\\AKSSF\\data_preparation\\sensitivity_drivers\\landcover\n",
      "C:\\Users\\dwmerrigan\\AppData\\Local\\Programs\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\n"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "arcpy.env.overwriteOutput = True\n",
    "today = datetime.datetime.now()\n",
    "# Make the time stamp.\n",
    "time_stamp = '{:%d%m%Y}'.format(today)\n",
    "print(time_stamp)\n",
    "\n",
    "path = os.getcwd()\n",
    "print (path)\n",
    "print (sys.base_exec_prefix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create working output location to store intermediate data\n",
    "Create folder and gdb to store output data.\n",
    "Set data_dir to folder containing all AKSSF regional subfolders/geoadatabases\n",
    "* RS data folder: data_dir = r\"W:\\GIS\\AKSSF\"\n",
    "* DM data folder: data_dir = r\"D:\\GIS_Temp\\AKSSF_BeckyCopy\\AKSSF\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\GIS_temp\\\\AKSSF\\\\Bristol_Bay', 'D:\\\\GIS_temp\\\\AKSSF\\\\Cook_Inlet', 'D:\\\\GIS_temp\\\\AKSSF\\\\Copper_River', 'D:\\\\GIS_temp\\\\AKSSF\\\\Kodiak', 'D:\\\\GIS_temp\\\\AKSSF\\\\Prince_William_Sound']\n",
      "Working Folder already created... D:\\GIS_temp\\AKSSF_land_met\n",
      "Output location already exists D:\\GIS_temp\\AKSSF_land_met\\AKSSF_land_met.gdb\n"
     ]
    }
   ],
   "source": [
    "# Set AKSSF Data directory\n",
    "# data_dir = r\"W:\\GIS\\AKSSF\"\n",
    "\n",
    "data_dir = r\"D:\\GIS_Temp\\AKSSF\"\n",
    "arcpy.env.workspace = data_dir\n",
    "regions = arcpy.ListWorkspaces(workspace_type=\"Folder\")\n",
    "rois= []\n",
    "print (regions)\n",
    "\n",
    "# Path to create output folder/gdb\n",
    "temppath = r\"D:\\GIS_temp\" # Output folder\n",
    "dirname = 'AKSSF_land_met'\n",
    "tempgdbname = 'AKSSF_land_met.gdb'\n",
    "temp_dir = os.path.join(temppath, dirname)\n",
    "\n",
    "# Create temporary working gdb\n",
    "if not arcpy.Exists(temp_dir):\n",
    "    os.makedirs(temp_dir)\n",
    "else:\n",
    "    print('Working Folder already created...', temp_dir)\n",
    "\n",
    "outcheck = os.path.join(temp_dir, tempgdbname)\n",
    "\n",
    "if arcpy.Exists(outcheck):\n",
    "    print ('Output location already exists', outcheck)\n",
    "    outgdb = outcheck\n",
    "if not arcpy.Exists(outcheck):\n",
    "    print('Creating output GDB')\n",
    "    tempgdb = arcpy.CreateFileGDB_management(temp_dir,tempgdbname)\n",
    "    print ('Output geodatabase created at', outcheck)\n",
    "    outgdb = tempgdb.getOutput(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "['D:\\\\GIS_temp\\\\AKSSF\\\\Bristol_Bay',\n 'D:\\\\GIS_temp\\\\AKSSF\\\\Cook_Inlet',\n 'D:\\\\GIS_temp\\\\AKSSF\\\\Copper_River',\n 'D:\\\\GIS_temp\\\\AKSSF\\\\Kodiak',\n 'D:\\\\GIS_temp\\\\AKSSF\\\\Prince_William_Sound']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set regional workspaces from AKSSF data folder and store in list\n",
    "arcpy.env.workspace = data_dir\n",
    "# Use All of the cores on the machine\n",
    "arcpy.env.parallelProcessingFactor = \"100%\"\n",
    "\n",
    "regions  = arcpy.ListWorkspaces(workspace_type=\"Folder\")\n",
    "regions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create aspect and north grids if they do not already exists\n",
    "### NHDPlus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Code to create merged elevation grid, aspect and north aspect grids for entire AKSSF region with NHDPlus elev data\n",
    "\n",
    "# elrasters = []\n",
    "# nhd_dat = r\"C:\\\\Users\\\\dwmerrigan\\\\Documents\\\\GitHub\\\\AKSSF\\\\hydrography\\\\AKSSF_NHDPlus\\\\extracts\"\n",
    "# for dirpath, dirname, filenames in arcpy.da.Walk(nhd_dat, 'RasterDataset'):\n",
    "#     for filename in filenames:\n",
    "#         if filename == \"elev_cm.tif\":\n",
    "#             elras = os.path.join(dirpath, filename)\n",
    "#             elrasters.append(elras)\n",
    "#             print (f'Appending {elras} to list')\n",
    "#\n",
    "# spatial_ref = arcpy.Describe(elrasters[0]).spatialReference\n",
    "# print (spatial_ref.name)\n",
    "# # Merge all nhdplus elevation rasters together\n",
    "# elevname = 'AKSSF_NHDPlus_elev_cm.tif'\n",
    "# AKSSF_elev_cm = arcpy.MosaicToNewRaster_management(elrasters,temp_dir,elevname, spatial_ref, \"32_BIT_SIGNED\", \"5\", \"1\", \"LAST\",\"FIRST\")\n",
    "#\n",
    "# #make aspect raster for all of akssf from merged nhdplus\n",
    "# from arcpy.sa import *\n",
    "# aspname = 'AKSSF_NHDPlus_aspect.tif'\n",
    "# asp_rast = Aspect(in_raster= AKSSF_elev_cm, method='Planar')\n",
    "# asp_rast.save(os.path.join(temp_dir, aspname))\n",
    "#\n",
    "# # make north raster from aspect raster\n",
    "# norname = 'AKSSF_NHDPlus_north_aspect.tif'\n",
    "# aspect_in = Raster(asp_rast)\n",
    "# north_raster = Con((aspect_in>=0)&(aspect_in<=45),1,Con((aspect_in<=360)&(aspect_in>=315),1,0))\n",
    "# north_raster.save(os.path.join(temp_dir, norname))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TauDEM - Create grids using Timms Composite DEM for entire TauDEM processed areas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Code to create merged elevation grid, aspect and north aspect grids for entire AKSSF region with TauDEM elev data\n",
    "# compDEM = r\"C:\\Users\\dwmerrigan\\Documents\\GitHub\\AKSSF\\hydrography\\data\\topography\\AKSSF_Elevation_Composite_10m.tif\"\n",
    "# clip to study area\n",
    "# AKSSF_elev_10m = r\"D:\\\\GIS_temp\\\\AKSSF_Composite_10m_extract.tif\"\n",
    "#\n",
    "# #make aspect raster for all of akssf from merged nhdplus\n",
    "# from arcpy.sa import *\n",
    "# aspname = 'AKSSF_Composite_10m_aspect.tif'\n",
    "# asp_rast = Aspect(in_raster= AKSSF_elev_10m, method='Planar')\n",
    "# asp_rast.save(os.path.join(temp_dir, aspname))\n",
    "#\n",
    "# # make north raster from aspect raster\n",
    "# norname = 'AKSSF_Composite_10m_north_aspect.tif'\n",
    "# aspect_in = Raster(asp_rast)\n",
    "# north_raster = Con((aspect_in>=0)&(aspect_in<=45),1,Con((aspect_in<=360)&(aspect_in>=315),1,0))\n",
    "# north_raster.save(os.path.join(temp_dir, norname))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Begin GIS portion\n",
    "- May be faster and save more space to create aspect rasters here using extract by mask?\n",
    "- May be able to use aspect raster with tabulate intersection to calculate percent North (aspects from 315-45 degrees)\n",
    "\n",
    "### <b>UPDATE - 2021-10-12 Do not run aspect metrics at this time. Only need to run percent North for watersheds</b>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cook_Inlet using data from D:\\GIS_temp\\AKSSF\\Cook_Inlet folder\n",
      "Cook_Inlet in ['Cook_Inlet', 'Copper_River'] AKSSF list, using cat_fields ['cat_ID_txt', 'NHDPlusID', 'cat_ID_con'] and watershed fields ['cat_ID_txt', 'cat_ID', 'cat_ID_con']\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-6-5a898ad24d94>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    174\u001B[0m                         \u001B[0mcatspath\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdirpath\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    175\u001B[0m                         \u001B[0mcatsname\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mroi\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;34m\"_\"\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 176\u001B[1;33m                         \u001B[0mcats\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0marcpy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mFeatureClassToFeatureClass_conversion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcatspath\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutgdb\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mcatsname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    177\u001B[0m                         \u001B[0mcatlstfields\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0marcpy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mListFields\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcats\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    178\u001B[0m                         \u001B[0mcatfieldnames\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\conversion.py\u001B[0m in \u001B[0;36mFeatureClassToFeatureClass\u001B[1;34m(in_features, out_path, out_name, where_clause, field_mapping, config_keyword)\u001B[0m\n\u001B[0;32m   2348\u001B[0m     \u001B[1;32mfrom\u001B[0m \u001B[0marcpy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marcobjects\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marcobjectconversion\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mconvertArcObjectToPythonObject\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2349\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2350\u001B[1;33m         \u001B[0mretval\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconvertArcObjectToPythonObject\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mFeatureClassToFeatureClass_conversion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mgp_fixargs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0min_features\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwhere_clause\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfield_mapping\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig_keyword\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2351\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mretval\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2352\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\geoprocessing\\_base.py\u001B[0m in \u001B[0;36m__getattr__\u001B[1;34m(self, attr)\u001B[0m\n\u001B[0;32m    508\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mattr\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"__methods__\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    509\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 510\u001B[1;33m         \u001B[0mval\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_gp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    511\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcallable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mval\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    512\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[1;32mlambda\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mval\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mgp_fixargs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from arcpy.sa import *\n",
    "import re\n",
    "\n",
    "# Set data_dir equal to folder containing AKSSF regional subfolders containing GDBs and raster datasets\n",
    "arcpy.env.workspace = data_dir\n",
    "arcpy.env.overwriteOutput = True\n",
    "sr = arcpy.SpatialReference(3338) #'NAD_1983_Alaska_Albers'\n",
    "arcpy.env.outputCoordinateSystem = sr\n",
    "regions  = arcpy.ListWorkspaces(workspace_type=\"Folder\")\n",
    "\n",
    "# Lists for variables not needed at present time\n",
    "#cat_asp_ztables = []\n",
    "#wtd_asp_ztables = []\n",
    "#cat_pernorth_taba_tables=[]\n",
    "\n",
    "# Lists to store output tables\n",
    "wtd_pernorth_taba_tables=[]\n",
    "wtd_lp_tabint_tables = []\n",
    "wtd_glac_tabint_tables = []\n",
    "wtd_wet_taba_tables = []\n",
    "cat_elev_ztables = []\n",
    "wtd_elev_ztables = []\n",
    "cat_slope_ztables = []\n",
    "wtd_slope_ztables = []\n",
    "\n",
    "\n",
    "# Clear lists\n",
    "cat_cur_fields = []\n",
    "wtd_cur_fields = []\n",
    "\n",
    "# Start timing function\n",
    "processStart = time.time()\n",
    "processStartdt = datetime.datetime.now()\n",
    "\n",
    "# Seperate data by\n",
    "nhdplus_dat = ['Cook_Inlet','Copper_River']\n",
    "tauDem_dat = ['Bristol_Bay', 'Kodiak', 'Prince_William_Sound']\n",
    "\n",
    "# Loop through all processing areas\n",
    "rois = nhdplus_dat + tauDem_dat\n",
    "#rois = ['Cook_Inlet']\n",
    "# Uncomment and change list accordingly below to test individual roi/rois\n",
    "# rois = ['Kodiak']\n",
    "for roi in rois:\n",
    "    # Loop through regional folders\n",
    "    for region in regions:\n",
    "        if roi in str(region):\n",
    "            print(f'{roi} using data from {region} folder')\n",
    "            # Set data and variables unique to regions with NHDPlus Data\n",
    "            if roi in nhdplus_dat:\n",
    "                # # NHDPLus raster data for DM pc, variables will be overwritten below if they exist in source folder.\n",
    "                # asp_rast = r\"D:\\Basedata\\AKSSF_Basedata\\AKSSF_NHDPlus_aspect.tif\"\n",
    "                # elev_rast = r\"D:\\Basedata\\AKSSF_Basedata\\AKSSF_NHDPlus_elev_cm.ti\n",
    "                # f\"\n",
    "                # nor_rast = r\"D:\\Basedata\\AKSSF_Basedata\\AKSSF_NHDPlus_north_aspect.tif\"\n",
    "                lakes_fc = r\"D:\\\\Basedata\\\\AKSSF_Basedata\\\\AKSSF_Basedata.gdb\\\\AKSSF_NHDPlus_LakePond_alb\"\n",
    "                # Fields for update cursor\n",
    "                cat_cur_fields = ['cat_ID_txt', 'NHDPlusID',\"cat_ID_con\"]\n",
    "                wtd_cur_fields = ['cat_ID_txt', 'cat_ID',\"cat_ID_con\"]\n",
    "                print (f'{roi} in {nhdplus_dat} AKSSF list, using cat_fields {cat_cur_fields} and watershed fields {wtd_cur_fields}')\n",
    "                # print(f'NHD data: north raster - {nor_rast}')\n",
    "                # print(f'         aspect raster - {asp_rast}')\n",
    "                # print(f'      elevation raster - {elev_rast}')\n",
    "                print('----------')\n",
    "            # Set data and variables unique to regions with TauDEM Data\n",
    "            elif roi in tauDem_dat:\n",
    "                # # TauDEM raster data for DM pc, variables will be overwritten below if they exist in source folder.\n",
    "                # nor_rast = r\"D:\\Basedata\\AKSSF_Basedata\\AKSSF_Composite_10m_north_aspect.tif\"\n",
    "                # asp_rast = r\"D:\\Basedata\\AKSSF_Basedata\\AKSSF_Composite_10m_aspect.tif\"\n",
    "                # elev_rast = r\"D:\\Basedata\\AKSSF_Basedata\\AKSSF_Composite_10m_extract.tif\"\n",
    "\n",
    "                lakes_fc = r\"D:\\\\Basedata\\\\AKSSF_Basedata\\\\AKSSF_Basedata.gdb\\\\AKSSF_NHD_LakesPonds_alb\"\n",
    "                # Fields for update cursor\n",
    "                cat_cur_fields = ['cat_ID_txt', 'gridcode',\"cat_ID_con\"]\n",
    "                wtd_cur_fields = ['cat_ID_txt', 'cat_ID',\"cat_ID_con\"]\n",
    "\n",
    "                print (f'{roi} in {tauDem_dat} TauDEM list, using cat_fields {cat_cur_fields} and watershed fields {wtd_cur_fields}')\n",
    "                # print(f'TauDem data: north raster - {nor_rast}')\n",
    "                # print(f'         aspect raster - {asp_rast}')\n",
    "                # print(f'      elevation raster - {elev_rast}')\n",
    "                print('----------')\n",
    "\n",
    "            wtds = []\n",
    "            cats = []\n",
    "            # Start iter timing function\n",
    "            iteration_start = time.time()\n",
    "            # Set workspace to region folder\n",
    "            arcpy.env.workspace = region\n",
    "            gdb = arcpy.ListWorkspaces(workspace_type='FileGDB')\n",
    "            walk = arcpy.da.Walk(region, datatype = ['FeatureClass','RasterDataset'])\n",
    "            for dirpath, dirnames, filenames in walk:\n",
    "                for filename in filenames:\n",
    "                    # Create list of watersheds\n",
    "                    if ((\"wtd\" in filename) and (\"merge\" not in filename)):\n",
    "                        #print(f\" {filename} watershed identified\")\n",
    "                        wtds.append(os.path.join(dirpath, filename))\n",
    "                    # Set merged watersheds dataset\n",
    "                    elif 'wtds_merge'in filename:\n",
    "                        wtdpath = os.path.join(dirpath,filename)\n",
    "                        wtdname = roi +'_'+filename\n",
    "                        # Make local copy projected in AKAlbers\n",
    "                        wtd_merge = arcpy.FeatureClassToFeatureClass_conversion(wtdpath,outgdb,wtdname)\n",
    "                        print(f'Merged watershed dataset {filename} found')\n",
    "                        print('----------')\n",
    "                        wtdfieldnames = []\n",
    "                        wtdlstFields = arcpy.ListFields(wtd_merge)\n",
    "                        for field in wtdlstFields:\n",
    "                            wtdfieldnames.append(field.name)\n",
    "                        if str(wtd_cur_fields[0]) in wtdfieldnames:\n",
    "                            print (f'{wtd_cur_fields[0]} field already in dataset')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print (f'Adding {wtd_cur_fields[0]} field to watershed dataset {wtd_merge}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_txt field and concat cat_ID + region\n",
    "                            arcpy.AddField_management(wtd_merge, str(wtd_cur_fields[0]),field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(wtd_merge, wtd_cur_fields[0:2]) as cur:\n",
    "                                for row in cur:\n",
    "                                    int_val = int(row[1])\n",
    "                                    str(int_val).replace(\".0\",\"\").replace(\".0\",\"\")\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del(row)\n",
    "                            del(cur)\n",
    "                        if str(wtd_cur_fields[2]) in wtdfieldnames:\n",
    "                            print (f'{wtd_cur_fields[2]} field already in dataset {wtd_merge}')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print (f'Adding {wtd_cur_fields[2]} field to watershed dataset {wtd_merge}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_con field and concat cat_ID + region\n",
    "                            arcpy.AddField_management(wtd_merge, str(wtd_cur_fields[2]),field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(wtd_merge, wtd_cur_fields) as cur:\n",
    "                                for row in cur:\n",
    "                                    int_val = int(row[1])\n",
    "                                    row[2] = str(roi) +'_'+ str(int_val).replace(\".0\",\"\")\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del(row)\n",
    "                            del(cur)\n",
    "\n",
    "                    # Select glaciers fc\n",
    "                    elif 'glaciers' in filename:\n",
    "                        # Make local copy projected in AKAlbers\n",
    "                        glacpath = os.path.join(dirpath, filename)\n",
    "                        glacname = roi+'_'+filename\n",
    "                        glac_fc = arcpy.FeatureClassToFeatureClass_conversion(glacpath,outgdb,glacname)\n",
    "\n",
    "                    # Select elevation raster\n",
    "                    elif 'elev.tif' == filename:\n",
    "                        elev_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "                    # # Select aspect raster\n",
    "                    # elif 'aspect' in filename:\n",
    "                    #     asp_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "                    # Select north raster\n",
    "                    elif 'north.tif' == filename:\n",
    "                        nor_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "                    # Select slope raster\n",
    "                    elif 'slope.tif' == filename:\n",
    "                        slope_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "                    # Select wetland raster\n",
    "                    elif 'wetlands.tif' == filename:\n",
    "                        wet_rast = os.path.join(dirpath, filename)\n",
    "\n",
    "                    # Select catch_int fc (catchments of interest for region) and make a copy\n",
    "                    elif 'cats_intersect' in filename:\n",
    "                        # Make local copy projected in AKAlbers\n",
    "                        catspath = os.path.join(dirpath,filename)\n",
    "                        catsname = roi+\"_\"+filename\n",
    "                        cats = arcpy.FeatureClassToFeatureClass_conversion(catspath, outgdb,catsname)\n",
    "                        catlstfields = arcpy.ListFields(cats)\n",
    "                        catfieldnames = []\n",
    "                        for field in catlstfields:\n",
    "                            catfieldnames.append(field.name)\n",
    "                        if str(cat_cur_fields[0]) in catfieldnames:\n",
    "                            print (f'{cat_cur_fields[0]} field already in dataset {cats}')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print (f'Adding {cat_cur_fields[0]} field to catchment dataset {cats}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_txt field\n",
    "                            arcpy.AddField_management(cats, str(cat_cur_fields[0]), field_type='TEXT')\n",
    "                            # populate cat_ID_txt\n",
    "                            with arcpy.da.UpdateCursor(cats, cat_cur_fields[0:2]) as cur:\n",
    "                                for row in cur:\n",
    "                                    int_val = int(row[1])\n",
    "                                    str(int_val).replace(\".0\",\"\")\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del(row)\n",
    "                            del(cur)\n",
    "                        if str(cat_cur_fields[2]) in catfieldnames:\n",
    "                            print (f'{cat_cur_fields[2]} field already in dataset {cats}')\n",
    "                            print('----------')\n",
    "                        else:\n",
    "                            print (f'Adding {cat_cur_fields[2]} field to catchment dataset {cats}')\n",
    "                            print('----------')\n",
    "                            # add cat_ID_txt field & cat_ID + region concat field\n",
    "                            arcpy.AddField_management(cats,str(cat_cur_fields[2]),field_type='TEXT')\n",
    "                            # populate cat_ID_con\n",
    "                            with arcpy.da.UpdateCursor(cats, cat_cur_fields) as cur:\n",
    "                                for row in cur:\n",
    "                                    int_val = int(row[1])\n",
    "                                    row[2] = str(roi) +'_'+ str(int_val).replace(\".0\",\"\")\n",
    "                                    # Update rows\n",
    "                                    cur.updateRow(row)\n",
    "                                del(row)\n",
    "                            del(cur)\n",
    "\n",
    "            print (f'Calculating topographic metrics for catchments & watersheds of interest in {roi}'\n",
    "                   f' region')\n",
    "            print ('----------')\n",
    "            print(f'Geodatabase: {gdb}')\n",
    "            print ('----------')\n",
    "            print (f'Watershed Merge: {wtd_merge}')\n",
    "            print (f'  Projection {arcpy.Describe(wtd_merge).spatialReference.name}')\n",
    "            print ('----------')\n",
    "            print (f'Catchment Intersect: {cats}')\n",
    "            print (f'  Projection {arcpy.Describe(cats).spatialReference.name}')\n",
    "            print ('----------')\n",
    "            print (f'Elevation Raster: {elev_rast}')\n",
    "            print (f'  Projection: {arcpy.Describe(elev_rast).spatialReference.name}')\n",
    "            print ('----------')\n",
    "            print (f'North Aspect Raster: {nor_rast}')\n",
    "            print (f'  Projection: {arcpy.Describe(nor_rast).spatialReference.name}')\n",
    "            print ('----------')\n",
    "            print (f'Wetlands Raster: {wet_rast}')\n",
    "            print (f'  Projection {arcpy.Describe(wet_rast).spatialReference.name}')\n",
    "            print ('----------')\n",
    "            print (f'Slope Raster: {slope_rast}')\n",
    "            print (f'  Projection {arcpy.Describe(slope_rast).spatialReference.name}')\n",
    "            print ('----------')        \n",
    "            print (f'Lakes Ponds fc: {lakes_fc}')\n",
    "            print (f'  Projection {arcpy.Describe(lakes_fc).spatialReference.name}')\n",
    "            print ('----------')\n",
    "            print (f'Glaciers fc: {glac_fc} ')\n",
    "            print (f'  Projection {arcpy.Describe(glac_fc).spatialReference.name}')\n",
    "            print ('----------')\n",
    "            print (f'{len(wtds)} Watersheds to process')\n",
    "            print ('----------')\n",
    "            print (f'Catchment intersect {cats} selected')\n",
    "            print ('----------')\n",
    "\n",
    "            try:\n",
    "                # Set output names/paths for watershed and catchment tables\n",
    "                wtd_merge_name = roi + \"_Watersheds_Merge\"\n",
    "                wtd_merge_path = os.path.join(outgdb,wtd_merge_name)\n",
    "\n",
    "                # # Aspect variables\n",
    "                # wtd_merge_asp_table_name = roi + \"_Watersheds_Merge_AspectZstats\"\n",
    "                # wtd_merge_asp_table_path = os.path.join(outgdb, wtd_merge_asp_table_name)\n",
    "                # cat_asp_table_name = roi + \"_Catchments_AspectZstats\"\n",
    "                # cat_asp_table_path = os.path.join(outgdb, cat_asp_table_name)\n",
    "\n",
    "                # Percent North variables\n",
    "                wtd_merge_pernorth_table_name = roi + \"_Watersheds_Merge_PercentNorth\"\n",
    "                wtd_merge_pernorth_table_path = os.path.join(outgdb, wtd_merge_pernorth_table_name)\n",
    "                # cat_pernorth_table_name = roi + \"_Catchments_PercentNorth\"\n",
    "                # cat_pernorth_table_path = os.path.join(outgdb, cat_pernorth_table_name)\n",
    "\n",
    "                # Elevation variables\n",
    "                wtd_merge_elev_table_name = roi + \"_Watersheds_Merge_ElevZstats\"\n",
    "                wtd_merge_elev_table_path = os.path.join(outgdb, wtd_merge_elev_table_name)\n",
    "                cat_elev_table_name = roi + \"_Catchments_ElevZstats\"\n",
    "                cat_elev_table_path = os.path.join(outgdb, cat_elev_table_name)\n",
    "                \n",
    "                # Slope variables\n",
    "                wtd_merge_slope_table_name = roi + \"_Watershed_Merge_SlopeZstats\"\n",
    "                wtd_merge_slope_table_path = os.path.join(outgdb, wtd_merge_slope_table_name)\n",
    "                cat_slope_table_name = roi + \"_Catchments_SlopeZstats\"\n",
    "                cat_slope_table_path = os.path.join(outgdb, cat_slope_table_name)\n",
    "\n",
    "                # Lakes Ponds variables\n",
    "                wtd_merge_lp_table_name = roi + \"_Watershed_Merge_LakesPonds\"\n",
    "                wtd_merge_lp_table_path = os.path.join(outgdb, wtd_merge_lp_table_name)\n",
    "                cat_lp_table_name = roi + \"_Catchments_LakesPonds\"\n",
    "                cat_lp_path = os.path.join(outgdb, cat_lp_table_name)\n",
    "\n",
    "                # Wetlands variables\n",
    "                wtd_merge_wetlands_table_name = roi + \"_Watershed_Merge_Wetlands\"\n",
    "                wtd_merge_wetlands_table_path = os.path.join(outgdb, wtd_merge_wetlands_table_name)\n",
    "                cat_wetlands_table_name = roi + \"_Catchments_Wetlands\"\n",
    "                cat_wetlands_table_path = os.path.join(outgdb, cat_wetlands_table_name)\n",
    "\n",
    "                # Glaciers\n",
    "                wtd_merge_glac_table_name = roi + \"_Watershed_Merge_Glaciers\"\n",
    "                wtd_merge_glac_table_path = os.path.join(outgdb, wtd_merge_glac_table_name)\n",
    "                cat_glac_table_name = roi + \"_Catchments_Glaciers\"\n",
    "                cat_glac_table_path = os.path.join(outgdb, cat_glac_table_name)\n",
    "\n",
    "                # # Watersheds already merged so no need to run at this time\n",
    "                # if not arcpy.Exists(wtd_merge):\n",
    "                #     mergestart = time.time()\n",
    "                #     # Merge watersheds\n",
    "                #     wtd_merge = arcpy.Merge_management( wtds, wtd_merge_path ,add_source='ADD_SOURCE_INFO')\n",
    "                #     # Add wtd_id field\n",
    "                #     arcpy.AddField_management(wtd_merge,'cat_ID_txt',field_type='TEXT')\n",
    "                #     arcpy.AddField_management(wtd_merge,\"cat_ID_con\",field_type='TEXT')\n",
    "                #     # Add region field\n",
    "                #     arcpy.AddField_management(wtd_merge,'region',field_type='TEXT')\n",
    "                #     # Populate watershed id and region information using update cursor - faster than field calc\n",
    "                #     with arcpy.da.UpdateCursor(wtd_merge,['MERGE_SRC','cat_ID_txt','region','cat_ID_con']) as cur:\n",
    "                #         for row in cur:\n",
    "                #             row[1] = re.findall('\\d+', row[0])[0]\n",
    "                #             row[2] = roi\n",
    "                #             row[3] = + str(roi) + '_'+ str(re.findall('\\d+', row[0])[0])\n",
    "                #             # Update\n",
    "                #             cur.updateRow(row)\n",
    "                #         del(row)\n",
    "                #     del(cur)\n",
    "                #     mergestop = time.time()\n",
    "                #     mergetime = int (mergestop - mergestart)\n",
    "                #     print(f'Watershed Merge for {roi} Elapsed time: ({datetime.timedelta(seconds=mergetime)})')\n",
    "                #     print('----------')\n",
    "                # else:\n",
    "                #     print (f'Merged watershed dataset {wtd_merge} already created')\n",
    "                #     print('----------')\n",
    "\n",
    "                # Begin Zonal Stats\n",
    "                zstat_start = time.time()\n",
    "                print(f'Begin Elevation zonal statistics min/mean/max std dev for watersheds and catchments in {roi}'\n",
    "                      f' region')\n",
    "\n",
    "                # Elevation Zonal statistics  for watersheds\n",
    "                print(f'Calculating {roi} watershed elevation zonal stats...')\n",
    "                arcpy.env.snapRaster = elev_rast\n",
    "                arcpy.env.cellSize = elev_rast\n",
    "                wtd_elev_metrics_table = ZonalStatisticsAsTable(in_zone_data = wtd_merge,\n",
    "                                                                zone_field = wtd_cur_fields[0],\n",
    "                                                                in_value_raster = elev_rast,\n",
    "                                                                out_table = wtd_merge_elev_table_path,\n",
    "                                                                statistics_type='ALL'\n",
    "                                                                )\n",
    "                # Add region identifier field for watershed tables                                                )\n",
    "                arcpy.AddField_management(wtd_elev_metrics_table,'region',field_type='TEXT')\n",
    "                # Add cat_ID_Con field\n",
    "                arcpy.AddField_management(wtd_elev_metrics_table,'cat_ID_con',field_type='TEXT')\n",
    "                # Update region field\n",
    "                with arcpy.da.UpdateCursor(wtd_elev_metrics_table,['region','cat_ID_txt','cat_ID_con']) as cur:\n",
    "                    for row in cur:\n",
    "                        row[0] = roi\n",
    "                        row[2] = roi+\"_\"+str(row[1]).replace(\".0\",\"\")\n",
    "                        # Update\n",
    "                        cur.updateRow(row)\n",
    "                    del(row)\n",
    "                del(cur)\n",
    "                # Append watershed elev table to list\n",
    "                wtd_elev_ztables.append(wtd_elev_metrics_table)\n",
    "\n",
    "                # Elevation zonal statistics for catchments\n",
    "                print(f'Calculating {roi} catchment elevation zonal stats...')\n",
    "                arcpy.env.snapRaster = elev_rast\n",
    "                arcpy.env.cellSize = elev_rast\n",
    "                cat_elev_metrics_table = ZonalStatisticsAsTable(in_zone_data = cats ,\n",
    "                                                                zone_field = cat_cur_fields[0],\n",
    "                                                                in_value_raster = elev_rast,\n",
    "                                                                out_table = cat_elev_table_path,\n",
    "                                                                statistics_type='ALL'\n",
    "                                                                )\n",
    "                # Add region identifier field for catchment table\n",
    "                arcpy.AddField_management(cat_elev_metrics_table,'region',field_type='TEXT')\n",
    "                # Add cat_ID_Con field\n",
    "                arcpy.AddField_management(cat_elev_metrics_table,'cat_ID_con',field_type='TEXT')\n",
    "\n",
    "                # Update fields\n",
    "                with arcpy.da.UpdateCursor(cat_elev_metrics_table,['region','cat_ID_txt','cat_ID_con']) as cur:\n",
    "                    for row in cur:\n",
    "                        row[0] = roi\n",
    "                        row[2] = roi+\"_\"+str(row[1]).replace(\".0\",\"\")\n",
    "                        # Update\n",
    "                        cur.updateRow(row)\n",
    "                    del(row)\n",
    "                del(cur)\n",
    "                # Append catchment elev table to list\n",
    "                cat_elev_ztables.append(cat_elev_metrics_table)\n",
    "\n",
    "                # Slope zonal statistics for catchments\n",
    "                print(f'Calculating {roi} catchment slope zonal stats...')\n",
    "                arcpy.env.snapRaster = slope_rast\n",
    "                arcpy.env.cellSize = slope_rast\n",
    "                cat_slope_metrics_table = ZonalStatisticsAsTable(in_zone_data = cats ,\n",
    "                                                                zone_field = cat_cur_fields[0],\n",
    "                                                                in_value_raster = slope_rast,\n",
    "                                                                out_table = cat_slope_table_path,\n",
    "                                                                statistics_type='ALL'\n",
    "                                                                )\n",
    "                # Add region identifier field for catchment table\n",
    "                arcpy.AddField_management(cat_slope_metrics_table,'region',field_type='TEXT')\n",
    "                # Add cat_ID_Con field\n",
    "                arcpy.AddField_management(cat_slope_metrics_table,'cat_ID_con',field_type='TEXT')\n",
    "\n",
    "                # Update region field\n",
    "                with arcpy.da.UpdateCursor(cat_slope_metrics_table,['region','cat_ID_txt','cat_ID_con']) as cur:\n",
    "                    for row in cur:\n",
    "                        row[0] = roi\n",
    "                        row[2] = roi+\"_\"+str(row[1]).replace(\".0\",\"\")\n",
    "                        # Update\n",
    "                        cur.updateRow(row)\n",
    "                    del(row)\n",
    "                del(cur)\n",
    "                # Append catchment slope table to list\n",
    "\n",
    "                cat_slope_ztables.append(cat_slope_metrics_table)\n",
    "                # Watershed slope Zonal Statistics\n",
    "                print(f'Begin Slope zonal statistics min/mean/max std dev for watersheds and catchments in {roi}'\n",
    "                      f' region')\n",
    "\n",
    "                # Elevation Zonal statistics  for watersheds\n",
    "                print(f'Calculating {roi} watershed slope zonal stats...')\n",
    "                arcpy.env.snapRaster = slope_rast\n",
    "                arcpy.env.cellSize = slope_rast\n",
    "                wtd_slope_metrics_table = ZonalStatisticsAsTable(in_zone_data = wtd_merge,\n",
    "                                                                zone_field = wtd_cur_fields[0],\n",
    "                                                                in_value_raster = slope_rast,\n",
    "                                                                out_table = wtd_merge_slope_table_path,\n",
    "                                                                statistics_type='ALL'\n",
    "                                                                )\n",
    "                # Add region identifier field for watershed tables                                                )\n",
    "                arcpy.AddField_management(wtd_slope_metrics_table,'region',field_type='TEXT')\n",
    "                # Add cat_ID_Con field\n",
    "                arcpy.AddField_management(wtd_slope_metrics_table,'cat_ID_con',field_type='TEXT')\n",
    "\n",
    "                # Update region field\n",
    "                with arcpy.da.UpdateCursor(wtd_slope_metrics_table,['region','cat_ID_txt','cat_ID_con']) as cur:\n",
    "                    for row in cur:\n",
    "                        row[0] = roi\n",
    "                        row[2] = roi+\"_\"+str(row[1]).replace(\".0\",\"\")\n",
    "                        # Update\n",
    "                        cur.updateRow(row)\n",
    "                    del(row)\n",
    "                del(cur)\n",
    "                # Append watershed slope table to list\n",
    "                wtd_slope_ztables.append(wtd_slope_metrics_table)\n",
    "\n",
    "                # # Aspect Zonal statistics  for watersheds\n",
    "                # print(f'Calculating {roi} watershed aspect zonal stats...')\n",
    "                # wtd_asp_metrics_table = ZonalStatisticsAsTable(in_zone_data = wtd_merge, zone_field =\"cat_ID_txt\",\n",
    "                #                                                in_value_raster = asp_rast, out_table = wtd_merge_asp_table_path,\n",
    "                #                                                statistics_type='ALL')\n",
    "                # arcpy.AddField_management(wtd_asp_metrics_table, 'region', field_type='TEXT')\n",
    "                # Add cat_ID_Con field\n",
    "                # arcpy.AddField_management(wtd_asp_metrics_table,'cat_ID_con',field_type='TEXT')\n",
    "                # arcpy.CalculateField_management(wtd_asp_metrics_table, 'region', 'roi')\n",
    "                # Update region field\n",
    "                # with arcpy.da.UpdateCursor(wtd_asp_metrics_table,['region','cat_ID_txt','cat_ID_con']) as cur:\n",
    "                #     for row in cur:\n",
    "                #         row[0] = roi\n",
    "                #         row[2] = roi+\"_\"+str(row[1]).replace(\".0\",\"\")\n",
    "                #         # Update\n",
    "                #         cur.updateRow(row)\n",
    "                #     del(row)\n",
    "                # del(cur)\n",
    "                # wtd_asp_ztables.append(wtd_asp_metrics_table)\n",
    "\n",
    "                # # Aspect Zonal statistics for catchments\n",
    "                # print(f'Calculating {roi} catchment aspect zonal stats...')\n",
    "                # cat_asp_metrics_table = ZonalStatisticsAsTable(in_zone_data = cats, zone_field =\"cat_ID_txt\",\n",
    "                #                                                in_value_raster = asp_rast, out_table = cat_asp_table_path,\n",
    "                #                                                statistics_type='ALL')\n",
    "                # arcpy.AddField_management(cat_asp_metrics_table, 'region', field_type='TEXT')\n",
    "                # Add cat_ID_Con field\n",
    "                # arcpy.AddField_management(cat_asp_metrics_table,'cat_ID_con',field_type='TEXT')\n",
    "                # arcpy.CalculateField_management(cat_asp_metrics_table, 'region', 'roi')\n",
    "                # Update region field\n",
    "                # with arcpy.da.UpdateCursor(cat_asp_metrics_table,['region','cat_ID_txt','cat_ID_con']) as cur:\n",
    "                #     for row in cur:\n",
    "                #         row[0] = roi\n",
    "                #         row[2] = roi+\"_\"+str(row[1]).replace(\".0\",\"\")\n",
    "                #         # Update\n",
    "                #         cur.updateRow(row)\n",
    "                #     del(row)\n",
    "                # del(cur)\n",
    "                # cat_asp_ztables.append(cat_asp_metrics_table)\n",
    "\n",
    "                zstat_stop = time.time()\n",
    "                zstat_time = int (zstat_stop - zstat_start)\n",
    "                print(f'Zonal Stats for {roi} Elapsed time: ({datetime.timedelta(seconds=zstat_time)})')\n",
    "                print('----------')\n",
    "\n",
    "                # Tabulate Area with north grid and watersheds\n",
    "                tabarea_start = time.time()\n",
    "                print(f'Begin tabulate area of north facing cells for watersheds and catchments in {roi} region')\n",
    "                print('----------')\n",
    "                # Percent North Tabulate area for watersheds\n",
    "                wtd_per_north_tabarea = arcpy.sa.TabulateArea(in_zone_data= wtd_merge,\n",
    "                                                              zone_field= wtd_cur_fields[0],\n",
    "                                                              in_class_data=nor_rast,\n",
    "                                                              class_field=\"Value\",\n",
    "                                                              out_table=wtd_merge_pernorth_table_path\n",
    "                                                              )\n",
    "                # Add region and percent north fields\n",
    "                arcpy.AlterField_management(wtd_per_north_tabarea,'CAT_ID_TXT','CAT_ID_TXT_DEL','CAT_ID_TXT_DEL')\n",
    "                arcpy.AddField_management(wtd_per_north_tabarea, 'region', field_type='TEXT')\n",
    "                arcpy.AddField_management(wtd_per_north_tabarea, 'wtd_north_per', field_type='Float')\n",
    "                arcpy.AddField_management(wtd_per_north_tabarea, wtd_cur_fields[0], field_type='TEXT')\n",
    "                arcpy.AddField_management(wtd_per_north_tabarea, wtd_cur_fields[2], field_type='TEXT')\n",
    "                wtdnorfields = [f.name for f in arcpy.ListFields(wtd_per_north_tabarea)]\n",
    "                print (wtdnorfields)\n",
    "                with arcpy.da.UpdateCursor(wtd_per_north_tabarea, wtdnorfields) as cur:\n",
    "                    for row in cur:\n",
    "                        row[4] = roi\n",
    "                        row[5] = row[3]/(row[3]+row[2])*100\n",
    "                        row[6] = row[1]\n",
    "                        row[7] = roi +'_'+ str(row[1]).replace(\".0\",\"\")\n",
    "                        # Update\n",
    "                        cur.updateRow(row)\n",
    "                    del(row)\n",
    "                del(cur)\n",
    "                # Drop UPPERCASE field form tab area\n",
    "                arcpy.DeleteField_management(wtd_per_north_tabarea,'CAT_ID_TXT_DEL')\n",
    "                # Append watershed percent north table to list\n",
    "                wtd_pernorth_taba_tables.append(wtd_per_north_tabarea)\n",
    "\n",
    "                # Percent Lakes Ponds using Tabulate Intersection for watersheds\n",
    "                print(f'Begin tabulate intersection between {lakes_fc} and watersheds in {roi} region')\n",
    "                print('----------')\n",
    "                wtd_lp_tabint = arcpy.TabulateIntersection_analysis(wtd_merge,\n",
    "                                                                    zone_fields=wtd_cur_fields[0],\n",
    "                                                                    in_class_features=lakes_fc,\n",
    "                                                                    out_table=wtd_merge_lp_table_path,\n",
    "                                                                    class_fields='Ftype',\n",
    "                                                                    out_units=\"SQUARE_METERS\"\n",
    "                                                                    )\n",
    "                # Add region and cat id fields\n",
    "                arcpy.AlterField_management(wtd_lp_tabint,'PERCENTAGE','wtd_lake_per','wtd_lake_per')\n",
    "                arcpy.AlterField_management(wtd_lp_tabint,'AREA','wtd_lake_area_sqm','wtd_lake_area_sqm')\n",
    "                arcpy.AddField_management(wtd_lp_tabint, 'region', field_type='TEXT')\n",
    "                arcpy.AddField_management(wtd_lp_tabint, wtd_cur_fields[1], field_type='TEXT')\n",
    "                arcpy.AddField_management(wtd_lp_tabint, wtd_cur_fields[2], field_type='TEXT')\n",
    "                wtdlpfields = [f.name for f in arcpy.ListFields(wtd_lp_tabint)]\n",
    "                print (wtdlpfields)\n",
    "                with arcpy.da.UpdateCursor(wtd_lp_tabint, wtdlpfields) as cur:\n",
    "                    for row in cur:\n",
    "                        row[5] = roi\n",
    "                        row[6] = row[1]\n",
    "                        row[7] = roi +'_'+ str(row[1]).replace(\".0\",\"\")\n",
    "                        # Update\n",
    "                        cur.updateRow(row)\n",
    "                    del(row)\n",
    "                del(cur)\n",
    "\n",
    "                # Append watershed lakes ponds table to list\n",
    "                wtd_lp_tabint_tables.append(wtd_lp_tabint)\n",
    "\n",
    "                # Percent glaciers using Tabulate Intersection for watersheds\n",
    "                print(f'Begin tabulate intersection between {glac_fc} and watersheds in {roi} region')\n",
    "                print('----------')\n",
    "                wtd_glac_tabint = arcpy.TabulateIntersection_analysis(wtd_merge,\n",
    "                                                                    zone_fields=wtd_cur_fields[0],\n",
    "                                                                    in_class_features=glac_fc,\n",
    "                                                                    out_table=wtd_merge_glac_table_path,\n",
    "                                                                    class_fields='O1Region',\n",
    "                                                                    out_units=\"SQUARE_METERS\"\n",
    "                                                                    )\n",
    "                # Add region and cat id fields\n",
    "                arcpy.AlterField_management(wtd_glac_tabint,'PERCENTAGE','wtd_glacier_per','wtd_glacier_per')\n",
    "                arcpy.AlterField_management(wtd_glac_tabint,'AREA','wtd_glacier_area_sqm','wtd_glacier_area_sqm')\n",
    "                arcpy.AddField_management(wtd_glac_tabint, 'region', field_type='TEXT')\n",
    "                arcpy.AddField_management(wtd_glac_tabint, wtd_cur_fields[1], field_type='TEXT')\n",
    "                arcpy.AddField_management(wtd_glac_tabint, wtd_cur_fields[2], field_type='TEXT')\n",
    "                wtdglacfields = [f.name for f in arcpy.ListFields(wtd_glac_tabint)]\n",
    "                print (wtdglacfields)\n",
    "                with arcpy.da.UpdateCursor(wtd_glac_tabint, wtdglacfields) as cur:\n",
    "                    for row in cur:\n",
    "                        row[5] = roi\n",
    "                        row[6] = row[1]\n",
    "                        row[7] = roi +'_'+ str(row[1]).replace(\".0\",\"\")\n",
    "                        # Update\n",
    "                        cur.updateRow(row)\n",
    "                    del(row)\n",
    "                del(cur)\n",
    "                # Append watershed percent north table to list\n",
    "                wtd_glac_tabint_tables.append(wtd_glac_tabint)\n",
    "\n",
    "                # Tabulate Area with wetlands grid and watersheds\n",
    "                print(f'Begin tabulate intersection between {wet_rast} and watersheds in {roi} region')\n",
    "                print('----------')\n",
    "                # Percent North Tabulate area for watersheds\n",
    "                wtd_per_wet_tabarea = arcpy.sa.TabulateArea(in_zone_data= wtd_merge,\n",
    "                                                              zone_field= wtd_cur_fields[0],\n",
    "                                                              in_class_data=wet_rast,\n",
    "                                                              class_field=\"Value\",\n",
    "                                                              out_table=wtd_merge_wetlands_table_path\n",
    "                                                              )\n",
    "                # Add region and percent wet fields\n",
    "                arcpy.AlterField_management(wtd_per_wet_tabarea,'CAT_ID_TXT','CAT_ID_TXT_DEL','CAT_ID_TXT_DEL')\n",
    "                arcpy.AddField_management(wtd_per_wet_tabarea, 'region', field_type='TEXT')\n",
    "                arcpy.AddField_management(wtd_per_wet_tabarea, 'wtd_wet_per', field_type='Float')\n",
    "                arcpy.AddField_management(wtd_per_wet_tabarea, wtd_cur_fields[0], field_type='TEXT')\n",
    "                arcpy.AddField_management(wtd_per_wet_tabarea, wtd_cur_fields[2], field_type='TEXT')\n",
    "                wtdwetfields = [f.name for f in arcpy.ListFields(wtd_per_wet_tabarea)]\n",
    "                print (wtdwetfields)\n",
    "                with arcpy.da.UpdateCursor(wtd_per_wet_tabarea, wtdwetfields) as cur:\n",
    "                    for row in cur:\n",
    "                        row[4] = roi\n",
    "                        row[5] = row[3]/(row[3]+row[2])*100\n",
    "                        row[6] = row[1]\n",
    "                        row[7] = roi +'_'+ str(row[1]).replace(\".0\",\"\")\n",
    "                        # Update\n",
    "                        cur.updateRow(row)\n",
    "                    del(row)\n",
    "                del(cur)\n",
    "                # Drop UPPERCASE field form tab area\n",
    "                arcpy.DeleteField_management(wtd_per_wet_tabarea,'CAT_ID_TXT_DEL')\n",
    "                # Append watershed percent north table to list\n",
    "                wtd_wet_taba_tables.append(wtd_per_wet_tabarea)\n",
    "\n",
    "                # # Percent North Tabulate Area for catchments\n",
    "                # cat_per_north_tabarea = arcpy.sa.TabulateArea(in_zone_data= cats, zone_field='cat_ID_con',\n",
    "                #                                             in_class_data=nor_rast,\"Value\",\n",
    "                #                                             out_table=cat_pernorth_table_path)\n",
    "\n",
    "                # # Add and calculate region identifier field for catchment table\n",
    "                # arcpy.AlterField_management(cat_per_north_tabarea,'CAT_ID_TXT','CAT_ID_TXT_DEL','CAT_ID_TXT_DEL')\n",
    "                # arcpy.AddField_management(cat_per_north_tabarea, 'region', field_type='TEXT')\n",
    "                # arcpy.AddField_management(cat_per_north_tabarea, 'cat_north_per', field_type='Float')\n",
    "                # arcpy.AddField_management(cat_per_north_tabarea, cat_cur_fields[0], field_type='TEXT')\n",
    "                # arcpy.AddField_management(cat_per_north_tabarea, cat_cur_fields[2], field_type='TEXT')\n",
    "                # catnorfields = [f.name for f in arcpy.ListFields(cat_per_north_tabarea)]\n",
    "                # print (catnorfields)\n",
    "                # with arcpy.da.UpdateCursor(cat_per_north_tabarea,catnorfields) as cur:\n",
    "                #     for row in cur:\n",
    "                #         row[4] = roi\n",
    "                #         row[5] = row[3]/(row[3]+row[2])*100\n",
    "                #         row[6] = row[1]\n",
    "                #         row[7] = roi +'_'+ str(row[1]).replace(\".0\",\"\")\n",
    "                #         # Update\n",
    "                #         cur.updateRow(row)\n",
    "                #     del(row)\n",
    "                # del(cur)\n",
    "                # Drop UPPERCASE field form tab area\n",
    "                # arcpy.DeleteField_management(cat_per_north_tabarea,'CAT_ID_TXT_DEL')\n",
    "                # # Append catchment percent north table to list\n",
    "                # cat_pernorth_taba_tables.append(cat_per_north_tabarea)\n",
    "\n",
    "                tabarea_stop = time.time()\n",
    "                tabarea_time = int (tabarea_stop - tabarea_start)\n",
    "                print(f'Tabulate area/intersections for {roi} Elapsed time: ({datetime.timedelta(seconds=tabarea_time)})')\n",
    "                print('----------')\n",
    "\n",
    "            except:\n",
    "                e = sys.exc_info()[1]\n",
    "                print(e.args[0])\n",
    "                arcpy.AddError(e.args[0])\n",
    "\n",
    "            iter_stop = time.time()\n",
    "            iter_time = int(iter_stop - iteration_start)\n",
    "            print(f'All Covariates for {roi} completed. Elapsed time: ({datetime.timedelta(seconds=iter_time)})')\n",
    "            print('----------')\n",
    "\n",
    "    else:\n",
    "        print(f'Region {str(roi)} not found in {region}')\n",
    "\n",
    "# End timing\n",
    "processEnd = time.time()\n",
    "processElapsed = int(processEnd - processStart)\n",
    "processSuccess_time = datetime.datetime.now()\n",
    "\n",
    "# Report success\n",
    "print(f'Process completed at {processSuccess_time.strftime(\"%Y-%m-%d %H:%M\")} '\n",
    "      f'(Elapsed time: {datetime.timedelta(seconds=processElapsed)})')\n",
    "print('----------')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Drop unnecessary fields and rename as needed from merged tables.\n",
    "- Create Key value dictionary and use update cursor to rename fields."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lists to store output tables\n",
    "# wtd_pernorth_taba_tables=[]\n",
    "# wtd_lp_tabint_tables = []\n",
    "# wtd_glac_tabint_tables = []\n",
    "# wtd_wet_taba_tables = []\n",
    "# cat_elev_ztables = []\n",
    "# wtd_elev_ztables = []\n",
    "# cat_slope_ztables = []\n",
    "# wtd_slope_ztables = []\n",
    "\n",
    "# Table names/paths\n",
    "wtd_per_north_table_out = os.path.join(outgdb, 'AKSSF_wtd_north_per')\n",
    "cat_elev_table_out = os.path.join(outgdb,'AKSSF_cat_elev')\n",
    "cat_slope_table_out = os.path.join(outgdb,'AKSSF_cat_slope')\n",
    "wtd_elev_table_out = os.path.join(outgdb, 'AKSSF_wtd_elev')\n",
    "wtd_per_glac_table_out = os.path.join(outgdb, 'AKSSF_wtd_glacier_per')\n",
    "wtd_per_lp_table_out = os.path.join(outgdb, 'AKSSF_wtd_lakepond_per')\n",
    "wtd_slope_table_out = os.path.join(outgdb, 'AKSSF_wtd_slope')\n",
    "wtd_wet_table_out = os.path.join(outgdb, 'AKSSF_wtd_wetland_per')\n",
    "\n",
    "# Merge all regional tables together\n",
    "outtables = []\n",
    "wtd_per_north = arcpy.Merge_management(wtd_pernorth_taba_tables, wtd_per_north_table_out)\n",
    "arcpy.AlterField_management(wtd_per_north,\"VALUE_0\",\"non_north_area\",\"non_north_area\")\n",
    "arcpy.AlterField_management(wtd_per_north,\"VALUE_1\",\"north_area\",\"north_area\")\n",
    "outtables.append(wtd_per_north)\n",
    "cat_elev = arcpy.Merge_management(cat_elev_ztables, cat_elev_table_out)\n",
    "outtables.append(cat_elev)\n",
    "wtd_elev = arcpy.Merge_management(wtd_elev_ztables, wtd_elev_table_out)\n",
    "outtables.append(wtd_elev)\n",
    "wtd_slope = arcpy.Merge_management(wtd_slope_ztables, wtd_slope_table_out)\n",
    "outtables.append(wtd_slope)\n",
    "cat_slope = arcpy.Merge_management(cat_slope_ztables, cat_slope_table_out)\n",
    "outtables.append(cat_slope)\n",
    "wtd_wet = arcpy.Merge_management(wtd_wet_taba_tables, wtd_wet_table_out)\n",
    "arcpy.AlterField_management(wtd_wet,\"VALUE_0\",\"non_wetland_area\",\"non_wetland_area\")\n",
    "arcpy.AlterField_management(wtd_wet,\"VALUE_1\",\"wetland_area\",\"wetland_area\")\n",
    "outtables.append(wtd_wet)\n",
    "wtd_glac = arcpy.Merge_management(wtd_glac_tabint_tables, wtd_per_glac_table_out)\n",
    "outtables.append(wtd_glac)\n",
    "wtd_lp = arcpy.Merge_management(wtd_lp_tabint_tables, wtd_per_lp_table_out)\n",
    "outtables.append(wtd_lp)\n",
    "print ('Tables merged')\n",
    "print('----------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Set up field dictionary\n",
    "elevDict = { 'ZONE_CODE': ('cat_elev_ZONE_CODE', 'wtd_elev_ZONE_CODE'),\n",
    "         'COUNT': ('cat_elev_COUNT', 'wtd_elev_COUNT'),\n",
    "          'AREA': ('cat_elev_AREA', 'wtd_elev_AREA'),\n",
    "          'MIN': ('cat_elev_MIN', 'wtd_elev_MIN'),\n",
    "          'MAX': ('cat_elev_MAX', 'wtd_elev_MAX'),\n",
    "          'RANGE': ('cat_elev_RANGE', 'wtd_elev_RANGE'),\n",
    "          'MEAN': ('cat_elev_MEAN', 'wtd_elev_MEAN'),\n",
    "          'STD': ('cat_elev_STD', 'wtd_elev_STD'),\n",
    "          'SUM': ('cat_elev_SUM', 'wtd_elev_SUM'),\n",
    "          'VARIETY': ('cat_elev_VARIETY', 'wtd_elev_VARIETY'),\n",
    "          'MAJORITY': ('cat_elev_MAJORITY', 'wtd_elev_MAJORITY'),\n",
    "          'MINORITY': ('cat_elev_MINORITY', 'wtd_elev_MINORITY'),\n",
    "          'MEDIAN': ('cat_elev_MEDIAN', 'wtd_elev_MEDIAN'),\n",
    "          'PCT90': ('cat_elev_PCT90', 'wtd_elev_PCT90')\n",
    "         }\n",
    "\n",
    "slopeDict = { 'ZONE_CODE': ('cat_slope_ZONE_CODE', 'wtd_slope_ZONE_CODE'),\n",
    "         'COUNT': ('cat_slope_COUNT', 'wtd_slope_COUNT'),\n",
    "          'AREA': ('cat_slope_AREA', 'wtd_slope_AREA'),\n",
    "          'MIN': ('cat_slope_MIN', 'wtd_slope_MIN'),\n",
    "          'MAX': ('cat_slope_MAX', 'wtd_slope_MAX'),\n",
    "          'RANGE': ('cat_slope_RANGE', 'wtd_slope_RANGE'),\n",
    "          'MEAN': ('cat_slope_MEAN', 'wtd_slope_MEAN'),\n",
    "          'STD': ('cat_slope_STD', 'wtd_slope_STD'),\n",
    "          'SUM': ('cat_slope_SUM', 'wtd_slope_SUM'),\n",
    "          'VARIETY': ('cat_slope_VARIETY', 'wtd_slope_VARIETY'),\n",
    "          'MAJORITY': ('cat_slope_MAJORITY', 'wtd_slope_MAJORITY'),\n",
    "          'MINORITY': ('cat_slope_MINORITY', 'wtd_slope_MINORITY'),\n",
    "          'MEDIAN': ('cat_slope_MEDIAN', 'wtd_slope_MEDIAN'),\n",
    "          'PCT90': ('cat_slope_PCT90', 'wtd_slope_PCT90')\n",
    "         }\n",
    "\n",
    "# Rename fields for elevation tables\n",
    "for field in arcpy.ListFields(wtd_elev):\n",
    "    keyval = field.name\n",
    "    if keyval in elevDict:\n",
    "        newname = elevDict[keyval][1]\n",
    "        newalias = elevDict[keyval][1]\n",
    "        print (keyval, newname)\n",
    "        arcpy.AlterField_management(wtd_elev, keyval, newname, newalias)\n",
    "\n",
    "for field in arcpy.ListFields(cat_elev):\n",
    "    keyval = field.name\n",
    "    if keyval in elevDict:\n",
    "        newname = elevDict[keyval][0]\n",
    "        newalias = elevDict[keyval][0]\n",
    "        print (keyval, newname)\n",
    "        arcpy.AlterField_management(cat_elev, keyval, newname, newalias)\n",
    "\n",
    "# Rename fields for slope tables\n",
    "for field in arcpy.ListFields(wtd_slope):\n",
    "    keyval = field.name\n",
    "    if keyval in slopeDict:\n",
    "        newname = slopeDict[keyval][1]\n",
    "        newalias = slopeDict[keyval][1]\n",
    "        print (keyval, newname)\n",
    "        arcpy.AlterField_management(wtd_slope, keyval, newname, newalias)\n",
    "\n",
    "for field in arcpy.ListFields(cat_slope):\n",
    "    keyval = field.name\n",
    "    if keyval in slopeDict:\n",
    "        newname = slopeDict[keyval][0]\n",
    "        newalias = slopeDict[keyval][0]\n",
    "        print (keyval, newname)\n",
    "        arcpy.AlterField_management(cat_slope, keyval, newname, newalias)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for table in outtables:\n",
    "    print (arcpy.Describe(table).basename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export copies of dbf tables as csv\n",
    "outdir = r\"C:\\Users\\dwmerrigan\\Documents\\GitHub\\AKSSF\\data_preparation\\sensitivity_drivers\"\n",
    "for table in outtables:\n",
    "    tablename = arcpy.Describe(table).basename + \".csv\"\n",
    "    tablepath = os.path.join(outdir,tablename)\n",
    "    print( tablepath)\n",
    "    arcpy.conversion.TableToTable(table, outdir, tablename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merge all\n",
    "alltables = os.path.join(outgdb, \"AKSSF_Covariates\")\n",
    "alltablesname = 'AKSSF_Covariates.csv'\n",
    "akssf_cov_tables_merge = arcpy.Merge_management(outtables, alltables)\n",
    "arcpy.conversion.TableToTable(akssf_cov_tables_merge, outdir, alltablesname)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert tables to pandas df and explore results\n",
    "* **Merge tables and export to csv**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.2f}'.format # only display 2 decimal places\n",
    "# list to store covariate data frames\n",
    "dfs = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make catchment elev df\n",
    "cat_df = pd.DataFrame()\n",
    "cat_field_list = []\n",
    "for field in arcpy.ListFields(cat_elev):\n",
    "    cat_field_list.append(field.name)\n",
    "cat_elev_arr = arcpy.da.TableToNumPyArray(cat_elev,cat_field_list)\n",
    "cat_df = pd.DataFrame(cat_elev_arr)\n",
    "cat_df = cat_df.drop([\"OBJECTID\",\"cat_elev_ZONE_CODE\"],axis=1)\n",
    "cat_df = cat_df.set_index('cat_ID_txt')\n",
    "dfs.append(cat_df)\n",
    "cat_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make catchment slope df\n",
    "cat_sl_df = pd.DataFrame()\n",
    "cat_sl_field_list = []\n",
    "for field in arcpy.ListFields(cat_slope):\n",
    "    cat_sl_field_list.append(field.name)\n",
    "    print (field.name, field.aliasName)\n",
    "cat_sl_arr = arcpy.da.TableToNumPyArray(cat_slope, cat_sl_field_list)\n",
    "cat_sl_df = pd.DataFrame(cat_sl_arr)\n",
    "cat_sl_df = cat_sl_df.drop([\"OBJECTID\", \"cat_slope_ZONE_CODE\"],axis=1)\n",
    "cat_sl_df = cat_sl_df.set_index('cat_ID_txt')\n",
    "dfs.append(cat_sl_df)\n",
    "cat_sl_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed elev df\n",
    "wtd_df = pd.DataFrame()\n",
    "wtd_field_list = []\n",
    "for field in arcpy.ListFields(wtd_elev):\n",
    "    wtd_field_list.append(field.name)\n",
    "\n",
    "wtd_elev_arr = arcpy.da.TableToNumPyArray(wtd_elev,wtd_field_list)\n",
    "wtd_df = pd.DataFrame(wtd_elev_arr)\n",
    "wtd_df = wtd_df.drop([\"OBJECTID\",\"wtd_elev_ZONE_CODE\"],axis=1)\n",
    "wtd_df = wtd_df.set_index('cat_ID_txt')\n",
    "dfs.append(wtd_df)\n",
    "wtd_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed slope df\n",
    "wtd_sl_df = pd.DataFrame()\n",
    "wtd_sl_field_list = []\n",
    "for field in arcpy.ListFields(wtd_slope):\n",
    "    wtd_sl_field_list.append(field.name)\n",
    "wtd_sl_arr = arcpy.da.TableToNumPyArray(wtd_slope, wtd_sl_field_list)\n",
    "wtd_sl_df = pd.DataFrame(wtd_sl_arr)\n",
    "wtd_sl_df = wtd_sl_df.drop([\"OBJECTID\", \"wtd_slope_ZONE_CODE\"],axis=1)\n",
    "wtd_sl_df = wtd_sl_df.set_index('cat_ID_txt')\n",
    "dfs.append(wtd_sl_df)\n",
    "wtd_sl_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed north df\n",
    "wtd_n_df = pd.DataFrame()\n",
    "wtd_n_field_list = []\n",
    "for field in arcpy.ListFields(wtd_per_north):\n",
    "    wtd_n_field_list.append(field.name)\n",
    "wtd_n_arr = arcpy.da.TableToNumPyArray(wtd_per_north,wtd_n_field_list)\n",
    "wtd_n_df = pd.DataFrame(wtd_n_arr)\n",
    "wtd_n_df = wtd_n_df.drop(\"OBJECTID\",axis=1)\n",
    "wtd_n_df = wtd_n_df.set_index('cat_ID_txt')\n",
    "dfs.append(wtd_n_df)\n",
    "wtd_n_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed wetland df\n",
    "wtd_wet_df = pd.DataFrame()\n",
    "wtd_wet_field_list = []\n",
    "for field in arcpy.ListFields(wtd_wet):\n",
    "    wtd_wet_field_list.append(field.name)\n",
    "wtd_wet_arr = arcpy.da.TableToNumPyArray(wtd_wet,wtd_wet_field_list)\n",
    "wtd_wet_df = pd.DataFrame(wtd_wet_arr)\n",
    "wtd_wet_df = wtd_wet_df.drop(\"OBJECTID\",axis=1)\n",
    "wtd_wet_df = wtd_wet_df.set_index('cat_ID_txt')\n",
    "dfs.append(wtd_wet_df)\n",
    "wtd_wet_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed lakes df\n",
    "wtd_lp_df = pd.DataFrame()\n",
    "wtd_lp_field_list = []\n",
    "for field in arcpy.ListFields(wtd_lp):\n",
    "    wtd_lp_field_list.append(field.name)\n",
    "wtd_lp_arr = arcpy.da.TableToNumPyArray(wtd_lp, wtd_lp_field_list)\n",
    "wtd_lp_df = pd.DataFrame(wtd_lp_arr)\n",
    "wtd_lp_df = wtd_lp_df.drop(\"OBJECTID\",axis=1)\n",
    "wtd_lp_df = wtd_lp_df.set_index('cat_ID_txt')\n",
    "dfs.append(wtd_lp_df)\n",
    "wtd_lp_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make watershed glacier df\n",
    "wtd_glac_df = pd.DataFrame()\n",
    "wtd_glac_field_list = []\n",
    "for field in arcpy.ListFields(wtd_glac):\n",
    "    wtd_glac_field_list.append(field.name)\n",
    "wtd_glac_arr = arcpy.da.TableToNumPyArray(wtd_glac, wtd_glac_field_list)\n",
    "wtd_glac_df = pd.DataFrame(wtd_glac_arr)\n",
    "wtd_glac_df = wtd_glac_df.drop(\"OBJECTID\",axis=1)\n",
    "wtd_glac_df = wtd_glac_df.set_index('cat_ID_txt')\n",
    "dfs.append(wtd_glac_df)\n",
    "wtd_glac_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merge all data frames together\n",
    "from functools import reduce\n",
    "df_final = reduce(lambda left,right: pd.merge(left,right,on='cat_ID_txt',how=\"outer\"), dfs)\n",
    "#df_final = pd.concat(dfs)\n",
    "df_final\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8caab41e",
   "language": "python",
   "display_name": "PyCharm (AKSSF)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}