---
title: "sites"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath(".."))

library(tidyverse)
library(googlesheets4)
library(readxl)
library(janitor)

```

Read in metadata for all the sites from the different providers/projects. Combine into one data frame:

* data_SiteID - this is the correct site id to link to the data table
* SiteID - this is the site id provided by the agency, but not always unique or linked directly to the data as provided
* AKOATS_ID
* waterbody_name - we will need this to confirm correct catchment location
* latitude
* longitude
* Source_Name
* Contact_Name

This file will be brought into GIS to link to catchments (e.g. add a catchment_ID) so that we can extract daymet air temperatures and correctly link them to sites.

# AKOATS

```{r akoats working continuous sites}

#get lat/longs for sites in akoats
akoats <- read_excel("S:/EPA AKTEMP/AKOATS_DATA_2020_working.xlsx", sheet = "CONTINUOUS_DATA")

```

# USFS

Forest service sites.

```{r usfs metadata}

depracated_metadata <- "https://docs.google.com/spreadsheets/d/1ymyhRyAomnJZTZqr4IbRmCnJjVaOIpCQYGscJDbCk-w/edit#gid=0"

usfs_md <- read_sheet(depracated_metadata, sheet = "Sheet1",
                    col_names = TRUE, na = "NA", col_types = "c-cn-cccccnnccccc") %>% 
  filter(ACCS_Project == "USFS_Chugach") %>% 
  rename(lat_new = Latitude, long_new = Longitude) %>% 
  select(-ACCS_Project)

usfs_md <- left_join(usfs_md, akoats %>% select(seq_id, Latitude, Longitude), by = c("AKOATS_ID" = "seq_id")) %>% 
  mutate(Latitude = case_when(is.na(Latitude) ~ lat_new,
                              TRUE ~ Latitude),
         Longitude = case_when(is.na(Longitude) ~ long_new,
                               TRUE ~ Longitude)) %>% 
  select(-lat_new, -long_new)

usfs_akssf_md <- usfs_md %>% 
  select(AKOATS_ID, SiteID = Agency_ID, SourceName, Contact_person, Latitude, Longitude, Waterbody_name)
```

# Stream temperature models - Deshka, Anchor, Kenai

Metadata were created for the AKTEMP database, read in from google sheet online.

Problems with bad naming conventions on some of this data. The sites were renamed by Leslie for this project and those names are what are in the data. For this project, use the data_siteID so that we can link to the data table later. For the raw metadata for akoats, can use the agency id when available or the data-site id when it is not (just the three kwf loggers).

```{r}

ci_tempmodel_md <- read_sheet(depracated_metadata, sheet = "Sheet1",
                              col_names = TRUE, na = "NA", col_types = "c-cn-cccccnnccccc") %>% 
  filter(ACCS_Project %in% c("Kenai Temperature Model", "Anchor Temperature Model", "Deshka Temperature Model")) %>% 
  mutate(SiteID = case_when(is.na(Agency_ID) ~ data_SiteID,
                            TRUE ~ Agency_ID)) 
  

ci_tempmodel_akssf_md <- ci_tempmodel_md %>% 
  select(AKOATS_ID, SiteID = data_SiteID, SourceName, Contact_person, Latitude, Longitude, Waterbody_name)
```

# KNB datasets - Sue Mauger, Trey Simmons

Cook Inletkeeper data archived on KNB. Note that some of this data are from Bristol Bay... combine everything in GIS and see if I need to filter out some to just cover the SC geography.

```{r}
cik_md <- read_csv("S:\\Stream Temperature Data\\Cook Inletkeeper\\SiteLevelMetadata_Mauger.csv") 

cik_akssf_md <- cik_md %>% 
  filter(grepl("CIK", SiteID)) %>% 
  select(AKOATS_ID, SiteID, SourceName, Contact_person, Latitude, Longitude, Waterbody_name)

cik_akssf_md
```

NPS data from interior - Trey Simmons archive on KNB. Note that some of these sites might also be outside the geography we want. Plot in GIS and see if need to exclude some.


```{r}
nps_md <- read_csv("S:\\Stream Temperature Data\\NPS Simmons\\data\\SiteLevelMetadata_Simmons.csv") 

nps_akssf_md <- nps_md %>% 
  filter(SiteID %in% c("Rufus Creek", "Caribou Creek", "Rock Creek WRST", 
                       "Gilahina River", "Crystal Creek", "Lakina River", "Long Lake Creek")) %>% 
  select(AKOATS_ID, SiteID, SourceName, Contact_person, Latitude, Longitude, Waterbody_name)

```


# USFWS datasets for SC: Kodiak refuge and some OSM and WRB sites

Read in metadata for Kodiak, OSM, and WRB. Note that they include a lot of notes so open each in Excel to ensure I am grabbing the correct rows with actual data.

Kodiak, note that Meg fixed some site names that should be cleared up here. 

```{r}

kodiak_md <- read_excel("S:\\Stream Temperature Data\\USFWS Perdue - complete 2020\\Kodiak_Oct2020.xlsx", sheet = "AKOATS_metadata") %>% 
  filter(Contact_person == "Bill Pyle") %>% 
  mutate(SiteID = case_when(grepl("akacr01", SiteID) ~ "kdk_akacr01",
                            grepl("ayarv03", SiteID) ~ "kdk_ayarv03",
                            grepl("concr01", SiteID) ~ "kdk_concr01",
                            TRUE ~ SiteID))

kodiak_akssf_md <- kodiak_md %>% 
  select(AKOATS_ID, SiteID, SourceName, Contact_person, Latitude, Longitude, Waterbody_name)


```


Office of Subsistence Management. Unfortunately, these are all waterbody names for site ids. We could add an osm prefix. Reviewing AKOATS -  Tanada Creek and Long Lake Creek are in the copper river basin, Afognak and Buskin are on kodiak, Newhalen River in Bristol Bay. This metadata sheet doesn't have the akoats ids so will need to add them in. 

```{r}
osm_md <- read_excel("S:\\Stream Temperature Data\\USFWS Perdue - complete 2020\\OSM_Oct2020.xlsx", sheet = "OSM_Sites_AKOATS") %>% 
  filter(Contact_person == "Don Rivard")

#link to akoats ids, three duplicates, keep first record. Second record for Gisasa is particularly suspicious bc lat/long very different and note with comment from krista bartz/nps. Also, different agency_ids for afognak and buskin rivers, add in akoats ids manually.
# new longitude for Tanada creek is totally wrong
osm_md <- left_join(osm_md, akoats %>% filter(SourceName == "fwsAlaskaOSM") %>% select(Agency_ID, seq_id)) %>% 
  filter(!seq_id %in% c(2096, 2095, 1581)) %>% 
  mutate(seq_id = case_when(Agency_ID == "kdk_aforv01" ~ 1569,
                            Agency_ID == "kdk_busrv01" ~ 1568,
                            TRUE ~ seq_id),
         Latitude = case_when(Lat_revised == "SAME" | Agency_ID == "Tanada Creek" ~ Latitude, 
                              TRUE ~ as.numeric(Lat_revised)),
         Longitude = case_when(Long_revised == "SAME" | Agency_ID == "Tanada Creek" ~ Longitude, 
                              TRUE ~ as.numeric(Long_revised)))  

#filter on sites needed for this project
osm_akssf_md <- osm_md %>% 
  filter(Agency_ID %in% c("Long Lake Creek", "Tanada Creek",
                          "kdk_aforv01", "kdk_busrv01")) %>% 

  select(AKOATS_ID = seq_id, SiteID = Agency_ID, SourceName, Contact_person, Latitude, Longitude, Waterbody_name)

osm_akssf_md
```

Water resources branch sites. Four sites on Kodiak island for this project: akalura r, dog salmon r, ayakulik r, and karluk r. Eegegik river in Bristol Bay in other repo.

```{r}
wrb_md <- read_excel("S:\\Stream Temperature Data\\USFWS Perdue - complete 2020\\WRB_Oct2020.xlsx", sheet = "AKOATS_metadata") %>% 
  filter(Contact_person == "Meg Perdue")

wrb_akssf_md <- wrb_md %>% 
  filter(Waterbody_name %in% c("Akalura River", "Dog Salmon River", "Ayakulik River", "Karluk River")) %>% 
  select(AKOATS_ID, SiteID, SourceName, Contact_person, Latitude, Longitude, Waterbody_name)

```

Combine the FWS sites for this project.

```{r}
fws_akssf_md <- bind_rows(kodiak_md, osm_akssf_md, wrb_akssf_md) %>% 
  select(AKOATS_ID, Waterbody_name, SiteID, SourceName, Contact_person, Latitude, Longitude)
```

# USGS sites in akoats

In GIS, selected all sites within AKSSF study areas: SC, Kodiak, and BB, and then selected subset with SourceName of USGS.

```{r}
usgs_md <- read_excel("data_preparation/akoats_workings_usgs.xls") %>% 
  select(AKOATS_ID = seq_id, Waterbody_name, SiteID = Agency_ID, SourceName, Contact_person, Latitude, Longitude)
```




# ACCS sites in Little Susitna watershed

The sites from 2016 are in akoats. We should have some additional sites and hopefully some of the same sites from 2019-2020 so that we have more than one summer of data.

```{r}
accs_2016_md <- akoats %>% filter(Contact_person == "Dan Bogan", Sensor_access == "Road") %>% 
  select(AKOATS_ID = seq_id, Waterbody_name, SiteID = Agency_ID, SourceName, Contact_person, Latitude, Longitude)

```

27 sites established in 2019 that are in the logger database on google drive.

```{r}
# Delete if write access necessary - You will be prompted to log in to google and enter to receive an authorization token when calling read_sheet

gs4_deauth() 

# set link to sheet
temp_log_db_gs = "https://docs.google.com/spreadsheets/d/1K5gY-fmjBrofKXGFaSMeOj_KmYFHkrGTE_inwZj9zec/edit#gid=2031679772"

#Read in temp logger deployment google sheet
logDb <- read_sheet(temp_log_db_gs,sheet = "TEMP LOGGER DEPLOYMENT",
                    col_names = TRUE,
                    col_types = "c") %>% 
  clean_names() 

#54 loggers at 27 sites
ls_sites <- logDb %>% 
  mutate(deployment_date = as.Date(deployment_date_m_d_y, format = "%m/%d/%Y"),
         deployment_year = year(deployment_date)) %>% 
  filter(project_name == "Little Su Temperature Network", deployment_year == 2019) 
  
#lat/longs are 1 per site so can use distinct to get locations
ls_sites %>% distinct(deployment_site_name, deployment_latitude_dd, deployment_longitude_dd) %>% count(deployment_site_name)

#append a _2019 to these sites so we can differentiate them from the sites in 2016.
ls_sites <- ls_sites %>% 
  distinct(deployment_site_name, deployment_latitude_dd, deployment_longitude_dd) %>% 
  mutate(SourceName = "uaaAKNHP", Contact_person = "Dan Bogan",
         Waterbody_name = case_when(grepl("Little Su", deployment_site_name) ~ "Little Susitna River",
                                    grepl("Lake Creek", deployment_site_name) ~ "Lake Creek",
                                    grepl("Trib", deployment_site_name) ~ "Tributary to Little Susitna River",
                                    grepl("trib", deployment_site_name) ~ "Tributary to Nancy Lakes",
                                    deployment_site_name == "Archangel" ~ "Archangel Creek",
                                    TRUE ~ deployment_site_name),
         Latitude = as.numeric(deployment_latitude_dd),
         Longitude = as.numeric(deployment_longitude_dd),
         SiteID = paste(deployment_site_name, "_2019", sep = "")) %>% 
  select(SiteID, Latitude, Longitude, Waterbody_name, SourceName, Contact_person)

ls_sites %>% select(SiteID, Waterbody_name)
```

Combine 2016 and 2019 logger locations - check in GIS to see if some of these are close enough to be the same site id.

```{r}
accs_md <- bind_rows(accs_2016_md, ls_sites)


```

Notes from Little Su Repo. These are all different time series so can easily be combined when we get the catchment ids on them. If Dustin cleans up site names in little su repo, can bring that metadata sheet over here.

Some sites are in the same catchment and relatively close so we could manage them with on SiteID:

* Archangel Creek, two sites are 41 m apart
* LS12 and Little Su 14, 32 m apart
* Little Su 12 is right next to USGS gage 15290000, just fyi
* Coho and Coho Creek are 29 m apart
* Lake Creek and Lake Creek 2 are 14 m apart
* Lake Creek 1 and Lake above Nancy are 12 m apart
* Papoose and Papoose Creek are 10 m apart
* Tributary 4 and Lake Complex are 34 m apart

Some of the mainstem sites are in the same catchment, but pretty far apart so we should probably keep them as separate sites:

* Little Su 6 and LS8 are about ~190 m apart along the flowline and are in the same catchment
* Little Su 3 and LS3 are ~ 800 m apart as the crow flies, but much further on stream line, same catchment though
* LS1 and Little Su 2 are 400 m apart direct line distance and in the same catchment
* Little Su 1 and LS PUF are ~400 m apart direct line distance and in the same catchment


# Bristol Bay metadata

This is all being resolved as part of Southwest Alaska Salmon Habitat Partnership project. Bring in metadata list from that repo.

```{r}
bb_md <- readRDS("W:/Github/SWSHP-Bristol-Bay-Thermal-Diversity/output/bristol_bay_site_metadata.rds")
```



# Combine all raw metadata files with only the needed attributes

```{r}

akssf_md <- bind_rows(usfs_akssf_md, ci_tempmodel_akssf_md, cik_akssf_md, nps_akssf_md, fws_akssf_md, bb_md, usgs_md, accs_md)

write_csv(akssf_md, path = "output/akssf_metadata.csv")
```


