---
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
title: "AKSSF Temperature Data Summary"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 6
    fig_caption: yes
    code_folding: hide
    toc: true
    number_sections: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document last updated `r Sys.time()` by Rebecca Shaftel (rsshaftel@alaska.edu). 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(ggpubr)
library(kableExtra)
library(tidyverse)
```

This report summarizes the  thermal sensitivity results and draft predictive model for team meeting on February 4, 2022.
Goals of the meeting are to discuss model inputs and methods and prediction scenarios.

Objectives: 

1. Build a predictive model for stream thermal sensitivity (TS). 
2. Develop scenarios to understand effects of warming stream temperatures on subsistence salmon populations.

The [project mapper](https://arcg.is/1nLqei) is updated with TS by year linked to sites and layers showing a lot of the input data and various hydrologic layers. 

# Thermal sensitivity

DFA output were biased for some years because most of the sites (half or more) came from one watershed with high thermal sensitivity. For some reason, the pattern in these sites all appeared on the trend and their TS was really low.

```{r dfa plots}
dfa_dat <- read_csv("DFA/output_8Nov21/GlobalModelEstimates_1trend_DUE.csv")
trend_dat <- read_csv("DFA/output_8Nov21/GlobalTrends_1trend_DUE.csv")

dfa_dat %>% 
  ggplot(aes(TempSens)) +
  geom_freqpoly(aes(color = Region)) +
  facet_wrap(~Year)

trend_dat %>% 
  ggplot(aes(x = DOY, y = Trend)) +
  geom_line() + 
  facet_wrap(~Year)
```

As an alternative modeling method to estimate stream thermal sensitivity, Tim used the air temperature coefficient in a time series model for each site and summer using an auto-regressive lag of 1 day on the residuals (AR1 model). These results better matched the patterns between air-stream temperatures at the different sites.

```{r ar1 TS}
mod_dat <- readRDS("data_preparation/final_data/model_data2022-01-19.rds")

mod_dat %>% 
  ggplot(aes(TempSens)) +
  geom_freqpoly(aes(color = Region)) +
  facet_wrap(~Year)

```



# Reduce covariates

Reduced the list of covariates based on pairwise correlations and VIF. Also transformed covariates that were heavily right skewed. Logit for percent cover and log10 for continuous.
Final list of covariates, which are also listed with data sources on google drive [sheet](https://docs.google.com/spreadsheets/d/1Ir1HyvhHqfbUSaGjiI-ygeQwJkh2QsihtWueJfDQ4cA/edit#gid=0):

* stream order
* stream gradient
* distance to coast
* catchment min, max, and mean slope
* watershed min, max, and mean slope
* catchment min, max, and mean elevation
* watershed min, max, and mean elevation
* watershed percent north aspect
* watershed percent wetland cover
* watershed percent glacier cover
* watershed percent lake cover
* watershed area
* watershed LCLD (last day of longest continuous snow season) by year (2001-2019) - converted to a snow index, the residuals of a model predicting LCLD using mean watershed slope^2^
* total summer precipitation at site by year

Removed covariates with pairwise correlations r > 0.7 and VIF > 3. 
Final list of 12 covariates:

* log stream gradient
* mean catchment elevation
* watershed percent north aspect
* mean watershed slope
* log watershed area
* distance to coast
* watershed wetland percent
* logit glacier cover
* logit lake cover
* snow index
* summer precipitation

# Select random effects

Created a global model with all 12 covariates plus five interactions from Tim's Bristol Bay paper and results. Selected best random effects.

Interactions: 

* snow x north
* slope x north
* north x precip
* slope x precip
* slope x snow

Random effects:

* none
* random intercept for Site (multiple TS by year for sites)
* random intercept for Site within HUC8
* random intercept for Site within Region (this doesn't make a lot of sense per mixed effects literature since we only have 5 regions -- could include as a fixed effect)

Lowest AIC for HUC8/Site. Predictions can be made at level 1 for HUC8, except for the one or two without data, which we can predict using level 0, or population mean for random intercept.

# Develop model set 

Dredged all subsets of global model to determine variable importance: sum of model weights for all models in which a covariate appears.  

```{r variable importance}

lme1_dr <- readRDS("output/lme1m_dredge_results.rds")

#variable importance
lme1_dr %>% 
  as_tibble() %>% 
  mutate(across(cat_elev_MEAN:'wtd_north_per:wtd_slope_MEAN', ~ case_when(!is.na(.) ~ weight))) %>% 
  select(cat_elev_MEAN:'wtd_north_per:wtd_slope_MEAN') %>% 
  colSums(., na.rm = TRUE) %>% 
  enframe() %>% 
  arrange(desc(value)) %>% 
  ggplot(aes(y = fct_reorder(name, value), x = value)) +
  geom_point() +
  theme_bw() +
  labs(x = "Variable Importance Value", y = "Covariates and Interactions")

```

Model set of 6 models:

* global model
* model with top 11 covariates
* model with top 8 covariates
* model with top 3 covariates
* null model
* random forest model (I randomly selected only 1 year for each site to avoid pseudo-replicating the spatial covariates)

# Cross validation

Model selection via cross-validation. Constructing appropriate cross-validation groups to select the optimal model depends on our prediction goals. This paper on [cross-validation strategies](https://onlinelibrary.wiley.com/doi/10.1111/ecog.02881) has some interesting ideas and guidance. I tried three different methods: temporal splits by groups of years, spatial splits by sites in the same HUC10, and leave-one-out.

* Temporal cross-validation groups were constructed by splitting the years into five groups. For temporal cross-validation, predictions were made for the withheld data using the random intercept for HUC8.
* Spatial cross-validation was done by constructing 21 groups with 6 HUC10 each that were not adjacent. Predictions were made using the population intercept since these groups overlapped with the HUC8 groupings used for the random intercept.
* Leave-one-out cross validation withholds one site at a time, builds the model, and predicts for the years of data for that site. I removed any sites that were the only site in a HUC8 in order to predict with the HUC8 random intercepts.

For all cross-validations, the predictions on the testing data were saved and used to calculate RMSE, MAE, and observed versus predicted correlations. 

The figure shows the mean RMSE +/- 1SE across cross-validation groups. SE for LOO is missing because groups are only a single site so the RMSE was calculated for all predictions and not by group.

```{r fig.width = 12, fig.height = 12}

xval_results <- read_rds("output/xval_results.rds") 

xval_results %>% 
  # filter(!xval_type == "LOO") %>% 
  mutate(modelf = factor(model, levels = c("global", "lme_11vars", "lme_8vars", "lme_3vars", "lme_null", "rf"),
                         labels = c("global", "11 vars.", "8 vars.", "3 vars.", "null", "RF"))) %>% 
  ggplot(aes(x = modelf, y = rmse, color = xval_type)) +
  stat_summary(fun = "mean", geom = "point", size = 3) +
  stat_summary(fun.data = "mean_se", geom = "errorbar", width = 0.2) +
  labs(y = "RMSE on Validation Data", x = "Model", color = "Cross-validation Type") +
  theme_bw() +
  theme(legend.position = "bottom", text = element_text(size = 16))
```

Table summarizing average RMSE, MAE, and correlations for observed versus predicted TS by model.

```{r }
xval_results %>% 
  group_by(model, xval_type) %>% 
  summarize(mean_rmse = mean(rmse), mean_mae = mean(mae), mean_cor = mean(obs_pred)) %>% 
  kable(digits = 2, col.names = c("Model", "Xval Type", "RMSE", "MAE", "Cor. (r)")) %>% 
  kable_styling()

```

Plots of observed versus predicted TS by the different models using the LOO cross-validation predictions. The RF model was constructed with just one year for each site as before. Pearson correlations between observed and predicted by region indicate that the models do really poorly for the regions with very little data: Copper, PWS, and Kodiak. We could try region specific models, but we would likely need to consider a smaller set of predictors as these regions don't have a lot of data.

```{r}
xval_preds <- readRDS("output/xval_preds.rds")

xval_preds %>% 
  filter(xval_type == "LOO") %>% 
  mutate(modelf = factor(model, levels = c("global", "lme_11vars", "lme_8vars", "lme_3vars", "lme_null", "rf"),
                         labels = c("global", "11 vars.", "8 vars.", "3 vars.", "null", "RF"))) %>% 
  ggplot(aes(x = TempSens, y = preds)) +
  geom_point() +
  geom_abline(aes(intercept = 0, slope = 1)) +
  facet_grid(cols = vars(Region), rows = vars(modelf)) +
  coord_cartesian(xlim = c(-0.2, 1), ylim = c(-0.2,1)) +
  stat_cor(label.x.npc = 0, label.y = c(seq(0.6, 1, 0.1)))

```



# Prediction scenarios

Spatial domain: salmon habitat for all regions. Select HU12 that intersect anadromous waters catalog.
Spatial scale: suggest using stream outlets associated with HU12 boundaries (smallest are 25 km^2^). We can use these outlets to create watersheds and calculate all the same covariates for prediction.  

Temporal scenarios:
Ideas for our scenarios include picking years with our temporal domain (2001-2019) that represent contrasting conditions that affect TS (e.g. low and high snow years). One problem with this idea is that one year could have really different conditions across our study area. Alternatively, we could set the conditions using a new data frame -- e.g. 90th percentile of snowpack by region. 



climate scenario plot - air tmep precip and snow pariwise plots to see realistic scenarios by region


```{r}

```


