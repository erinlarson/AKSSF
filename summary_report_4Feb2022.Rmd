---
title: "AKSSF Temperature Data Summary"
output:
  html_document: 
    df_print: paged
    fig_width: 10
    fig_height: 10
    fig_caption: yes
    code_folding: hide
    toc: true
    number_sections: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
editor_options: 
  chunk_output_type: inline
---

Document last updated `r Sys.time()` by Rebecca Shaftel (rsshaftel@alaska.edu). 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(ggpubr)
library(kableExtra)
library(rpart)
library(tidyverse)
```

This report summarizes the  thermal sensitivity results and draft predictive model for team meeting on February 4, 2022.
Goals of the meeting are to discuss model inputs and methods and prediction scenarios.

Objectives: 

1. Build a predictive model for stream thermal sensitivity (TS). 
2. Develop scenarios to understand effects of warming stream temperatures on subsistence salmon populations.

The [project mapper](https://arcg.is/1nLqei) is updated with TS by year linked to sites and layers showing a lot of the input data and various hydrologic layers. 

# Thermal sensitivity

DFA output were biased for some years because most of the sites (half or more) came from one watershed with high thermal sensitivity. For some reason, the pattern in these sites all appeared on the trend and their TS was really low.

```{r dfa plots}
dfa_dat <- read_csv("DFA/output_8Nov21/GlobalModelEstimates_1trend_DUE.csv")
trend_dat <- read_csv("DFA/output_8Nov21/GlobalTrends_1trend_DUE.csv")

dfa_dat %>% 
  ggplot(aes(TempSens)) +
  geom_freqpoly(aes(color = Region)) +
  facet_wrap(~Year)

trend_dat %>% 
  ggplot(aes(x = DOY, y = Trend)) +
  geom_line() + 
  facet_wrap(~Year)
```

As an alternative modeling method to estimate stream thermal sensitivity, Tim used the air temperature coefficient in a time series model for each site and summer using an auto-regressive lag of 1 day on the residuals (AR1 model). These results better matched the patterns between air-stream temperatures at the different sites.

```{r ar1 TS}
mod_dat <- readRDS("data_preparation/final_data/model_data2022-02-08.rds")

mod_dat %>% 
  ggplot(aes(TempSens)) +
  geom_freqpoly(aes(color = Region)) +
  facet_wrap(~Year)

```




# Relationships between TS and covariates


```{r pair plots against TS}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}

mod_dat %>% 
  select(TempSens, log_slope, log_cat_elev, log_cat_slope, 
         wtd_slope_MEAN, snow_ind, wtd_lcld_jd, summer_precip) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist, lower.panel = panel.smooth)

mod_dat %>% 
  select(TempSens, log_area, dist_coast_km, wtd_north_per,
         asrt_wet, asrt_glac, asrt_lake, glac_10) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist, lower.panel = panel.smooth)

```

Exploring interactions between covariates and TS. Starting with different variables and mean watershed slope.

```{r}
facet_var <- "wtd_slope_MEAN"

mod_dat %>% 
  mutate(fct = cut(get(facet_var), seq(from = min(get(facet_var)) - 0.01, to = max(get(facet_var)), length.out = 5))) %>% 
  ggplot() +
  geom_point(aes(y = TempSens, x = wtd_north_per, color = Region)) +
  geom_smooth(aes(y = TempSens, x = wtd_north_per), method = "lm") +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed slope and north aspect", subtitle = paste0("Facets are bins of ", facet_var))
```


```{r}
facet_var <- "wtd_slope_MEAN"

mod_dat %>% 
  mutate(fct = cut(get(facet_var), seq(from = min(get(facet_var)) - 0.01, to = max(get(facet_var)), length.out = 5))) %>% 
  ggplot() +
  geom_point(aes(y = TempSens, x = asrt_glac, color = Region)) +
  geom_smooth(aes(y = TempSens, x = asrt_glac), method = "lm") +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed slope and glacier cover", subtitle = paste0("Facets are bins of ", facet_var))
```


```{r}
facet_var <- "wtd_slope_MEAN"

mod_dat %>% 
  mutate(fct = cut(get(facet_var), seq(from = min(get(facet_var)) - 0.01, to = max(get(facet_var)), length.out = 5))) %>% 
  ggplot() +
  geom_point(aes(y = TempSens, x = asrt_lake, color = Region)) +
  geom_smooth(aes(y = TempSens, x = asrt_lake), method = "lm") +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed slope and lake cover", subtitle = paste0("Facets are bins of ", facet_var))
```


```{r}
facet_var <- "wtd_slope_MEAN"

mod_dat %>% 
  mutate(fct = cut(get(facet_var), seq(from = min(get(facet_var)) - 0.01, to = max(get(facet_var)), length.out = 5))) %>% 
  ggplot() +
  geom_point(aes(y = TempSens, x = asrt_wet, color = Region)) +
  geom_smooth(aes(y = TempSens, x = asrt_wet), method = "lm") +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed slope and wetland cover", subtitle = paste0("Facets are bins of ", facet_var))
```


```{r}
facet_var <- "wtd_slope_MEAN"

mod_dat %>% 
  mutate(fct = cut(get(facet_var), seq(from = min(get(facet_var)) - 0.01, to = max(get(facet_var)), length.out = 5))) %>% 
  ggplot() +
  geom_point(aes(y = TempSens, x = summer_precip, color = Region)) +
  geom_smooth(aes(y = TempSens, x = summer_precip), method = "lm") +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed slope and summer precipitation", subtitle = paste0("Facets are bins of ", facet_var))
```


```{r}
facet_var <- "wtd_slope_MEAN"

mod_dat %>% 
  mutate(fct = cut(get(facet_var), seq(from = min(get(facet_var)) - 0.01, to = max(get(facet_var)), length.out = 5))) %>% 
  ggplot() +
  geom_point(aes(y = TempSens, x = snow_ind, color = Region)) +
  geom_smooth(aes(y = TempSens, x = snow_ind), method = "lm") +
  facet_wrap(~fct) +
  labs(title = "TS by mean watershed slope and snow index", subtitle = paste0("Facets are bins of ", facet_var))
```


```{r}
facet_var <- "wtd_north_per"

mod_dat %>% 
  mutate(fct = cut(get(facet_var), seq(from = min(get(facet_var)) - 0.01, to = max(get(facet_var)), length.out = 5))) %>% 
  ggplot() +
  geom_point(aes(y = TempSens, x = snow_ind, color = Region)) +
  geom_smooth(aes(y = TempSens, x = snow_ind), method = "lm") +
  facet_wrap(~fct) +
  labs(title = "TS by watershed north percent and snow index", subtitle = paste0("Facets are bins of ", facet_var))
```

```{r}
facet_var <- "wtd_north_per"

mod_dat %>% 
  mutate(fct = cut(get(facet_var), seq(from = min(get(facet_var)) - 0.01, to = max(get(facet_var)), length.out = 5))) %>% 
  ggplot() +
  geom_point(aes(y = TempSens, x = summer_precip, color = Region)) +
  geom_smooth(aes(y = TempSens, x = summer_precip), method = "lm") +
  facet_wrap(~fct) +
  labs(title = "TS by watershed north percent and summer precipitation", subtitle = paste0("Facets are bins of ", facet_var))
```


# Reduce covariates

Reduced the list of covariates based on pairwise correlations and VIF. Also transformed covariates that were heavily right skewed. Arcsine square-root for proportions and log10 for continuous. Glacier cover still heavily right-skewed so added a binary presence-absence for glacier cover as well and checked VIF.
Final list of covariates, which are also listed with data sources on google drive [sheet](https://docs.google.com/spreadsheets/d/1Ir1HyvhHqfbUSaGjiI-ygeQwJkh2QsihtWueJfDQ4cA/edit#gid=0):

* stream order
* stream gradient
* distance to coast
* catchment min, max, and mean slope
* watershed min, max, and mean slope
* catchment min, max, and mean elevation
* watershed min, max, and mean elevation
* watershed percent north aspect
* watershed percent wetland cover
* watershed percent glacier cover
* watershed percent lake cover
* watershed area
* watershed LCLD (last day of longest continuous snow season) by year (2001-2019) - converted to a snow index, the residuals of a model predicting LCLD using mean watershed slope^2^
* total summer precipitation at site by year

Removed covariates with pairwise correlations r > 0.7 and VIF > 3. 
Final list of 13 covariates:

* log stream gradient
* log mean catchment elevation
* log mean catchment slope
* watershed percent north aspect
* mean watershed slope
* log watershed area
* distance to coast
* arcsine square-root watershed wetland percent
* arcsine square-root watershed glacier cover
* arcsine square-root watershed lake cover
* snow index
* summer precipitation
* glacier presence-absence

# Select random effects

Created a global model with all 13 covariates plus five interactions from Tim's Bristol Bay paper and results. Selected best random effects.

Interactions: 

* snow x north
* slope x north
* north x precip
* slope x precip
* slope x snow

Random effects:

* none
* random intercept for Site (multiple TS by year for sites)
* random intercept for Site within HUC8
* random intercept for Site within Region (this doesn't make a lot of sense per mixed effects literature since we only have 5 regions -- could include as a fixed effect)

Lowest AIC for HUC8/Site. Predictions can be made at level 1 for HUC8, except for the one or two without data, which we can predict using level 0, or population mean for random intercept.

# Develop model set 

Dredged all subsets of global model to determine variable importance: sum of model weights for all models in which a covariate appears.  

```{r variable importance}

lme1_dr <- readRDS("output/lme1m_dredge_results.rds")

#variable importance
lme1_dr %>% 
  as_tibble() %>% 
  mutate(across(asrt_glac:'wtd_north_per:wtd_slope_MEAN', ~ case_when(!is.na(.) ~ weight))) %>% 
  select(asrt_glac:'wtd_north_per:wtd_slope_MEAN') %>% 
  colSums(., na.rm = TRUE) %>% 
  enframe() %>% 
  arrange(desc(value)) %>% 
  ggplot(aes(y = fct_reorder(name, value), x = value)) +
  geom_point() +
  theme_bw() +
  labs(x = "Variable Importance Value", y = "Covariates and Interactions")

```

Model set of 6 models:

* global model
* model with top 12 covariates
* model with top 8 covariates
* model with top 4 covariates
* null model
* random forest model (I randomly selected only 1 year for each site to avoid pseudo-replicating the spatial covariates)

# Cross validation

Model selection via cross-validation. Constructing appropriate cross-validation groups to select the optimal model depends on our prediction goals. This paper on [cross-validation strategies](https://onlinelibrary.wiley.com/doi/10.1111/ecog.02881) has some interesting ideas and guidance. I tried three different methods: temporal splits by groups of years, spatial splits by sites in the same HUC10, and leave-one-out.

* Temporal cross-validation groups were constructed by splitting the years into five groups. For temporal cross-validation, predictions were made for the withheld data using the random intercept for HUC8.
* Spatial cross-validation was done by constructing 21 groups with 6 HUC10 each that were not adjacent. Predictions were made using the population intercept since these groups overlapped with the HUC8 groupings used for the random intercept.
* Leave-one-out cross validation withholds one site at a time, builds the model, and predicts for the years of data for that site. I removed any sites that were the only site in a HUC8 in order to predict with the HUC8 random intercepts.

For all cross-validations, the predictions on the testing data were saved and used to calculate RMSE, MAE, and observed versus predicted correlations. 

The figure shows the mean RMSE +/- 1SE across cross-validation groups. SE for LOO is missing because groups are only a single site so the RMSE was calculated for all predictions and not by group.

```{r fig.width = 12, fig.height = 12}

xval_results <- read_rds("output/xval_results.rds") 

xval_results %>% 
  # filter(!xval_type == "LOO") %>% 
  mutate(modelf = factor(model, levels = c("global", "lme_12vars", "lme_8vars", "lme_4vars", "lme_null", "rf"),
                         labels = c("global", "12 vars.", "8 vars.", "4 vars.", "null", "RF"))) %>% 
  ggplot(aes(x = modelf, y = rmse, color = xval_type)) +
  stat_summary(fun = "mean", geom = "point", size = 3) +
  stat_summary(fun.data = "mean_se", geom = "errorbar", width = 0.2) +
  labs(y = "RMSE on Validation Data", x = "Model", color = "Cross-validation Type") +
  theme_bw() +
  theme(legend.position = "bottom", text = element_text(size = 16))
```

Table summarizing average RMSE, MAE, and correlations for observed versus predicted TS by model.

```{r }
xval_results %>% 
  group_by(model, xval_type) %>% 
  summarize(mean_rmse = mean(rmse), mean_mae = mean(mae), mean_cor = mean(obs_pred)) %>% 
  kable(digits = 2, col.names = c("Model", "Xval Type", "RMSE", "MAE", "Cor. (r)")) %>% 
  kable_styling()

```

Plots of observed versus predicted TS by the different models using the LOO cross-validation predictions. The RF model was constructed with just one year for each site as before. Pearson correlations between observed and predicted by region indicate that the models do really poorly for the regions with very little data: Copper, PWS, and Kodiak. We could try region specific models, but we would likely need to consider a smaller set of predictors as these regions don't have a lot of data.

```{r}
xval_preds <- readRDS("output/xval_preds.rds")

xval_preds %>% 
  filter(xval_type == "LOO") %>% 
  mutate(modelf = factor(model, levels = c("global", "lme_12vars", "lme_8vars", "lme_4vars", "lme_null", "rf"),
                         labels = c("global", "12 vars.", "8 vars.", "4 vars.", "null", "RF"))) %>% 
  ggplot(aes(x = TempSens, y = preds)) +
  geom_point() +
  geom_abline(aes(intercept = 0, slope = 1)) +
  facet_grid(cols = vars(Region), rows = vars(modelf)) +
  coord_cartesian(xlim = c(-0.2, 1), ylim = c(-0.2,1)) +
  stat_cor(label.x.npc = 0, label.y = c(seq(0.6, 1, 0.1)))

```

# CART analysis

Using rpart to create a regression tree with all of the data.

```{r}
ct_all <- rpart(TempSens ~ log_slope + log_cat_elev + log_cat_slope + wtd_north_per +
                      wtd_slope_MEAN + log_area + dist_coast_km + asrt_wet + 
                      asrt_glac + asrt_lake + snow_ind + summer_precip, 
                    data = mod_dat, method = "anova")
```

Print the cross validation results to select a complexity parameter for tree-pruning. From plotcp:
"A good choice of cp for pruning is often the leftmost value for which the mean lies below the horizontal line." Select cp for 12 splits.

```{r}
plotcp(ct_all) 
printcp(ct_all) 
cp_parm <- 0.011197

```

Prune the tree and print the results.

```{r}
ct_pr <- prune(ct_all, cp = cp_parm)

# print(ct_pr)

par(xpd = TRUE)
plot(ct_pr, compress = TRUE)
text(ct_pr, use.n = TRUE)


```

Summary table indicates variable importance and also lists surrogate splits. Interesting that snow index and precipitation are the least important variables.

```{r}
summary(ct_pr)
```

## Cook Inlet Regression Tree


```{r}
ct_ci <- rpart(TempSens ~ log_slope + log_cat_elev + log_cat_slope + wtd_north_per +
                      wtd_slope_MEAN + log_area + dist_coast_km + asrt_wet + 
                      asrt_glac + asrt_lake + snow_ind + summer_precip, 
                    data = mod_dat %>% filter(Region == "Cook_Inlet"), method = "anova")
# plotcp(ct_ci) 
# printcp(ct_ci) 
cp_parm <- 0.010000
ct_cipr <- prune(ct_ci, cp = cp_parm)

par(xpd = TRUE)
plot(ct_cipr, compress = TRUE)
text(ct_cipr, use.n = TRUE)

```

Summary table indicates variable importance and also lists surrogate splits. Interesting that snow index and precipitation are the least important variables.

```{r}
summary(ct_cipr)
```




## Bristol Bay Regression Tree


```{r}
ct_bb <- rpart(TempSens ~ log_slope + log_cat_elev + log_cat_slope + wtd_north_per +
                      wtd_slope_MEAN + log_area + dist_coast_km + asrt_wet + 
                      asrt_glac + asrt_lake + snow_ind + summer_precip, 
                    data = mod_dat %>% filter(Region == "Bristol_Bay"), method = "anova")

# plotcp(ct_bb) 
# printcp(ct_bb) 
cp_parm <- 0.014908     
ct_bbpr <- prune(ct_bb, cp = cp_parm)

par(xpd = TRUE)
plot(ct_bbpr, compress = TRUE)
text(ct_bbpr, use.n = TRUE)

```

Summary table indicates variable importance and also lists surrogate splits. Interesting that snow index and precipitation are the least important variables.

```{r}
summary(ct_bbpr)
```



## Kodiak Regression Tree


```{r}
ct_kod <- rpart(TempSens ~ log_slope + log_cat_elev + log_cat_slope + wtd_north_per +
                      wtd_slope_MEAN + log_area + dist_coast_km + asrt_wet + 
                      asrt_glac + asrt_lake + snow_ind + summer_precip, 
                    data = mod_dat %>% filter(Region == "Kodiak"), method = "anova")
plotcp(ct_kod) 
printcp(ct_kod) 
cp_parm <- 0.053756      
ct_kodpr <- prune(ct_kod, cp = cp_parm)

par(xpd = TRUE)
plot(ct_kodpr, compress = TRUE)
text(ct_kodpr, use.n = TRUE)

```

Summary table indicates variable importance and also lists surrogate splits. Interesting that snow index and precipitation are the least important variables.

```{r}
summary(ct_kodpr)
```



## Prince William Sound Regression Tree


```{r}
ct_pws <- rpart(TempSens ~ log_slope + log_cat_elev + log_cat_slope + wtd_north_per +
                      wtd_slope_MEAN + log_area + dist_coast_km + asrt_wet + 
                      asrt_glac + asrt_lake + snow_ind + summer_precip, 
                    data = mod_dat %>% filter(Region == "Prince_William_Sound"), method = "anova")
 # plotcp(ct_pws) 
 # printcp(ct_pws) 
cp_parm <- 0.020529      
ct_pwspr <- prune(ct_pws, cp = cp_parm)

par(xpd = TRUE)
plot(ct_pwspr, compress = TRUE)
text(ct_pwspr, use.n = TRUE)

```

Summary table indicates variable importance and also lists surrogate splits. Interesting that snow index and precipitation are the least important variables.

```{r}
summary(ct_pwspr)
```



## Copper River Regression Tree


```{r}
ct_cr <- rpart(TempSens ~ log_slope + log_cat_elev + log_cat_slope + wtd_north_per +
                      wtd_slope_MEAN + log_area + dist_coast_km + asrt_wet + 
                      asrt_glac + asrt_lake + snow_ind + summer_precip, 
                    data = mod_dat %>% filter(Region == "Copper_River"), method = "anova")
 plotcp(ct_cr) 
 printcp(ct_cr) 
cp_parm <- 0.010826      
ct_crpr <- prune(ct_cr, cp = cp_parm)

par(xpd = TRUE)
plot(ct_crpr, compress = TRUE)
text(ct_crpr, use.n = TRUE)

```

Summary table indicates variable importance and also lists surrogate splits. Interesting that snow index and precipitation are the least important variables.

```{r}
summary(ct_crpr)
```



# Post-hoc exploration of regional TS

For each region, the number of sites is listed along with a table showing the years for each site, and pairplots of TS against each of the covariates (separated into two plots since there are so many).

Copper River

```{r Copper}
region_in <- "Copper_River"

print(paste0(region_in, ": ", nrow(mod_dat %>% filter(Region == region_in) %>% distinct(Site)), " sites"))

mod_dat %>% 
  filter(Region == region_in) %>% 
  group_by(Site) %>% 
  summarize(years = paste(Year, collapse = ", ")) %>% 
  kable(longtable = TRUE) %>%
  kable_styling()

mod_dat %>% 
  filter(Region == region_in) %>% 
  select(TempSens, log_slope, log_cat_elev, log_cat_slope, 
         wtd_slope_MEAN, snow_ind, wtd_lcld_jd, summer_precip) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist, lower.panel = panel.smooth)

mod_dat %>% 
  filter(Region == region_in) %>% 
  select(TempSens, log_area, dist_coast_km, wtd_north_per,
         asrt_wet, asrt_glac, asrt_lake, glac_10) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist, lower.panel = panel.smooth)
```

Kodiak

```{r Kodiak}
region_in <- "Kodiak"

print(paste0(region_in, ": ", nrow(mod_dat %>% filter(Region == region_in) %>% distinct(Site)), " sites"))

mod_dat %>% 
  filter(Region == region_in) %>% 
  group_by(Site) %>% 
  summarize(years = paste(Year, collapse = ", ")) %>% 
  kable(longtable = TRUE) %>%
  kable_styling()

mod_dat %>% 
  filter(Region == region_in) %>% 
  select(TempSens, log_slope, log_cat_elev, log_cat_slope, 
         wtd_slope_MEAN, snow_ind, wtd_lcld_jd, summer_precip) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist, lower.panel = panel.smooth)

mod_dat %>% 
  filter(Region == region_in) %>% 
  select(TempSens, log_area, dist_coast_km, wtd_north_per,
         asrt_wet, asrt_glac, asrt_lake, glac_10) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist, lower.panel = panel.smooth)
```

Prince William Sound

```{r PWS}
region_in <- "Prince_William_Sound"

print(paste0(region_in, ": ", nrow(mod_dat %>% filter(Region == region_in) %>% distinct(Site)), " sites"))

mod_dat %>% 
  filter(Region == region_in) %>% 
  group_by(Site) %>% 
  summarize(years = paste(Year, collapse = ", ")) %>% 
  kable(longtable = TRUE) %>%
  kable_styling()

mod_dat %>% 
  filter(Region == region_in) %>% 
  select(TempSens, log_slope, log_cat_elev, log_cat_slope, 
         wtd_slope_MEAN, snow_ind, wtd_lcld_jd, summer_precip) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist, lower.panel = panel.smooth)

mod_dat %>% 
  filter(Region == region_in) %>% 
  select(TempSens, log_area, dist_coast_km, wtd_north_per,
         asrt_wet, asrt_glac, asrt_lake, glac_10) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist, lower.panel = panel.smooth)
```

Cook Inlet

```{r Cook Inlet}
region_in <- "Cook_Inlet"

print(paste0(region_in, ": ", nrow(mod_dat %>% filter(Region == region_in) %>% distinct(Site)), " sites"))

mod_dat %>% 
  filter(Region == region_in) %>% 
  group_by(Site) %>% 
  summarize(years = paste(Year, collapse = ", ")) %>% 
  kable(longtable = TRUE) %>%
  kable_styling()

mod_dat %>% 
  filter(Region == region_in) %>% 
  select(TempSens, log_slope, log_cat_elev, log_cat_slope, 
         wtd_slope_MEAN, snow_ind, wtd_lcld_jd, summer_precip) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist, lower.panel = panel.smooth)

mod_dat %>% 
  filter(Region == region_in) %>% 
  select(TempSens, log_area, dist_coast_km, wtd_north_per,
         asrt_wet, asrt_glac, asrt_lake, glac_10) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist, lower.panel = panel.smooth)
```

Bristol Bay

```{r Bristol Bay}
region_in <- "Bristol_Bay"

print(paste0(region_in, ": ", nrow(mod_dat %>% filter(Region == region_in) %>% distinct(Site)), " sites"))

mod_dat %>% 
  filter(Region == region_in) %>% 
  group_by(Site) %>% 
  summarize(years = paste(Year, collapse = ", ")) %>% 
  kable(longtable = TRUE) %>%
  kable_styling()

mod_dat %>% 
  filter(Region == region_in) %>% 
  select(TempSens, log_slope, log_cat_elev, log_cat_slope, 
         wtd_slope_MEAN, snow_ind, wtd_lcld_jd, summer_precip) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist, lower.panel = panel.smooth)

mod_dat %>% 
  filter(Region == region_in) %>% 
  select(TempSens, log_area, dist_coast_km, wtd_north_per,
         asrt_wet, asrt_glac, asrt_lake, glac_10) %>% 
  pairs(upper.panel = panel.cor, diag.panel = panel.hist, lower.panel = panel.smooth)
```


# Prediction scenarios

Spatial domain: salmon habitat for all regions. Select HU12 that intersect anadromous waters catalog.
Spatial scale: suggest using stream outlets associated with HU12 boundaries (smallest are 25 km^2^). We can use these outlets to create watersheds and calculate all the same covariates for prediction.  

Temporal scenarios:
Ideas for our scenarios include picking years within our temporal domain (2001-2019) that represent contrasting conditions that affect TS (e.g. low and high snow years). One problem with this idea is that conditions could vary across our study area so a high snow year in Bristol Bay may not match Copper. Alternatively, we could develop scenarios for prediction using high and low ranges of snowpack or precipitation for each region. 

Comparison of standardized (subtract mean and divide by sd) temperature and precipitation values by different regions:

* 2019 warm and low precipitation
* 2016 warm and high precipitation
* 2006 cold and high precipitation
* 2017 and 2009 about normal years for both

```{r}
clim_dat <- readRDS("output/clim_dat.rds")

scale_var <- function(x){
  (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)
}

#deviations from normal
clim_dat %>% 
  group_by(Region, parameter) %>% 
  dplyr::mutate(Value_metric_sc = scale_var(Value_metric)) %>% 
  ggplot(aes(x = Year, y = Value_metric_sc, color = Region)) +
  geom_line() +
  scale_x_continuous(breaks = c(seq(2000, 2019, 2))) +
  facet_wrap(~parameter, scales = "free", ncol = 1) +
  geom_hline(aes(yintercept = 0)) +
  theme_bw() +
  theme(legend.position = "right") +
  labs(y = "Standardized values")

```

Snowtel data indicate April 1st SWE (magnitude of spring snowpack) for different sites across some of our regions. Note that there were no snowtel sites with data for Bristol Bay or Kodiak. From Tim's paper, 2012 and 2013 were high snow years and 2015 was a low snow year.

* 2012 was a high snow year
* 2015 was a low snow year

```{r}
snowtel <- readRDS("output/snowtel.rds")

#deviations from normal
snowtel %>% 
  filter(!grepl("Gulk|Tela", Snowtel_site)) %>% 
  group_by(Snowtel_site) %>% 
  mutate(swe_sc = scale_var(Apr1_SWE)) %>% 
  ggplot(aes(x = Year, y = swe_sc, color = Snowtel_site)) +
  geom_line() +
  scale_x_continuous(breaks = c(seq(2000, 2019, 2))) +
  theme_bw() +
  theme(legend.position = "right") +
  geom_hline(aes(yintercept = 0)) +
  facet_wrap(~Region, ncol = 1) +
  labs(y = "Standardized values")

```


